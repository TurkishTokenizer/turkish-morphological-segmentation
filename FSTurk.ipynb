{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cLqzYiYHpZ0-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765851870001,"user_tz":-180,"elapsed":2282,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"0b982d5a-ae16-4e4f-c250-173f4516f4c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6txQNlZvQ8P","outputId":"aa73b23d-12fd-4545-a4ad-046c4b6b68ef","executionInfo":{"status":"ok","timestamp":1765851880881,"user_tz":-180,"elapsed":10108,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pynini in /usr/local/lib/python3.12/dist-packages (2.1.7)\n","Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (3.1.1)\n","The wurlitzer extension is already loaded. To reload it, use:\n","  %reload_ext wurlitzer\n"]}],"source":["# Setup\n","!pip install --only-binary :all: pynini\n","!pip install wurlitzer\n","import pynini\n","%load_ext wurlitzer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_HcAl-2_lcJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b5c433b-c150-43f0-a9f3-3c4cab831232","executionInfo":{"status":"ok","timestamp":1765853967124,"user_tz":-180,"elapsed":80647,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“‚ DEV SÃ–ZLÃœK YÃœKLENÄ°YOR: /content/drive/MyDrive/FSTurk/CLEANED_ROOTS_V2.json...\n","âœ… SÃ–ZLÃœK YÃœKLENDÄ°! Toplam Kelime: 196,428\n","============================================================\n","LEXICON LOADED\n","============================================================\n","Nouns: 143884 words\n","Verbs: 17377 words (infinitives â†’ roots extracted)\n","Adjectives: 28025 words\n","Pronouns: 186 words\n","Adverbs: 3894 words\n","Conjunctions: 185 words\n","Postpositions: 2826 words\n","Proper Nouns: 0 words\n","\n","Verb root examples:\n","  aban â†’ aban\n","  abanacaÄŸ â†’ abanacaÄŸ\n","  aband â†’ aband\n","  abanozlaÅŸ â†’ abanozlaÅŸ\n","  abar â†’ abar\n","\n","============================================================\n","DEBUG: Testing verb compositions\n","============================================================\n","\n","'gel' + verb_imperative:\n","  âœ“ gel+NOUN+IMP+2SG\n","  âœ“ gel+VERB+IMP+2SG\n","\n","'gelsin' + verb_imperative:\n","  âœ“ gel+NOUN+IMP+3SG\n","  âœ“ gel+VERB+IMP+3SG\n","\n","============================================================\n","SINGLE WORD ANALYSIS\n","============================================================\n","\n","gel:\n","  gel+NOUN\n","  gel+NOUN+IMP+2SG\n","  gel+VERB\n","  gel+VERB+IMP+2SG\n","\n","gelin:\n","  gel+NOUN+IMP+2PL\n","  gel+NOUN+POSS.2SG\n","  gel+VERB+IMP+2PL\n","  gel+VERB+POSS.2SG\n","  gel+VERB+VOICE.in\n","  ... and 1 more analyses\n","\n","gelsin:\n","  gel+NOUN+IMP+3SG\n","  gel+VERB+IMP+3SG\n","\n","gelsem:\n","  gel+NOUN+COND+1SG\n","  gel+VERB+COND+1SG\n","  gelsem+VERB\n","  gelsem+VERB+IMP+2SG\n","\n","geleyim:\n","  gel+NOUN+OPT+1SG\n","  gel+VERB+OPT+1SG\n","  geley+VERB+DER.im\n","  geley+VERB+POSS.1SG\n","\n","gelmeli:\n","  gel+NOUN+NEC+3SG\n","  gel+VERB+NEC+3SG\n","  gelme+ADJ+DER.li\n","\n","yazmalÄ±yÄ±m:\n","  yaz+ADJ+NEC+1SG\n","  yaz+NOUN+NEC+1SG\n","  yaz+VERB+NEC+1SG\n","\n","okusana:\n","  oku+NOUN+OPT+2SG+EMPH\n","  oku+VERB+OPT+2SG+EMPH\n","  okusan+VERB+DAT\n","  okusan+VERB+OPT+3SG\n","\n","gÃ¶rseler:\n","  gÃ¶r+NOUN+COND+3PL\n","  gÃ¶r+NOUN+COP.COND+3PL\n","  gÃ¶r+VERB+COND+3PL\n","  gÃ¶r+VERB+COP.COND+3PL\n","  gÃ¶rsel+ADJ+AOR+3SG\n","  ... and 3 more analyses\n","\n","okudum:\n","  oku+NOUN+PAST+1SG\n","  oku+VERB+PAST+1SG\n","  okud+NOUN+POSS.1SG\n","  okud+VERB+DER.im\n","  okud+VERB+POSS.1SG\n","  ... and 2 more analyses\n","\n","gelebilecek:\n","  gel+NOUN+ABIL+FUT+3SG\n","  gel+VERB+ABIL+FUT+3SG\n","  geleb+VERB+VOICE.il+FUT+3SG\n","\n","geliyorum:\n","  gel+NOUN+PRES.CONT+1SG\n","  gel+VERB+PRES.CONT+1SG\n","\n","kitaplardan:\n","  kitap+NOUN+PL+ABL\n","  kitap+VERB+PL+ABL\n","\n","kalemlik:\n","  kalem+ADJ+DER.lik\n","  kalem+NOUN+DER.lik\n","\n","evdekiler:\n","  ev+NOUN+LOC+KI+PL\n","  ev+VERB+LOC+KI+PL\n","  evd+NOUN+DAT+KI+PL\n","  evde+NOUN+DER.ki+PL\n","  evde+NOUN+KI+PL\n","  ... and 1 more analyses\n","\n","============================================================\n","CONTEXT-AWARE ANALYSIS (Viterbi)\n","============================================================\n","\n","Sentence: 'yÃ¼zÃ¼ gÃ¼zel'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," yÃ¼zÃ¼            | NOUN   | yÃ¼z+NOUN+ACC\n"," gÃ¼zel           | VERB   | gÃ¼zel+VERB\n","\n","Sentence: 'denizde yÃ¼z'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," denizde         | NOUN   | deniz+NOUN+LOC\n"," yÃ¼z             | VERB   | yÃ¼z+VERB\n","\n","Sentence: 'bana gÃ¼l'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," bana            | PRON   | bana+PRON\n"," gÃ¼l             | VERB   | gÃ¼l+VERB\n","\n","Sentence: 'kÄ±rmÄ±zÄ± gÃ¼l'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," kÄ±rmÄ±zÄ±         | ADJ    | kÄ±rmÄ±zÄ±+ADJ\n"," gÃ¼l             | NOUN   | gÃ¼l+NOUN\n","\n","Sentence: 'okula git'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," okula           | NOUN   | ok+NOUN+POSS.3SG+INS\n"," git             | VERB   | git+VERB\n","\n","Sentence: 'gÃ¼zel bir ev'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," gÃ¼zel           | ADJ    | gÃ¼zel+ADJ\n"," bir             | NOUN   | bir+NOUN\n"," ev              | VERB   | ev+VERB\n","\n","Sentence: 'evde misin'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," evde            | VERB   | ev+VERB+LOC+2SG\n"," misin           | QUES   | mi+QUES+2SG\n","\n","Sentence: 'kitap okumayÄ± severim'\n"," Word            | Tag    | Selected Analysis\n","--------------------------------------------------\n"," kitap           | NOUN   | kitap+NOUN\n"," okumayÄ±         | NOUN   | oku+VERB+DER.ma+ACC\n"," severim         | VERB   | sev+VERB+AOR+1SG\n"]}],"source":["import pynini\n","import json\n","import math\n","from collections import defaultdict\n","\n","# ===== LOAD LEXICON FROM JSON =====\n","# ===== LOAD LEXICON WITH DEBUGGING =====\n","import os\n","\n","LEXICON_FILE = '/content/drive/MyDrive/FSTurk/CLEANED_ROOTS_V2.json'\n","\n","def load_lexicon():\n","    print(f\"ðŸ“‚ DEV SÃ–ZLÃœK YÃœKLENÄ°YOR: {LEXICON_FILE}...\")\n","\n","    if not os.path.exists(LEXICON_FILE):\n","        print(f\"âŒ HATA: '{LEXICON_FILE}' bulunamadÄ±! Drive'daki dosya yolunu/ismini kontrol et.\")\n","        return {}\n","\n","    with open(LEXICON_FILE, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    # Ä°statistikler\n","    total = sum(len(v) for v in data.values())\n","    print(f\"âœ… SÃ–ZLÃœK YÃœKLENDÄ°! Toplam Kelime: {total:,}\")\n","\n","    # FSTurk kodunun beklediÄŸi formata (kÃ¼Ã§Ã¼k harf key'lere) Ã§eviriyoruz:\n","    normalized_data = {\n","        'nouns': data.get('NOUN', []),\n","        'verbs': data.get('VERB', []),\n","        'adjectives': data.get('ADJ', []),\n","        'pronouns': data.get('PRON', []),\n","        'adverbs': data.get('ADV', []),\n","        'conjunctions': data.get('CONJ', []),\n","        'postpositions': data.get('ADP', []),\n","        'proper_nouns': data.get('PROPN', []),\n","        'numerals': data.get('NUM', []),\n","        'interjections': []\n","    }\n","\n","    return normalized_data\n","\n","# Lexicon deÄŸiÅŸkenini gÃ¼ncelle\n","lexicon = load_lexicon()\n","\n","# ===== SAFETY BLOCK: FORCE CRITICAL ROOTS =====\n","for word in [\"yaz\", \"sev\", \"gÃ¶r\", \"yap\", \"Ã§Ä±k\", \"yÃ¼z\", \"gÃ¼l\", \"sÄ±k\", \"gÃ¼lÃ¼mse\"]:\n","    if word not in lexicon['verbs']: lexicon['verbs'].append(word)\n","\n","for word in [\"deniz\", \"yÃ¼z\", \"kalem\", \"gÃ¼l\", \"ev\", \"kardeÅŸ\", \"boya\"]:\n","    if word not in lexicon['nouns']: lexicon['nouns'].append(word)\n","\n","for word in [\"kÄ±rmÄ±zÄ±\", \"mavi\", \"gÃ¼zel\", \"bir\", \"Ã§ok\"]:\n","    if word not in lexicon['adjectives']: lexicon['adjectives'].append(word)\n","\n","# Fix for \"of\" (Interjection)\n","if \"interjections\" not in lexicon: lexicon[\"interjections\"] = []\n","if \"of\" not in lexicon[\"interjections\"]: lexicon[\"interjections\"].append(\"of\")\n","\n","# ===== HELPER FUNCTIONS =====\n","def extract_verb_root(verb_infinitive):\n","    \"\"\"Extract verb root from infinitive form (remove -mak/-mek).\"\"\"\n","    if verb_infinitive.endswith('mak'):\n","        return verb_infinitive[:-3]\n","    elif verb_infinitive.endswith('mek'):\n","        return verb_infinitive[:-3]\n","    else:\n","        # If it doesn't end with -mak/-mek, return as is (might be already a root)\n","        return verb_infinitive\n","\n","def create_alternating_roots(words, tag):\n","    \"\"\"\n","    Create FST roots with consonant softening (Ã¼nsÃ¼z yumuÅŸamasÄ±).\n","    pâ†’b, Ã§â†’c, tâ†’d, kâ†’g/ÄŸ when followed by vowel-initial suffix.\n","    \"\"\"\n","    roots = []\n","\n","    for word in words:\n","        # Original form (before consonant-initial suffixes)\n","        roots.append(pynini.cross(word, f\"{word}+{tag}\"))\n","\n","        # Check if word ends with p, Ã§, t, k (sert Ã¼nsÃ¼z)\n","        if len(word) > 1 and word[-1] in ['p', 'Ã§', 't', 'k']:\n","            # Get the stem without final consonant\n","            stem = word[:-1]\n","            final = word[-1]\n","\n","            # Determine softened form (yumuÅŸak Ã¼nsÃ¼z)\n","            if final == 'p':\n","                softened = stem + 'b'\n","            elif final == 'Ã§':\n","                softened = stem + 'c'\n","            elif final == 't':\n","                softened = stem + 'd'\n","            elif final == 'k':\n","                # k â†’ ÄŸ after vowels, k â†’ g after consonants\n","                if len(stem) > 0 and stem[-1] in 'aeÄ±ioÃ¶uÃ¼':\n","                    softened = stem + 'ÄŸ'\n","                else:\n","                    softened = stem + 'g'\n","\n","            # Add softened form (before vowel-initial suffixes)\n","            roots.append(pynini.cross(softened, f\"{word}+{tag}\"))\n","\n","    return pynini.union(*roots) if roots else pynini.cross(\"\", \"\")\n","\n","# ===== ROOTS BY LEXICAL CATEGORY =====\n","\n","# Nouns (Ad) - with consonant softening\n","noun_roots = create_alternating_roots(\n","    lexicon.get('nouns', []),\n","    'NOUN'\n",") if lexicon.get('nouns') else pynini.cross(\"\", \"\")\n","\n","# Adjectives (SÄ±fat) - with consonant softening\n","adj_roots = create_alternating_roots(\n","    lexicon.get('adjectives', []),\n","    'ADJ'\n",") if lexicon.get('adjectives') else pynini.cross(\"\", \"\")\n","\n","# Verbs (Fiil) - extract roots and apply consonant softening\n","verb_root_list = [extract_verb_root(word) for word in lexicon.get('verbs', [])]\n","verb_roots = create_alternating_roots(\n","    verb_root_list,\n","    'VERB'\n",") if verb_root_list else pynini.cross(\"\", \"\")\n","\n","# Pronouns (Zamir) - usually don't undergo consonant softening, but include for completeness\n","pronoun_roots = pynini.union(*[\n","    pynini.cross(word, f\"{word}+PRON\")\n","    for word in lexicon.get('pronouns', [])\n","]) if lexicon.get('pronouns') else pynini.cross(\"\", \"\")\n","\n","irregular_pronouns = pynini.union(\n","    pynini.cross(\"bana\", \"ben+PRON+DAT\"),\n","    pynini.cross(\"sana\", \"sen+PRON+DAT\")\n",")\n","pronoun_roots = pynini.union(pronoun_roots, irregular_pronouns)\n","\n","# Adverbs (Zarf) - no inflection\n","adverb_roots = pynini.union(*[\n","    pynini.cross(word, f\"{word}+ADV\")\n","    for word in lexicon.get('adverbs', [])\n","]) if lexicon.get('adverbs') else pynini.cross(\"\", \"\")\n","\n","# Postpositions (Edat) - no inflection\n","postposition_roots = pynini.union(*[\n","    pynini.cross(word, f\"{word}+POSTP\")\n","    for word in lexicon.get('postpositions', [])\n","]) if lexicon.get('postpositions') else pynini.cross(\"\", \"\")\n","\n","# Interjections (Ãœnlem) - no inflection\n","interjection_roots = pynini.union(*[\n","    pynini.cross(word, f\"{word}+INTERJ\")\n","    for word in lexicon.get('interjections', [])\n","]) if lexicon.get('interjections') else pynini.cross(\"\", \"\")\n","\n","# Conjunctions (BaÄŸlaÃ§) - no inflection\n","conjunction_roots = pynini.union(*[\n","    pynini.cross(word, f\"{word}+CONJ\")\n","    for word in lexicon.get('conjunctions', [])\n","]) if lexicon.get('conjunctions') else pynini.cross(\"\", \"\")\n","\n","# Proper Nouns (Ã–zel Ä°simler) - with consonant softening\n","proper_noun_roots = create_alternating_roots(\n","    lexicon.get('proper_nouns', []),\n","    'PROPN'\n",") if lexicon.get('proper_nouns') else pynini.cross(\"\", \"\")\n","\n","# Question Particles\n","question_particles = pynini.union(\n","    pynini.cross(\"mi\", \"mi+QUES\"),\n","    pynini.cross(\"mÄ±\", \"mÄ±+QUES\"),\n","    pynini.cross(\"mu\", \"mu+QUES\"),\n","    pynini.cross(\"mÃ¼\", \"mÃ¼+QUES\"),\n","    pynini.cross(\"misin\", \"mi+QUES+2SG\"),\n","    pynini.cross(\"mÄ±sÄ±n\", \"mÄ±+QUES+2SG\"),\n","    pynini.cross(\"musun\", \"mu+QUES+2SG\"),\n","    pynini.cross(\"mÃ¼sÃ¼n\", \"mÃ¼+QUES+2SG\"),\n","    pynini.cross(\"miyim\", \"mi+QUES+1SG\"),\n","    pynini.cross(\"mÄ±yÄ±m\", \"mÄ±+QUES+1SG\"),\n","    pynini.cross(\"muyum\", \"mu+QUES+1SG\"),\n","    pynini.cross(\"mÃ¼yÃ¼m\", \"mÃ¼+QUES+1SG\"),\n","    pynini.cross(\"miyiz\", \"mi+QUES+1PL\"),\n","    pynini.cross(\"mÄ±yÄ±z\", \"mÄ±+QUES+1PL\"),\n","    pynini.cross(\"muyuz\", \"mu+QUES+1PL\"),\n","    pynini.cross(\"mÃ¼yÃ¼z\", \"mÃ¼+QUES+1PL\"),\n","    pynini.cross(\"misiniz\", \"mi+QUES+2PL\"),\n","    pynini.cross(\"mÄ±sÄ±nÄ±z\", \"mÄ±+QUES+2PL\"),\n","    pynini.cross(\"musunuz\", \"mu+QUES+2PL\"),\n","    pynini.cross(\"mÃ¼sÃ¼nÃ¼z\", \"mÃ¼+QUES+2PL\")\n",")\n","\n","# ===== DERIVATIONAL SUFFIXES (YAPIM EKLERÄ°) =====\n","\n","# A) NOUN-TO-NOUN (Ä°simden Ä°sim)\n","# Includes -lik, -ci, -li, -siz, -cil, -das, -gil\n","deriv_n_n = pynini.union(\n","    # -lik (yer, alet, meslek)\n","    pynini.cross(\"lÄ±k\", \"+DER.lik\"), pynini.cross(\"lik\", \"+DER.lik\"),\n","    pynini.cross(\"luk\", \"+DER.luk\"), pynini.cross(\"lÃ¼k\", \"+DER.lÃ¼k\"),\n","\n","    # -ci (meslek, alÄ±ÅŸkanlÄ±k)\n","    pynini.cross(\"cÄ±\", \"+DER.ci\"), pynini.cross(\"ci\", \"+DER.ci\"),\n","    pynini.cross(\"cu\", \"+DER.ci\"), pynini.cross(\"cÃ¼\", \"+DER.ci\"),\n","    pynini.cross(\"Ã§Ä±\", \"+DER.ci\"), pynini.cross(\"Ã§i\", \"+DER.ci\"),\n","\n","    # -li (bulundurma)\n","    pynini.cross(\"lÄ±\", \"+DER.li\"), pynini.cross(\"li\", \"+DER.li\"),\n","    pynini.cross(\"lu\", \"+DER.li\"), pynini.cross(\"lÃ¼\", \"+DER.li\"),\n","\n","    # -siz (yoksunluk)\n","    pynini.cross(\"sÄ±z\", \"+DER.siz\"), pynini.cross(\"siz\", \"+DER.siz\"),\n","    pynini.cross(\"suz\", \"+DER.siz\"), pynini.cross(\"sÃ¼z\", \"+DER.siz\"),\n","\n","    # -ki (aitlik - usually attaches to Locative or Time nouns)\n","    pynini.cross(\"ki\", \"+DER.ki\"), pynini.cross(\"kÃ¼\", \"+DER.ki\"),\n","\n","    # -cE (dil, kÃ¼Ã§Ã¼ltme)\n","    pynini.cross(\"ca\", \"+DER.ce\"), pynini.cross(\"ce\", \"+DER.ce\"),\n","    pynini.cross(\"Ã§a\", \"+DER.ce\"), pynini.cross(\"Ã§e\", \"+DER.ce\"),\n","\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# B) VERB-TO-NOUN (Fiilden Ä°sim)\n","# Includes mastar (-mak), isim-fiil (-ma), sÄ±fat-fiil (-an, -asi, -mez...), -gi, -gin\n","deriv_v_n = pynini.union(\n","    # -mak (Mastar)\n","    pynini.cross(\"mak\", \"+DER.mak\"), pynini.cross(\"mek\", \"+DER.mak\"),\n","\n","    # -ma (Ä°sim-Fiil/Olumsuzluk deÄŸil) - Disambiguation Needed later\n","    pynini.cross(\"ma\", \"+DER.ma\"), pynini.cross(\"me\", \"+DER.ma\"),\n","\n","    # -iÅŸ (KalÄ±cÄ± isim)\n","    pynini.cross(\"Ä±ÅŸ\", \"+DER.iÅŸ\"), pynini.cross(\"iÅŸ\", \"+DER.iÅŸ\"),\n","    pynini.cross(\"uÅŸ\", \"+DER.iÅŸ\"), pynini.cross(\"Ã¼ÅŸ\", \"+DER.iÅŸ\"),\n","\n","    # -im (Fiilden isim)\n","    pynini.cross(\"Ä±m\", \"+DER.im\"), pynini.cross(\"im\", \"+DER.im\"),\n","    pynini.cross(\"um\", \"+DER.im\"), pynini.cross(\"Ã¼m\", \"+DER.im\"),\n","\n","    # -gin (SÄ±fat/Ä°sim)\n","    pynini.cross(\"gÄ±n\", \"+DER.gin\"), pynini.cross(\"gin\", \"+DER.gin\"),\n","    pynini.cross(\"gun\", \"+DER.gin\"), pynini.cross(\"gÃ¼n\", \"+DER.gin\"),\n","    pynini.cross(\"kÄ±n\", \"+DER.gin\"), pynini.cross(\"kin\", \"+DER.gin\"),\n","\n","    # -ici (Fail)\n","    pynini.cross(\"Ä±cÄ±\", \"+DER.ici\"), pynini.cross(\"ici\", \"+DER.ici\"),\n","    pynini.cross(\"ucu\", \"+DER.ici\"), pynini.cross(\"Ã¼cÃ¼\", \"+DER.ici\"),\n","\n","    # -an (SÄ±fat Fiil)\n","    pynini.cross(\"an\", \"+DER.an\"), pynini.cross(\"en\", \"+DER.an\"),\n","\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# C) NOUN-TO-VERB (Ä°simden Fiil)\n","deriv_n_v = pynini.union(\n","    # -le (Ä°simden fiil)\n","    pynini.cross(\"la\", \"+DER.la\"), pynini.cross(\"le\", \"+DER.la\"),\n","\n","    # -len (DÃ¶nÃ¼ÅŸlÃ¼)\n","    pynini.cross(\"lan\", \"+DER.lan\"), pynini.cross(\"len\", \"+DER.lan\"),\n","\n","    # -leÅŸ (OluÅŸ)\n","    pynini.cross(\"laÅŸ\", \"+DER.laÅŸ\"), pynini.cross(\"leÅŸ\", \"+DER.laÅŸ\"),\n","\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# D) VERB-TO-VERB (Fiilden Fiil / Ã‡atÄ±)\n","# Note: These usually go BEFORE the tense markers.\n","deriv_v_v = pynini.union(\n","    # -t (Ettirgen)\n","    pynini.cross(\"t\", \"+VOICE.t\"),\n","\n","    # -dir (Ettirgen)\n","    pynini.cross(\"dÄ±r\", \"+VOICE.dir\"), pynini.cross(\"dir\", \"+VOICE.dir\"),\n","    pynini.cross(\"dur\", \"+VOICE.dir\"), pynini.cross(\"dÃ¼r\", \"+VOICE.dir\"),\n","    pynini.cross(\"tÄ±r\", \"+VOICE.dir\"), pynini.cross(\"tir\", \"+VOICE.dir\"),\n","\n","    # -il (Edilgen)\n","    pynini.cross(\"Ä±l\", \"+VOICE.il\"), pynini.cross(\"il\", \"+VOICE.il\"),\n","    pynini.cross(\"ul\", \"+VOICE.il\"), pynini.cross(\"Ã¼l\", \"+VOICE.il\"),\n","\n","    # -n (DÃ¶nÃ¼ÅŸlÃ¼)\n","    pynini.cross(\"Ä±n\", \"+VOICE.in\"), pynini.cross(\"in\", \"+VOICE.in\"),\n","    pynini.cross(\"n\", \"+VOICE.in\"),\n","\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# ===== NOUN/ADJECTIVE MORPHOLOGY =====\n","# Include proper nouns that can take case markers\n","nominal_roots = pynini.union(noun_roots, adj_roots, pronoun_roots, proper_noun_roots)\n","\n","# 1. Update Nominal Path (Noun -> Noun Deriv -> Plural...)\n","nominal_derived = (nominal_roots + deriv_n_n).optimize()\n","# Note: We also need to allow Verbs to become Nouns (e.g. okumayÄ±)\n","verb_to_noun_derived = (verb_roots + deriv_v_v + deriv_v_n).optimize()\n","\n","# Combine both as valid nominal bases\n","nominal_base_all = pynini.union(nominal_derived, verb_to_noun_derived)\n","\n","plural = pynini.union(\n","    pynini.cross(\"lar\", \"+PL\"),\n","    pynini.cross(\"ler\", \"+PL\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# Re-define plural/possessive/case attached to nominal_base_all\n","nominal_pl = nominal_base_all + plural\n","\n","possessive = pynini.union(\n","    # Post-Vowel (su, si...)\n","    pynini.cross(\"si\", \"+POSS.3SG\"), pynini.cross(\"sÄ±\", \"+POSS.3SG\"),\n","    pynini.cross(\"su\", \"+POSS.3SG\"), pynini.cross(\"sÃ¼\", \"+POSS.3SG\"),\n","    # Post-Consonant (u, Ã¼...) - THIS WAS MISSING\n","    pynini.cross(\"i\", \"+POSS.3SG\"), pynini.cross(\"Ä±\", \"+POSS.3SG\"),\n","    pynini.cross(\"u\", \"+POSS.3SG\"), pynini.cross(\"Ã¼\", \"+POSS.3SG\"),\n","\n","    pynini.cross(\"imiz\", \"+POSS.1PL\"), pynini.cross(\"Ä±mÄ±z\", \"+POSS.1PL\"),\n","    pynini.cross(\"umuz\", \"+POSS.1PL\"), pynini.cross(\"Ã¼mÃ¼z\", \"+POSS.1PL\"),\n","    pynini.cross(\"iniz\", \"+POSS.2PL\"), pynini.cross(\"Ä±nÄ±z\", \"+POSS.2PL\"),\n","    pynini.cross(\"unuz\", \"+POSS.2PL\"), pynini.cross(\"Ã¼nÃ¼z\", \"+POSS.2PL\"),\n","    pynini.cross(\"leri\", \"+POSS.3PL\"), pynini.cross(\"larÄ±\", \"+POSS.3PL\"),\n","    pynini.cross(\"im\", \"+POSS.1SG\"), pynini.cross(\"Ä±m\", \"+POSS.1SG\"),\n","    pynini.cross(\"um\", \"+POSS.1SG\"), pynini.cross(\"Ã¼m\", \"+POSS.1SG\"),\n","    pynini.cross(\"in\", \"+POSS.2SG\"), pynini.cross(\"Ä±n\", \"+POSS.2SG\"),\n","    pynini.cross(\"un\", \"+POSS.2SG\"), pynini.cross(\"Ã¼n\", \"+POSS.2SG\")\n",")\n","\n","case_after_poss = pynini.union(\n","    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n","    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n","    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n","    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n","    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n","    pynini.cross(\"na\", \"+DAT\"), pynini.cross(\"ne\", \"+DAT\"),\n","    pynini.cross(\"yÄ±\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n","    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yÃ¼\", \"+ACC\"),\n","    pynini.cross(\"nÄ±\", \"+ACC\"), pynini.cross(\"ni\", \"+ACC\"),\n","    pynini.cross(\"nu\", \"+ACC\"), pynini.cross(\"nÃ¼\", \"+ACC\"),\n","    pynini.cross(\"nÄ±n\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n","    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nÃ¼n\", \"+GEN\"),\n","    # FIX: Added le/la for cases like 'kardeÅŸim-le'\n","    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n","    pynini.cross(\"la\", \"+INS\"), pynini.cross(\"le\", \"+INS\"),\n","    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# FIX: CASE (Added post-consonant Genitive in, Ä±n, un, Ã¼n)\n","case_no_poss = pynini.union(\n","    # Genitive\n","    pynini.cross(\"nÄ±n\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n","    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nÃ¼n\", \"+GEN\"),\n","    pynini.cross(\"Ä±n\", \"+GEN\"), pynini.cross(\"in\", \"+GEN\"), # Added\n","    pynini.cross(\"un\", \"+GEN\"), pynini.cross(\"Ã¼n\", \"+GEN\"), # Added\n","\n","    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n","    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n","    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n","    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n","    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n","    pynini.cross(\"a\", \"+DAT\"), pynini.cross(\"e\", \"+DAT\"),\n","    pynini.cross(\"yÄ±\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n","    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yÃ¼\", \"+ACC\"),\n","    pynini.cross(\"Ä±\", \"+ACC\"), pynini.cross(\"i\", \"+ACC\"),\n","    pynini.cross(\"u\", \"+ACC\"), pynini.cross(\"Ã¼\", \"+ACC\"),\n","    pynini.cross(\"la\", \"+INS\"), pynini.cross(\"le\", \"+INS\"),\n","    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n","    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","ki_suffix = pynini.union(\n","    pynini.cross(\"ki\", \"+KI\"),\n","    pynini.cross(\"kÃ¼\", \"+KI\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","plural_after_ki = pynini.union(\n","    pynini.cross(\"ler\", \"+PL\"),\n","    pynini.cross(\"lar\", \"+PL\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","possessive_path = nominal_pl + possessive + case_after_poss + ki_suffix + plural_after_ki\n","\n","case_no_poss = pynini.union(\n","    pynini.cross(\"larÄ±n\", \"+GEN\"), pynini.cross(\"lerin\", \"+GEN\"),\n","    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n","    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n","    pynini.cross(\"nÄ±n\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n","    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nÃ¼n\", \"+GEN\"),\n","    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n","    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n","    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n","    pynini.cross(\"a\", \"+DAT\"), pynini.cross(\"e\", \"+DAT\"),\n","    pynini.cross(\"yÄ±\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n","    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yÃ¼\", \"+ACC\"),\n","    pynini.cross(\"Ä±\", \"+ACC\"), pynini.cross(\"i\", \"+ACC\"),\n","    pynini.cross(\"u\", \"+ACC\"), pynini.cross(\"Ã¼\", \"+ACC\"),\n","    pynini.cross(\"la\", \"+INS\"), pynini.cross(\"le\", \"+INS\"),\n","    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n","    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","ki_suffix_case = pynini.union(\n","    pynini.cross(\"ki\", \"+KI\"),\n","    pynini.cross(\"kÃ¼\", \"+KI\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","plural_after_ki_case = pynini.union(\n","    pynini.cross(\"ler\", \"+PL\"),\n","    pynini.cross(\"lar\", \"+PL\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","case_only_path = nominal_pl + case_no_poss + ki_suffix_case + plural_after_ki_case\n","\n","nominal_final_base = pynini.union(possessive_path, case_only_path).optimize()\n","\n","# 2. Update Verb Path (Noun -> Verb OR Verb -> Verb)\n","noun_to_verb_stem = (nominal_roots + deriv_n_v).optimize()\n","# YapÄ±m eklerini (Ã§atÄ±larÄ±) 0 ile 3 kez arasÄ±nda dÃ¶ngÃ¼ye sok\n","verb_to_verb_stem = (verb_roots + pynini.closure(deriv_v_v, 0, 3)).optimize()\n","verb_stems_all = pynini.union(noun_to_verb_stem, verb_to_verb_stem)\n","\n","copula = pynini.union(\n","    pynini.cross(\"ydi\", \"+COP.PAST\"), pynini.cross(\"ydÄ±\", \"+COP.PAST\"),\n","    pynini.cross(\"ydu\", \"+COP.PAST\"), pynini.cross(\"ydÃ¼\", \"+COP.PAST\"),\n","    pynini.cross(\"ymÄ±ÅŸ\", \"+COP.EVID\"), pynini.cross(\"ymiÅŸ\", \"+COP.EVID\"),\n","    pynini.cross(\"ymuÅŸ\", \"+COP.EVID\"), pynini.cross(\"ymÃ¼ÅŸ\", \"+COP.EVID\"),\n","    pynini.cross(\"yse\", \"+COP.COND\"), pynini.cross(\"ysa\", \"+COP.COND\"),\n","    pynini.cross(\"di\", \"+COP.PAST\"), pynini.cross(\"dÄ±\", \"+COP.PAST\"),\n","    pynini.cross(\"du\", \"+COP.PAST\"), pynini.cross(\"dÃ¼\", \"+COP.PAST\"),\n","    pynini.cross(\"ti\", \"+COP.PAST\"), pynini.cross(\"tÄ±\", \"+COP.PAST\"),\n","    pynini.cross(\"tu\", \"+COP.PAST\"), pynini.cross(\"tÃ¼\", \"+COP.PAST\"),\n","    pynini.cross(\"miÅŸ\", \"+COP.EVID\"), pynini.cross(\"mÄ±ÅŸ\", \"+COP.EVID\"),\n","    pynini.cross(\"muÅŸ\", \"+COP.EVID\"), pynini.cross(\"mÃ¼ÅŸ\", \"+COP.EVID\"),\n","    pynini.cross(\"se\", \"+COP.COND\"), pynini.cross(\"sa\", \"+COP.COND\"),\n","    pynini.cross(\"dir\", \"+COP.PRES\"), pynini.cross(\"dÄ±r\", \"+COP.PRES\"),\n","    pynini.cross(\"dur\", \"+COP.PRES\"), pynini.cross(\"dÃ¼r\", \"+COP.PRES\"),\n","    pynini.cross(\"tir\", \"+COP.PRES\"), pynini.cross(\"tÄ±r\", \"+COP.PRES\"),\n","    pynini.cross(\"tur\", \"+COP.PRES\"), pynini.cross(\"tÃ¼r\", \"+COP.PRES\")\n",")\n","\n","person = pynini.union(\n","    pynini.cross(\"im\", \"+1SG\"), pynini.cross(\"Ä±m\", \"+1SG\"),\n","    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"Ã¼m\", \"+1SG\"),\n","    pynini.cross(\"in\", \"+2SG\"), pynini.cross(\"Ä±n\", \"+2SG\"),\n","    pynini.cross(\"un\", \"+2SG\"), pynini.cross(\"Ã¼n\", \"+2SG\"),\n","    pynini.cross(\"Ä±z\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n","    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"Ã¼z\", \"+1PL\"),\n","    pynini.cross(\"nÄ±z\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n","    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nÃ¼z\", \"+2PL\"),\n","    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","nominal_with_cop = copula + person\n","nominal_without_cop = pynini.cross(\"\", \"\")\n","nominal_complete = nominal_final_base + pynini.union(nominal_with_cop, nominal_without_cop).optimize()\n","\n","# ===== VERB MORPHOLOGY =====\n","\n","# Voice, Ability, Negation (all optional)\n","voice = pynini.union(\n","    pynini.cross(\"Ä±l\", \"+PASS\"), pynini.cross(\"il\", \"+PASS\"),\n","    pynini.cross(\"ul\", \"+PASS\"), pynini.cross(\"Ã¼l\", \"+PASS\"),\n","    pynini.cross(\"Ä±n\", \"+REFL\"), pynini.cross(\"in\", \"+REFL\"),\n","    pynini.cross(\"un\", \"+REFL\"), pynini.cross(\"Ã¼n\", \"+REFL\"),\n","    pynini.cross(\"lan\", \"+REFL\"), pynini.cross(\"len\", \"+REFL\"),\n","    pynini.cross(\"Ä±ÅŸ\", \"+RECIP\"), pynini.cross(\"iÅŸ\", \"+RECIP\"),\n","    pynini.cross(\"uÅŸ\", \"+RECIP\"), pynini.cross(\"Ã¼ÅŸ\", \"+RECIP\"),\n","    pynini.cross(\"t\", \"+CAUS\"), pynini.cross(\"d\", \"+CAUS\"),\n","    pynini.cross(\"dÄ±r\", \"+CAUS\"), pynini.cross(\"dir\", \"+CAUS\"),\n","    pynini.cross(\"dur\", \"+CAUS\"), pynini.cross(\"dÃ¼r\", \"+CAUS\"),\n","    pynini.cross(\"tÄ±r\", \"+CAUS\"), pynini.cross(\"tir\", \"+CAUS\"),\n","    pynini.cross(\"tur\", \"+CAUS\"), pynini.cross(\"tÃ¼r\", \"+CAUS\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","ability = pynini.union(\n","    pynini.cross(\"ebil\", \"+ABIL\"),\n","    pynini.cross(\"abil\", \"+ABIL\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","negation = pynini.union(\n","    pynini.cross(\"ma\", \"+NEG\"),\n","    pynini.cross(\"me\", \"+NEG\"),\n","    pynini.cross(\"\", \"\")\n",")\n","\n","# Bildirme Kipleri (Indicative)\n","indicative_tense = pynini.union(\n","    pynini.cross(\"iyor\", \"+PRES.CONT\"), pynini.cross(\"Ä±yor\", \"+PRES.CONT\"),\n","    pynini.cross(\"uyor\", \"+PRES.CONT\"), pynini.cross(\"Ã¼yor\", \"+PRES.CONT\"),\n","    pynini.cross(\"ecek\", \"+FUT\"), pynini.cross(\"acak\", \"+FUT\"),\n","    pynini.cross(\"Ä±r\", \"+AOR\"), pynini.cross(\"ir\", \"+AOR\"),\n","    pynini.cross(\"ur\", \"+AOR\"), pynini.cross(\"Ã¼r\", \"+AOR\"),\n","    pynini.cross(\"ar\", \"+AOR\"), pynini.cross(\"er\", \"+AOR\"),\n","    pynini.cross(\"r\", \"+AOR\"),\n","    pynini.cross(\"dÄ±\", \"+PAST\"), pynini.cross(\"di\", \"+PAST\"),\n","    pynini.cross(\"du\", \"+PAST\"), pynini.cross(\"dÃ¼\", \"+PAST\"),\n","    pynini.cross(\"tÄ±\", \"+PAST\"), pynini.cross(\"ti\", \"+PAST\"),\n","    pynini.cross(\"tu\", \"+PAST\"), pynini.cross(\"tÃ¼\", \"+PAST\"),\n","    pynini.cross(\"mÄ±ÅŸ\", \"+INFER\"), pynini.cross(\"miÅŸ\", \"+INFER\"),\n","    pynini.cross(\"muÅŸ\", \"+INFER\"), pynini.cross(\"mÃ¼ÅŸ\", \"+INFER\")\n",")\n","\n","indicative_person = pynini.union(\n","    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"Ã¼m\", \"+1SG\"),\n","    pynini.cross(\"Ä±m\", \"+1SG\"), pynini.cross(\"im\", \"+1SG\"),\n","    pynini.cross(\"m\", \"+1SG\"),\n","    pynini.cross(\"sun\", \"+2SG\"), pynini.cross(\"sÃ¼n\", \"+2SG\"),\n","    pynini.cross(\"sÄ±n\", \"+2SG\"), pynini.cross(\"sin\", \"+2SG\"),\n","    pynini.cross(\"n\", \"+2SG\"),\n","    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"Ã¼z\", \"+1PL\"),\n","    pynini.cross(\"Ä±z\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n","    pynini.cross(\"k\", \"+1PL\"),\n","    pynini.cross(\"sunuz\", \"+2PL\"), pynini.cross(\"sÃ¼nÃ¼z\", \"+2PL\"),\n","    pynini.cross(\"sÄ±nÄ±z\", \"+2PL\"), pynini.cross(\"siniz\", \"+2PL\"),\n","    pynini.cross(\"nÄ±z\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n","    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nÃ¼z\", \"+2PL\"),\n","    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n","    pynini.cross(\"\", \"+3SG\")\n",")\n","\n","# Dilek/Tasarlama Kipleri (Subjunctive/Optative)\n","\n","# Ä°stek Kipi (Optative)\n","optative_mood_person = pynini.union(\n","    pynini.cross(\"ayÄ±m\", \"+OPT+1SG\"), pynini.cross(\"eyim\", \"+OPT+1SG\"),\n","    pynini.cross(\"ayum\", \"+OPT+1SG\"), pynini.cross(\"eyÃ¼m\", \"+OPT+1SG\"),\n","    pynini.cross(\"asÄ±n\", \"+OPT+2SG\"), pynini.cross(\"esin\", \"+OPT+2SG\"),\n","    pynini.cross(\"asun\", \"+OPT+2SG\"), pynini.cross(\"esÃ¼n\", \"+OPT+2SG\"),\n","    pynini.cross(\"asana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"esene\", \"+OPT+2SG+EMPH\"),\n","    pynini.cross(\"sana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"sene\", \"+OPT+2SG+EMPH\"),\n","    pynini.cross(\"asan\", \"+OPT+2SG\"), pynini.cross(\"esen\", \"+OPT+2SG\"),\n","    pynini.cross(\"a\", \"+OPT+3SG\"), pynini.cross(\"e\", \"+OPT+3SG\"),\n","    pynini.cross(\"alÄ±m\", \"+OPT+1PL\"), pynini.cross(\"elim\", \"+OPT+1PL\"),\n","    pynini.cross(\"alum\", \"+OPT+1PL\"), pynini.cross(\"elÃ¼m\", \"+OPT+1PL\"),\n","    pynini.cross(\"asÄ±nÄ±z\", \"+OPT+2PL\"), pynini.cross(\"esiniz\", \"+OPT+2PL\"),\n","    pynini.cross(\"asunuz\", \"+OPT+2PL\"), pynini.cross(\"esÃ¼nÃ¼z\", \"+OPT+2PL\"),\n","    pynini.cross(\"alar\", \"+OPT+3PL\"), pynini.cross(\"eler\", \"+OPT+3PL\")\n",")\n","\n","# Dilek-KoÅŸul Kipi (Conditional)\n","conditional_mood_person = pynini.union(\n","    pynini.cross(\"sam\", \"+COND+1SG\"), pynini.cross(\"sem\", \"+COND+1SG\"),\n","    pynini.cross(\"san\", \"+COND+2SG\"), pynini.cross(\"sen\", \"+COND+2SG\"),\n","    pynini.cross(\"sa\", \"+COND+3SG\"), pynini.cross(\"se\", \"+COND+3SG\"),\n","    pynini.cross(\"sak\", \"+COND+1PL\"), pynini.cross(\"sek\", \"+COND+1PL\"),\n","    pynini.cross(\"sanÄ±z\", \"+COND+2PL\"), pynini.cross(\"seniz\", \"+COND+2PL\"),\n","    pynini.cross(\"sanuz\", \"+COND+2PL\"), pynini.cross(\"senÃ¼z\", \"+COND+2PL\"),\n","    pynini.cross(\"salar\", \"+COND+3PL\"), pynini.cross(\"seler\", \"+COND+3PL\")\n",")\n","\n","# ...\n","\n","\n","# Gereklilik Kipi (Necessitative)\n","necessitative_mood_person = pynini.union(\n","    pynini.cross(\"malÄ±yÄ±m\", \"+NEC+1SG\"), pynini.cross(\"meliyim\", \"+NEC+1SG\"),\n","    pynini.cross(\"malÄ±yum\", \"+NEC+1SG\"), pynini.cross(\"meliyÃ¼m\", \"+NEC+1SG\"),\n","    pynini.cross(\"malÄ±sÄ±n\", \"+NEC+2SG\"), pynini.cross(\"melisin\", \"+NEC+2SG\"),\n","    pynini.cross(\"malÄ±sun\", \"+NEC+2SG\"), pynini.cross(\"melisÃ¼n\", \"+NEC+2SG\"),\n","    pynini.cross(\"malÄ±\", \"+NEC+3SG\"), pynini.cross(\"meli\", \"+NEC+3SG\"),\n","    pynini.cross(\"malÄ±yÄ±z\", \"+NEC+1PL\"), pynini.cross(\"meliyiz\", \"+NEC+1PL\"),\n","    pynini.cross(\"malÄ±yuz\", \"+NEC+1PL\"), pynini.cross(\"meliyÃ¼z\", \"+NEC+1PL\"),\n","    pynini.cross(\"malÄ±sÄ±nÄ±z\", \"+NEC+2PL\"), pynini.cross(\"melisiniz\", \"+NEC+2PL\"),\n","    pynini.cross(\"malÄ±sunuz\", \"+NEC+2PL\"), pynini.cross(\"melisÃ¼nÃ¼z\", \"+NEC+2PL\"),\n","    pynini.cross(\"malÄ±lar\", \"+NEC+3PL\"), pynini.cross(\"meliler\", \"+NEC+3PL\")\n",")\n","\n","# Emir Kipi (Imperative)\n","imperative_mood_person = pynini.union(\n","    pynini.cross(\"sin\", \"+IMP+3SG\"), pynini.cross(\"sÄ±n\", \"+IMP+3SG\"),\n","    pynini.cross(\"sun\", \"+IMP+3SG\"), pynini.cross(\"sÃ¼n\", \"+IMP+3SG\"),\n","    pynini.cross(\"in\", \"+IMP+2PL\"), pynini.cross(\"Ä±n\", \"+IMP+2PL\"),\n","    pynini.cross(\"un\", \"+IMP+2PL\"), pynini.cross(\"Ã¼n\", \"+IMP+2PL\"),\n","    pynini.cross(\"iniz\", \"+IMP+2PL\"), pynini.cross(\"Ä±nÄ±z\", \"+IMP+2PL\"),\n","    pynini.cross(\"unuz\", \"+IMP+2PL\"), pynini.cross(\"Ã¼nÃ¼z\", \"+IMP+2PL\"),\n","    pynini.cross(\"sinler\", \"+IMP+3PL\"), pynini.cross(\"sÄ±nlar\", \"+IMP+3PL\"),\n","    pynini.cross(\"sunlar\", \"+IMP+3PL\"), pynini.cross(\"sÃ¼nler\", \"+IMP+3PL\")\n",")\n","\n","imperative_2sg_bare = pynini.cross(\"\", \"+IMP+2SG\")\n","\n","# Build verb structure\n","verb_base = verb_stems_all + ability + negation\n","\n","# Verb paths\n","verb_indicative = verb_base + indicative_tense + indicative_person\n","verb_optative = verb_base + optative_mood_person\n","verb_conditional = verb_base + conditional_mood_person\n","verb_necessitative = verb_base + necessitative_mood_person\n","verb_imperative = verb_base + pynini.union(imperative_mood_person, imperative_2sg_bare)\n","\n","# Combine all verb paths\n","verb_complete = pynini.union(\n","    verb_indicative,\n","    verb_optative,\n","    verb_conditional,\n","    verb_necessitative,\n","    verb_imperative\n",").optimize()\n","\n","# ===== FIX: PUNCTUATION SEPARATION =====\n","# Punctuation should be a separate graph, not appended to Nouns/Verbs\n","punctuation_roots = pynini.union(\n","    pynini.cross(\".\", \"PUNCT.period\"), # Tag is just the tag, no '+' prefix needed for standalone\n","    pynini.cross(\",\", \"PUNCT.comma\"),\n","    pynini.cross(\"?\", \"PUNCT.question\"),\n","    pynini.cross(\"!\", \"PUNCT.exclamation\"),\n","    pynini.cross(\":\", \"PUNCT.colon\"),\n","    pynini.cross(\";\", \"PUNCT.semicolon\")\n",")\n","\n","# ===== COMPLETE ANALYZER =====\n","simple_categories = pynini.union(\n","    adverb_roots,\n","    postposition_roots,\n","    interjection_roots,\n","    conjunction_roots,\n","    question_particles,\n","    punctuation_roots # Added here\n",")\n","\n","# REMOVED \"+ punctuation\" from these lines to stop hallucinating tags on commas\n","nominal_fst = nominal_complete.optimize()\n","verb_fst = verb_complete.optimize()\n","simple_fst = simple_categories.optimize()\n","\n","turkish_analyzer = pynini.union(nominal_fst, verb_fst, simple_fst).optimize()\n","\n","def analyze(word):\n","    \"\"\"\n","    Analyze a word. Tries exact match first, then lowercase match.\n","    This handles 'Ali' -> 'ali' lookup automatically.\n","    \"\"\"\n","    try:\n","        # 1. Try exact match (Good for specific abbreviations if you have them)\n","        lattice = pynini.compose(word, turkish_analyzer)\n","        if lattice.start() == pynini.NO_STATE_ID:\n","            # 2. If failed, try lowercase (Handles \"Ali\" -> \"ali\" in lexicon)\n","            lattice = pynini.compose(word.lower(), turkish_analyzer)\n","\n","        analyses = []\n","        seen = set()\n","        try:\n","            for path in lattice.paths().ostrings():\n","                if path not in seen:\n","                    analyses.append(path)\n","                    seen.add(path)\n","        except:\n","            pass\n","\n","        return sorted(analyses) if analyses else [f\"No analysis found for: {word}\"]\n","    except Exception as e:\n","        return [f\"Error: {str(e)}\"]\n","# ===== CONTEXT AWARENESS & DISAMBIGUATION =====\n","\n","class ContextAwareDisambiguator:\n","    def __init__(self):\n","        # Transition Matrix\n","        self.transitions = {\n","            'START': {'NOUN': 0.3, 'PRON': 0.3, 'ADJ': 0.15, 'VERB': 0.1, 'ADV': 0.1},\n","\n","            # Adjectives transition to Nouns or other Adjectives\n","            'ADJ': {'NOUN': 0.6, 'ADJ': 0.35, 'VERB': 0.05},\n","\n","            # Nouns transition to...\n","            'NOUN': {'VERB': 0.3, 'NOUN': 0.15, 'CONJ': 0.15, 'POSTP': 0.15, 'QUES': 0.2, 'PUNCT': 0.05},\n","\n","            # Pronouns transition to...\n","            'PRON': {'VERB': 0.4, 'NOUN': 0.1, 'CONJ': 0.2, 'POSTP': 0.2, 'QUES': 0.1},\n","\n","            # Conjunctions (de/da) transition to Verbs or Nouns\n","            'CONJ': {'NOUN': 0.3, 'VERB': 0.4, 'ADJ': 0.1, 'PRON': 0.1, 'ADV': 0.1},\n","\n","            # Question particles\n","            'QUES': {'PUNCT': 0.9, 'VERB': 0.1},\n","\n","            # Verbs (End of clause/sentence)\n","            'VERB': {'PUNCT': 0.6, 'CONJ': 0.1, 'QUES': 0.25, 'NOUN': 0.05},\n","\n","            'DEFAULT': {'NOUN': 0.2, 'VERB': 0.2}\n","        }\n","        self.tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'POSTP', 'CONJ', 'QUES']\n","\n","    def get_tag_from_analysis(self, analysis_str):\n","        if \"No analysis\" in analysis_str: return \"UNKNOWN\"\n","\n","        # Priority order\n","        if \"+QUES\" in analysis_str: return \"QUES\"\n","        if \"+CONJ\" in analysis_str: return \"CONJ\"\n","        if \"+POSTP\" in analysis_str: return \"POSTP\"\n","        if \"+PRON\" in analysis_str: return \"PRON\"\n","\n","        # FIX FOR 'DAKÄ°': -ki suffix creates Adjectives\n","        if \"+KI\" in analysis_str or \"+DER.ki\" in analysis_str:\n","            return \"ADJ\"\n","\n","        if \"+ADV\" in analysis_str: return \"ADV\"\n","\n","        # Derived Nouns (verb-to-noun)\n","        if \"+DER\" in analysis_str:\n","            if any(x in analysis_str for x in [\"+DER.ma\", \"+DER.mak\", \"+DER.lik\", \"+DER.iÅŸ\"]):\n","                return \"NOUN\"\n","\n","        if \"+ADJ\" in analysis_str: return \"ADJ\"\n","        if \"+VERB\" in analysis_str: return \"VERB\"\n","\n","        # Detached suffixes (like 'daki' analyzed as '+LOC+KI')\n","        if analysis_str.startswith(\"+\"):\n","            # If it contains KI, it's ADJ, otherwise likely NOUN suffix chain\n","            if \"KI\" in analysis_str: return \"ADJ\"\n","            return \"NOUN\"\n","\n","        return \"NOUN\"\n","\n","    def get_transition_prob(self, prev_tag, current_tag):\n","        if prev_tag in self.transitions:\n","            return self.transitions[prev_tag].get(current_tag, 0.001)\n","        return self.transitions['DEFAULT'].get(current_tag, 0.001)\n","\n","    def heuristic_weight(self, word, analysis, tag, position, sentence_len, next_token=None):\n","        score = 0.0\n","        word_lower = word.lower()\n","        is_capitalized = word[0].isupper()\n","\n","        # Rule 1: High-confidence Pronouns\n","        if word_lower in [\"ben\", \"sen\", \"o\", \"biz\", \"siz\", \"onlar\", \"bana\", \"sana\"]:\n","            if tag == \"PRON\": score += 5.0\n","            if tag == \"NOUN\": score -= 5.0\n","\n","        # Rule 2: \"Bir\" handling\n","        if word_lower == \"bir\":\n","            if tag == \"ADJ\": score += 2.0\n","            if tag == \"ADV\": score += 1.0\n","            if tag == \"NOUN\": score -= 1.0\n","\n","        # Rule 3: De/Da handling\n","        if word_lower in [\"de\", \"da\"]:\n","            if tag == \"CONJ\": score += 5.0\n","            if tag == \"NOUN\": score -= 5.0\n","\n","        # Rule 4: Contextual Verb vs Noun (Lookahead for Question)\n","        if next_token:\n","            next_is_question = next_token.lower().startswith((\"mi\", \"mÄ±\", \"mu\", \"mÃ¼\"))\n","            # MASSIVE BOOST to force verb interpretation before a question tag\n","            if next_is_question and tag == \"VERB\": score += 10.0\n","            if next_is_question and tag == \"NOUN\": score -= 10.0\n","\n","        # Rule 5: Strong Sentence End Preference\n","        if position == sentence_len - 1:\n","            if tag == \"VERB\": score += 3.0\n","            if tag == \"NOUN\": score -= 1.0\n","\n","        # Rule 6: Future Tense First Person Ambiguity\n","        if word_lower.endswith(\"eceÄŸim\") or word_lower.endswith(\"acaÄŸÄ±m\"):\n","            if tag == \"VERB\": score += 2.0\n","\n","        # Rule 7: Common Adjectives\n","        if word_lower in [\"gÃ¼zel\", \"kÄ±rmÄ±zÄ±\", \"mavi\", \"bÃ¼yÃ¼k\"]:\n","            if tag == \"ADJ\": score += 3.0\n","\n","        # Rule 8: Anti-Noun Rule for Question Particles\n","        # \"mi\", \"misin\" are almost NEVER nouns (unless talking about musical notes)\n","        if word_lower.startswith(\"mi\") and (\"?\" in word_lower or \"sin\" in word_lower or len(word) <= 5):\n","             if tag == \"NOUN\": score -= 10.0\n","\n","        # --- NEW RULE: Proper Noun Capitalization Handling ---\n","        # If word is Capitalized and NOT the first word, prefer PROPN\n","        if is_capitalized and position > 0:\n","            if tag == \"PROPN\": score += 4.0\n","            if tag == \"NOUN\": score -= 0.5 # Slight penalty for common noun usage\n","\n","        # If word is Capitalized and IS the first word, we can't be sure,\n","        # but if the lexicon says it's ONLY a PROPN (like 'Ahmet'), the FST analysis controls this.\n","        # -----------------------------------------------------\n","\n","        return score\n","\n","    def decode_sentence(self, sentence_tokens):\n","        # 1. Get all possible analyses\n","        lattice = []\n","        for word in sentence_tokens:\n","            raw_analyses = analyze(word)\n","            word_candidates = []\n","\n","            if not raw_analyses or \"No analysis\" in raw_analyses[0]:\n","                word_candidates.append({'analysis': f\"{word}+NOUN+UNKNOWN\", 'tag': 'NOUN', 'word': word})\n","            else:\n","                for ana in raw_analyses:\n","                    tag = self.get_tag_from_analysis(ana)\n","                    word_candidates.append({'analysis': ana, 'tag': tag, 'word': word})\n","            lattice.append(word_candidates)\n","\n","        n = len(lattice)\n","        if n == 0: return []\n","\n","        best_scores = [ {} for _ in range(n) ]\n","        backpointers = [ {} for _ in range(n) ]\n","\n","        # Step 0: Start\n","        next_tok = lattice[1][0]['word'] if n > 1 else None\n","        for i, candidate in enumerate(lattice[0]):\n","            trans_prob = self.get_transition_prob('START', candidate['tag'])\n","            heuristic = self.heuristic_weight(candidate['word'], candidate['analysis'], candidate['tag'], 0, n, next_tok)\n","            best_scores[0][i] = math.log(trans_prob) + heuristic\n","\n","        # Step 1..N: Viterbi Recursion\n","        for t in range(1, n):\n","            next_tok = lattice[t+1][0]['word'] if t < n - 1 else None\n","\n","            # Look at the BEST tag of the PREVIOUS word to help decide current word\n","            # This is a simplification of Viterbi context\n","            prev_best_idx = max(best_scores[t-1], key=best_scores[t-1].get)\n","            prev_tag = lattice[t-1][prev_best_idx]['tag']\n","            prev_word = lattice[t-1][prev_best_idx]['word']\n","\n","            for i, curr_cand in enumerate(lattice[t]):\n","                max_score = -float('inf')\n","                best_prev_idx = -1\n","\n","                heuristic = self.heuristic_weight(curr_cand['word'], curr_cand['analysis'], curr_cand['tag'], t, n, next_tok)\n","\n","                # --- NEW: CONTEXTUAL HEURISTIC (Adjective -> Noun) ---\n","                # If previous word was a known color/adj, force NOUN heavily\n","                if prev_word.lower() in [\"kÄ±rmÄ±zÄ±\", \"mavi\", \"yeÅŸil\", \"kara\", \"gÃ¼zel\", \"bÃ¼yÃ¼k\"]:\n","                    if curr_cand['tag'] == \"NOUN\": heuristic += 5.0\n","                    if curr_cand['tag'] == \"VERB\": heuristic -= 5.0\n","                # -----------------------------------------------------\n","\n","                for j, prev_cand in enumerate(lattice[t-1]):\n","                    prev_score = best_scores[t-1][j]\n","                    trans_prob = self.get_transition_prob(prev_cand['tag'], curr_cand['tag'])\n","\n","                    score = prev_score + math.log(trans_prob) + heuristic\n","\n","                    if score > max_score:\n","                        max_score = score\n","                        best_prev_idx = j\n","\n","                best_scores[t][i] = max_score\n","                backpointers[t][i] = best_prev_idx\n","\n","        # Backtracking\n","        best_last_idx = max(best_scores[n-1], key=best_scores[n-1].get)\n","        result_path = []\n","        curr_idx = best_last_idx\n","\n","        for t in range(n-1, -1, -1):\n","            cand = lattice[t][curr_idx]\n","            result_path.append(cand)\n","            if t > 0:\n","                curr_idx = backpointers[t][curr_idx]\n","\n","        return result_path[::-1]\n","\n","# Re-instantiate\n","disambiguator = ContextAwareDisambiguator()\n","\n","def post_process_results(results):\n","    \"\"\"\n","    Merges information from Question Particles into the preceding Verb.\n","    Example: 'yapacak' (3SG) + 'mÄ±sÄ±n' (2SG) -> Updates 'yapacak' to 2SG.\n","    \"\"\"\n","    # Iterate through tokens looking for VERB + QUES pairs\n","    for i in range(len(results) - 1):\n","        curr = results[i]\n","        next_tok = results[i+1]\n","\n","        if curr['tag'] == 'VERB' and next_tok['tag'] == 'QUES':\n","            # We found a Verb followed by a Question Particle\n","\n","            # 1. Detect Person in the Question Particle\n","            person_marker = None\n","            qs_analysis = next_tok['best_analysis']\n","\n","            if \"+1SG\" in qs_analysis: person_marker = \"+1SG\"\n","            elif \"+2SG\" in qs_analysis: person_marker = \"+2SG\"\n","            elif \"+1PL\" in qs_analysis: person_marker = \"+1PL\"\n","            elif \"+2PL\" in qs_analysis: person_marker = \"+2PL\"\n","            # 3SG is usually implicit, but if needed:\n","            elif \"+3PL\" in qs_analysis: person_marker = \"+3PL\"\n","\n","            # 2. Update the Verb Analysis\n","            if person_marker:\n","                current_analysis = curr['best_analysis']\n","\n","                # If the verb currently defaults to 3SG (or has no person tag), add the correct one.\n","                # Note: FST usually outputs +3SG for bare stems if configured,\n","                # or just ends with Tense.\n","\n","                if \"+3SG\" in current_analysis:\n","                    # Replace default 3SG with the actual person from the particle\n","                    curr['best_analysis'] = current_analysis.replace(\"+3SG\", person_marker)\n","                else:\n","                    # Append the person marker\n","                    curr['best_analysis'] = current_analysis + person_marker\n","\n","    return results\n","\n","def analyze_sentence_context_aware(sentence):\n","    \"\"\"Wrapper function to replace the old analyze_sentence\"\"\"\n","    # Simple tokenization (handling punctuation loosely)\n","    # In a real app, split punctuation marks effectively\n","    import re\n","    tokens = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n","\n","    results = disambiguator.decode_sentence(tokens)\n","\n","    formatted_output = []\n","    for item in results:\n","        formatted_output.append({\n","            'token': item['word'],\n","            'best_analysis': item['analysis'],\n","            'tag': item['tag']\n","        })\n","\n","    formatted_output = post_process_results(formatted_output)\n","\n","    return formatted_output\n","\n","def save_fst(filename):\n","    \"\"\"Save the compiled FST to a file.\"\"\"\n","    turkish_analyzer.write(filename)\n","    print(f\"FST saved to {filename}\")\n","\n","# Test\n","if __name__ == \"__main__\":\n","    print(\"=\"*60)\n","    print(\"LEXICON LOADED\")\n","    print(\"=\"*60)\n","    print(f\"Nouns: {len(lexicon.get('nouns', []))} words\")\n","    print(f\"Verbs: {len(lexicon.get('verbs', []))} words (infinitives â†’ roots extracted)\")\n","    print(f\"Adjectives: {len(lexicon.get('adjectives', []))} words\")\n","    print(f\"Pronouns: {len(lexicon.get('pronouns', []))} words\")\n","    print(f\"Adverbs: {len(lexicon.get('adverbs', []))} words\")\n","    print(f\"Conjunctions: {len(lexicon.get('conjunctions', []))} words\")\n","    print(f\"Postpositions: {len(lexicon.get('postpositions', []))} words\")\n","    print(f\"Proper Nouns: {len(lexicon.get('proper_nouns', []))} words\")\n","\n","    # Show some verb root examples\n","    if lexicon.get('verbs'):\n","        print(\"\\nVerb root examples:\")\n","        for verb in lexicon.get('verbs', [])[:5]:\n","            root = extract_verb_root(verb)\n","            print(f\"  {verb} â†’ {root}\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"DEBUG: Testing verb compositions\")\n","    print(\"=\"*60)\n","\n","    test = \"gel\"\n","    try:\n","        lattice = pynini.compose(test, verb_imperative)\n","        print(f\"\\n'{test}' + verb_imperative:\")\n","        for path in lattice.paths().ostrings():\n","            print(f\"  âœ“ {path}\")\n","    except Exception as e:\n","        print(f\"  âœ— Error: {e}\")\n","\n","    test2 = \"gelsin\"\n","    try:\n","        lattice = pynini.compose(test2, verb_imperative)\n","        print(f\"\\n'{test2}' + verb_imperative:\")\n","        for path in lattice.paths().ostrings():\n","            print(f\"  âœ“ {path}\")\n","    except Exception as e:\n","        print(f\"  âœ— Error: {e}\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"SINGLE WORD ANALYSIS\")\n","    print(\"=\"*60)\n","\n","    test_words = [\n","        # Dilek Kipleri\n","        \"gel\",            # come! (imperative 2sg)\n","        \"gelin\",          # come! (imperative 2pl)\n","        \"gelsin\",         # let him/her come (imperative 3sg)\n","        \"gelsem\",         # if I come (conditional)\n","        \"geleyim\",        # let me come (optative)\n","        \"gelmeli\",        # must come (necessitative)\n","        \"yazmalÄ±yÄ±m\",     # I must write\n","        \"okusana\",        # read then! (optative emphatic)\n","        \"gÃ¶rseler\",       # if they see (conditional)\n","        # Bildirme Kipleri\n","        \"okudum\",         # I read (past)\n","        \"gelebilecek\",    # will be able to come (future)\n","        \"geliyorum\",      # I am coming (present continuous)\n","        # Nouns\n","        \"kitaplardan\",    # from books\n","        \"kalemlik\",       # pencil case\n","        \"evdekiler\",      # those in the house\n","    ]\n","\n","    for word in test_words:\n","        print(f\"\\n{word}:\")\n","        analyses = analyze(word)\n","        for analysis in analyses[:5]:\n","            print(f\"  {analysis}\")\n","        if len(analyses) > 5:\n","            print(f\"  ... and {len(analyses) - 5} more analyses\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CONTEXT-AWARE ANALYSIS (Viterbi)\")\n","    print(\"=\"*60)\n","\n","    # These sentences contain ambiguous words\n","    ambiguous_sentences = [\n","        \"yÃ¼zÃ¼ gÃ¼zel\",           # yÃ¼z: Face (Noun) vs Swim (Verb) -> Expect NOUN\n","        \"denizde yÃ¼z\",          # yÃ¼z: Face (Noun) vs Swim (Verb) -> Expect VERB (Imperative)\n","        \"bana gÃ¼l\",             # gÃ¼l: Rose (Noun) vs Smile (Verb) -> Expect VERB\n","        \"kÄ±rmÄ±zÄ± gÃ¼l\",          # gÃ¼l: Rose (Noun) vs Smile (Verb) -> Expect NOUN (Adj modifies Noun)\n","        \"okula git\",            # git: Expect VERB\n","        \"gÃ¼zel bir ev\",         # gÃ¼zel: Adj, bir: Det, ev: Noun\n","        \"evde misin\",           # misin: Expect QUES\n","        \"kitap okumayÄ± severim\" # severim: Expect VERB (End of sentence)\n","    ]\n","\n","    for sent in ambiguous_sentences:\n","        print(f\"\\nSentence: '{sent}'\")\n","        results = analyze_sentence_context_aware(sent)\n","\n","        # Print formatted table\n","        print(f\" {'Word':<15} | {'Tag':<6} | {'Selected Analysis'}\")\n","        print(\"-\" * 50)\n","        for res in results:\n","            print(f\" {res['token']:<15} | {res['tag']:<6} | {res['best_analysis']}\")"]},{"cell_type":"code","source":["print(analyze_sentence_context_aware(\"gÃ¼lebilir misin\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCWe8gQdGzd_","executionInfo":{"status":"ok","timestamp":1765859975737,"user_tz":-180,"elapsed":23,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"20b53015-1cf0-4366-e0c8-2d261d055059"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'token': 'gÃ¼lebilir', 'best_analysis': 'gÃ¼l+VERB+ABIL+AOR+2SG', 'tag': 'VERB'}, {'token': 'misin', 'best_analysis': 'mi+QUES+2SG', 'tag': 'QUES'}]\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}