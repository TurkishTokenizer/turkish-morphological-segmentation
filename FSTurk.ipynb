{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6txQNlZvQ8P",
        "outputId": "d13d018b-7177-4629-9200-9f240c91b8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynini\n",
            "  Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (165.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.5/165.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynini\n",
            "Successfully installed pynini-2.1.7\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install --only-binary :all: pynini\n",
        "!pip install wurlitzer\n",
        "import pynini\n",
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j_HcAl-2_lcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6719108-7500-45c2-c390-922acb725e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: turkish_lexicon.json not found. Using default minimal lexicon.\n",
            "============================================================\n",
            "LEXICON LOADED\n",
            "============================================================\n",
            "Nouns: 3 words\n",
            "Verbs: 3 words (infinitives → roots extracted)\n",
            "Adjectives: 2 words\n",
            "Pronouns: 3 words\n",
            "Adverbs: 2 words\n",
            "Conjunctions: 3 words\n",
            "Postpositions: 2 words\n",
            "Proper Nouns: 0 words\n",
            "\n",
            "Verb root examples:\n",
            "  gel → gel\n",
            "  git → git\n",
            "  oku → oku\n",
            "\n",
            "============================================================\n",
            "DEBUG: Testing verb compositions\n",
            "============================================================\n",
            "\n",
            "'gel' + verb_imperative:\n",
            "  ✓ gel+VERB+IMP+2SG\n",
            "\n",
            "'gelsin' + verb_imperative:\n",
            "  ✓ gel+VERB+IMP+3SG\n",
            "\n",
            "============================================================\n",
            "SINGLE WORD ANALYSIS\n",
            "============================================================\n",
            "\n",
            "gel:\n",
            "  gel+VERB\n",
            "  gel+VERB+IMP+2SG\n",
            "\n",
            "gelin:\n",
            "  gel+VERB+IMP+2PL\n",
            "  gel+VERB+POSS.2SG\n",
            "  gel+VERB+VOICE.in\n",
            "  gel+VERB+VOICE.in+IMP+2SG\n",
            "\n",
            "gelsin:\n",
            "  gel+VERB+IMP+3SG\n",
            "\n",
            "gelsem:\n",
            "  gel+VERB+COND+1SG\n",
            "\n",
            "geleyim:\n",
            "  gel+VERB+OPT+1SG\n",
            "\n",
            "gelmeli:\n",
            "  gel+VERB+NEC+3SG\n",
            "\n",
            "yazmalıyım:\n",
            "  No analysis found for: yazmalıyım\n",
            "\n",
            "okusana:\n",
            "  oku+VERB+OPT+2SG+EMPH\n",
            "\n",
            "görseler:\n",
            "  No analysis found for: görseler\n",
            "\n",
            "okudum:\n",
            "  oku+VERB+PAST+1SG\n",
            "\n",
            "gelebilecek:\n",
            "  gel+VERB+ABIL+FUT+3SG\n",
            "\n",
            "geliyorum:\n",
            "  gel+VERB+PRES.CONT+1SG\n",
            "\n",
            "kitaplardan:\n",
            "  kitap+NOUN+PL+ABL\n",
            "\n",
            "kalemlik:\n",
            "  No analysis found for: kalemlik\n",
            "\n",
            "evdekiler:\n",
            "  ev+NOUN+LOC+KI+PL\n",
            "\n",
            "============================================================\n",
            "CONTEXT-AWARE ANALYSIS (Viterbi)\n",
            "============================================================\n",
            "\n",
            "Sentence: 'yüzü güzel'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " yüzü            | NOUN   | yüzü+NOUN+UNKNOWN\n",
            " güzel           | ADJ    | güzel+ADJ\n",
            "\n",
            "Sentence: 'denizde yüz'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " denizde         | NOUN   | denizde+NOUN+UNKNOWN\n",
            " yüz             | NOUN   | yüz+NOUN+UNKNOWN\n",
            "\n",
            "Sentence: 'bana gül'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " bana            | PRON   | ben+PRON+DAT\n",
            " gül             | NOUN   | gül+NOUN+UNKNOWN\n",
            "\n",
            "Sentence: 'kırmızı gül'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " kırmızı         | NOUN   | kırmızı+NOUN+UNKNOWN\n",
            " gül             | NOUN   | gül+NOUN+UNKNOWN\n",
            "\n",
            "Sentence: 'okula git'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " okula           | VERB   | oku+VERB+INS\n",
            " git             | VERB   | git+VERB\n",
            "\n",
            "Sentence: 'güzel bir ev'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " güzel           | ADJ    | güzel+ADJ\n",
            " bir             | NOUN   | bir+NOUN+UNKNOWN\n",
            " ev              | NOUN   | ev+NOUN\n",
            "\n",
            "Sentence: 'evde misin'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " evde            | NOUN   | ev+NOUN+LOC\n",
            " misin           | QUES   | mi+QUES+2SG\n",
            "\n",
            "Sentence: 'kitap okumayı severim'\n",
            " Word            | Tag    | Selected Analysis\n",
            "--------------------------------------------------\n",
            " kitap           | NOUN   | kitap+NOUN\n",
            " okumayı         | NOUN   | oku+VERB+DER.ma+ACC\n",
            " severim         | NOUN   | severim+NOUN+UNKNOWN\n"
          ]
        }
      ],
      "source": [
        "import pynini\n",
        "import json\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# ===== LOAD LEXICON FROM JSON =====\n",
        "def load_lexicon(json_file='turkish_lexicon.json'):\n",
        "    \"\"\"Load Turkish lexicon from JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {json_file} not found. Using default minimal lexicon.\")\n",
        "        return {\n",
        "            \"nouns\": [\"ev\", \"kitap\", \"masa\"],\n",
        "            \"verbs\": [\"gel\", \"git\", \"oku\"],\n",
        "            \"adjectives\": [\"güzel\", \"iyi\"],\n",
        "            \"pronouns\": [\"ben\", \"sen\", \"o\"],\n",
        "            \"adverbs\": [\"çok\", \"az\"],\n",
        "            \"conjunctions\": [\"ve\", \"da\", \"de\"],\n",
        "            \"postpositions\": [\"gibi\", \"için\"],\n",
        "            \"proper_nouns\": []\n",
        "        }\n",
        "\n",
        "# Load lexicon\n",
        "lexicon = load_lexicon()\n",
        "\n",
        "# ===== HELPER FUNCTIONS =====\n",
        "def extract_verb_root(verb_infinitive):\n",
        "    \"\"\"Extract verb root from infinitive form (remove -mak/-mek).\"\"\"\n",
        "    if verb_infinitive.endswith('mak'):\n",
        "        return verb_infinitive[:-3]\n",
        "    elif verb_infinitive.endswith('mek'):\n",
        "        return verb_infinitive[:-3]\n",
        "    else:\n",
        "        # If it doesn't end with -mak/-mek, return as is (might be already a root)\n",
        "        return verb_infinitive\n",
        "\n",
        "def create_alternating_roots(words, tag):\n",
        "    \"\"\"\n",
        "    Create FST roots with consonant softening (ünsüz yumuşaması).\n",
        "    p→b, ç→c, t→d, k→g/ğ when followed by vowel-initial suffix.\n",
        "    \"\"\"\n",
        "    roots = []\n",
        "\n",
        "    for word in words:\n",
        "        # Original form (before consonant-initial suffixes)\n",
        "        roots.append(pynini.cross(word, f\"{word}+{tag}\"))\n",
        "\n",
        "        # Check if word ends with p, ç, t, k (sert ünsüz)\n",
        "        if len(word) > 1 and word[-1] in ['p', 'ç', 't', 'k']:\n",
        "            # Get the stem without final consonant\n",
        "            stem = word[:-1]\n",
        "            final = word[-1]\n",
        "\n",
        "            # Determine softened form (yumuşak ünsüz)\n",
        "            if final == 'p':\n",
        "                softened = stem + 'b'\n",
        "            elif final == 'ç':\n",
        "                softened = stem + 'c'\n",
        "            elif final == 't':\n",
        "                softened = stem + 'd'\n",
        "            elif final == 'k':\n",
        "                # k → ğ after vowels, k → g after consonants\n",
        "                if len(stem) > 0 and stem[-1] in 'aeıioöuü':\n",
        "                    softened = stem + 'ğ'\n",
        "                else:\n",
        "                    softened = stem + 'g'\n",
        "\n",
        "            # Add softened form (before vowel-initial suffixes)\n",
        "            roots.append(pynini.cross(softened, f\"{word}+{tag}\"))\n",
        "\n",
        "    return pynini.union(*roots) if roots else pynini.cross(\"\", \"\")\n",
        "\n",
        "# ===== ROOTS BY LEXICAL CATEGORY =====\n",
        "\n",
        "# Nouns (Ad) - with consonant softening\n",
        "noun_roots = create_alternating_roots(\n",
        "    lexicon.get('nouns', []),\n",
        "    'NOUN'\n",
        ") if lexicon.get('nouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Adjectives (Sıfat) - with consonant softening\n",
        "adj_roots = create_alternating_roots(\n",
        "    lexicon.get('adjectives', []),\n",
        "    'ADJ'\n",
        ") if lexicon.get('adjectives') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Verbs (Fiil) - extract roots and apply consonant softening\n",
        "verb_root_list = [extract_verb_root(word) for word in lexicon.get('verbs', [])]\n",
        "verb_roots = create_alternating_roots(\n",
        "    verb_root_list,\n",
        "    'VERB'\n",
        ") if verb_root_list else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Pronouns (Zamir) - usually don't undergo consonant softening, but include for completeness\n",
        "pronoun_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+PRON\")\n",
        "    for word in lexicon.get('pronouns', [])\n",
        "]) if lexicon.get('pronouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "irregular_pronouns = pynini.union(\n",
        "    pynini.cross(\"bana\", \"ben+PRON+DAT\"),\n",
        "    pynini.cross(\"sana\", \"sen+PRON+DAT\")\n",
        ")\n",
        "pronoun_roots = pynini.union(pronoun_roots, irregular_pronouns)\n",
        "\n",
        "# Adverbs (Zarf) - no inflection\n",
        "adverb_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+ADV\")\n",
        "    for word in lexicon.get('adverbs', [])\n",
        "]) if lexicon.get('adverbs') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Postpositions (Edat) - no inflection\n",
        "postposition_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+POSTP\")\n",
        "    for word in lexicon.get('postpositions', [])\n",
        "]) if lexicon.get('postpositions') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Interjections (Ünlem) - no inflection\n",
        "interjection_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+INTERJ\")\n",
        "    for word in lexicon.get('interjections', [])\n",
        "]) if lexicon.get('interjections') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Conjunctions (Bağlaç) - no inflection\n",
        "conjunction_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+CONJ\")\n",
        "    for word in lexicon.get('conjunctions', [])\n",
        "]) if lexicon.get('conjunctions') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Proper Nouns (Özel İsimler) - with consonant softening\n",
        "proper_noun_roots = create_alternating_roots(\n",
        "    lexicon.get('proper_nouns', []),\n",
        "    'PROPN'\n",
        ") if lexicon.get('proper_nouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Question Particles\n",
        "question_particles = pynini.union(\n",
        "    pynini.cross(\"mi\", \"mi+QUES\"),\n",
        "    pynini.cross(\"mı\", \"mı+QUES\"),\n",
        "    pynini.cross(\"mu\", \"mu+QUES\"),\n",
        "    pynini.cross(\"mü\", \"mü+QUES\"),\n",
        "    pynini.cross(\"misin\", \"mi+QUES+2SG\"),\n",
        "    pynini.cross(\"mısın\", \"mı+QUES+2SG\"),\n",
        "    pynini.cross(\"musun\", \"mu+QUES+2SG\"),\n",
        "    pynini.cross(\"müsün\", \"mü+QUES+2SG\"),\n",
        "    pynini.cross(\"miyim\", \"mi+QUES+1SG\"),\n",
        "    pynini.cross(\"mıyım\", \"mı+QUES+1SG\"),\n",
        "    pynini.cross(\"muyum\", \"mu+QUES+1SG\"),\n",
        "    pynini.cross(\"müyüm\", \"mü+QUES+1SG\"),\n",
        "    pynini.cross(\"miyiz\", \"mi+QUES+1PL\"),\n",
        "    pynini.cross(\"mıyız\", \"mı+QUES+1PL\"),\n",
        "    pynini.cross(\"muyuz\", \"mu+QUES+1PL\"),\n",
        "    pynini.cross(\"müyüz\", \"mü+QUES+1PL\"),\n",
        "    pynini.cross(\"misiniz\", \"mi+QUES+2PL\"),\n",
        "    pynini.cross(\"mısınız\", \"mı+QUES+2PL\"),\n",
        "    pynini.cross(\"musunuz\", \"mu+QUES+2PL\"),\n",
        "    pynini.cross(\"müsünüz\", \"mü+QUES+2PL\")\n",
        ")\n",
        "\n",
        "# ===== DERIVATIONAL SUFFIXES (YAPIM EKLERİ) =====\n",
        "\n",
        "# A) NOUN-TO-NOUN (İsimden İsim)\n",
        "# Includes -lik, -ci, -li, -siz, -cil, -das, -gil\n",
        "deriv_n_n = pynini.union(\n",
        "    # -lik (yer, alet, meslek)\n",
        "    pynini.cross(\"lık\", \"+DER.lik\"), pynini.cross(\"lik\", \"+DER.lik\"),\n",
        "    pynini.cross(\"luk\", \"+DER.luk\"), pynini.cross(\"lük\", \"+DER.lük\"),\n",
        "\n",
        "    # -ci (meslek, alışkanlık)\n",
        "    pynini.cross(\"cı\", \"+DER.ci\"), pynini.cross(\"ci\", \"+DER.ci\"),\n",
        "    pynini.cross(\"cu\", \"+DER.ci\"), pynini.cross(\"cü\", \"+DER.ci\"),\n",
        "    pynini.cross(\"çı\", \"+DER.ci\"), pynini.cross(\"çi\", \"+DER.ci\"),\n",
        "\n",
        "    # -li (bulundurma)\n",
        "    pynini.cross(\"lı\", \"+DER.li\"), pynini.cross(\"li\", \"+DER.li\"),\n",
        "    pynini.cross(\"lu\", \"+DER.li\"), pynini.cross(\"lü\", \"+DER.li\"),\n",
        "\n",
        "    # -siz (yoksunluk)\n",
        "    pynini.cross(\"sız\", \"+DER.siz\"), pynini.cross(\"siz\", \"+DER.siz\"),\n",
        "    pynini.cross(\"suz\", \"+DER.siz\"), pynini.cross(\"süz\", \"+DER.siz\"),\n",
        "\n",
        "    # -ki (aitlik - usually attaches to Locative or Time nouns)\n",
        "    pynini.cross(\"ki\", \"+DER.ki\"), pynini.cross(\"kü\", \"+DER.ki\"),\n",
        "\n",
        "    # -cE (dil, küçültme)\n",
        "    pynini.cross(\"ca\", \"+DER.ce\"), pynini.cross(\"ce\", \"+DER.ce\"),\n",
        "    pynini.cross(\"ça\", \"+DER.ce\"), pynini.cross(\"çe\", \"+DER.ce\"),\n",
        "\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# B) VERB-TO-NOUN (Fiilden İsim)\n",
        "# Includes mastar (-mak), isim-fiil (-ma), sıfat-fiil (-an, -asi, -mez...), -gi, -gin\n",
        "deriv_v_n = pynini.union(\n",
        "    # -mak (Mastar)\n",
        "    pynini.cross(\"mak\", \"+DER.mak\"), pynini.cross(\"mek\", \"+DER.mak\"),\n",
        "\n",
        "    # -ma (İsim-Fiil/Olumsuzluk değil) - Disambiguation Needed later\n",
        "    pynini.cross(\"ma\", \"+DER.ma\"), pynini.cross(\"me\", \"+DER.ma\"),\n",
        "\n",
        "    # -iş (Kalıcı isim)\n",
        "    pynini.cross(\"ış\", \"+DER.iş\"), pynini.cross(\"iş\", \"+DER.iş\"),\n",
        "    pynini.cross(\"uş\", \"+DER.iş\"), pynini.cross(\"üş\", \"+DER.iş\"),\n",
        "\n",
        "    # -im (Fiilden isim)\n",
        "    pynini.cross(\"ım\", \"+DER.im\"), pynini.cross(\"im\", \"+DER.im\"),\n",
        "    pynini.cross(\"um\", \"+DER.im\"), pynini.cross(\"üm\", \"+DER.im\"),\n",
        "\n",
        "    # -gin (Sıfat/İsim)\n",
        "    pynini.cross(\"gın\", \"+DER.gin\"), pynini.cross(\"gin\", \"+DER.gin\"),\n",
        "    pynini.cross(\"gun\", \"+DER.gin\"), pynini.cross(\"gün\", \"+DER.gin\"),\n",
        "    pynini.cross(\"kın\", \"+DER.gin\"), pynini.cross(\"kin\", \"+DER.gin\"),\n",
        "\n",
        "    # -ici (Fail)\n",
        "    pynini.cross(\"ıcı\", \"+DER.ici\"), pynini.cross(\"ici\", \"+DER.ici\"),\n",
        "    pynini.cross(\"ucu\", \"+DER.ici\"), pynini.cross(\"ücü\", \"+DER.ici\"),\n",
        "\n",
        "    # -an (Sıfat Fiil)\n",
        "    pynini.cross(\"an\", \"+DER.an\"), pynini.cross(\"en\", \"+DER.an\"),\n",
        "\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# C) NOUN-TO-VERB (İsimden Fiil)\n",
        "deriv_n_v = pynini.union(\n",
        "    # -le (İsimden fiil)\n",
        "    pynini.cross(\"la\", \"+DER.la\"), pynini.cross(\"le\", \"+DER.la\"),\n",
        "\n",
        "    # -len (Dönüşlü)\n",
        "    pynini.cross(\"lan\", \"+DER.lan\"), pynini.cross(\"len\", \"+DER.lan\"),\n",
        "\n",
        "    # -leş (Oluş)\n",
        "    pynini.cross(\"laş\", \"+DER.laş\"), pynini.cross(\"leş\", \"+DER.laş\"),\n",
        "\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# D) VERB-TO-VERB (Fiilden Fiil / Çatı)\n",
        "# Note: These usually go BEFORE the tense markers.\n",
        "deriv_v_v = pynini.union(\n",
        "    # -t (Ettirgen)\n",
        "    pynini.cross(\"t\", \"+VOICE.t\"),\n",
        "\n",
        "    # -dir (Ettirgen)\n",
        "    pynini.cross(\"dır\", \"+VOICE.dir\"), pynini.cross(\"dir\", \"+VOICE.dir\"),\n",
        "    pynini.cross(\"dur\", \"+VOICE.dir\"), pynini.cross(\"dür\", \"+VOICE.dir\"),\n",
        "    pynini.cross(\"tır\", \"+VOICE.dir\"), pynini.cross(\"tir\", \"+VOICE.dir\"),\n",
        "\n",
        "    # -il (Edilgen)\n",
        "    pynini.cross(\"ıl\", \"+VOICE.il\"), pynini.cross(\"il\", \"+VOICE.il\"),\n",
        "    pynini.cross(\"ul\", \"+VOICE.il\"), pynini.cross(\"ül\", \"+VOICE.il\"),\n",
        "\n",
        "    # -n (Dönüşlü)\n",
        "    pynini.cross(\"ın\", \"+VOICE.in\"), pynini.cross(\"in\", \"+VOICE.in\"),\n",
        "    pynini.cross(\"n\", \"+VOICE.in\"),\n",
        "\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# ===== NOUN/ADJECTIVE MORPHOLOGY =====\n",
        "# Include proper nouns that can take case markers\n",
        "nominal_roots = pynini.union(noun_roots, adj_roots, pronoun_roots, proper_noun_roots)\n",
        "\n",
        "# 1. Update Nominal Path (Noun -> Noun Deriv -> Plural...)\n",
        "nominal_derived = (nominal_roots + deriv_n_n).optimize()\n",
        "# Note: We also need to allow Verbs to become Nouns (e.g. okumayı)\n",
        "verb_to_noun_derived = (verb_roots + deriv_v_v + deriv_v_n).optimize()\n",
        "\n",
        "# Combine both as valid nominal bases\n",
        "nominal_base_all = pynini.union(nominal_derived, verb_to_noun_derived)\n",
        "\n",
        "plural = pynini.union(\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# Re-define plural/possessive/case attached to nominal_base_all\n",
        "nominal_pl = nominal_base_all + plural\n",
        "\n",
        "possessive = pynini.union(\n",
        "    pynini.cross(\"imiz\", \"+POSS.1PL\"), pynini.cross(\"ımız\", \"+POSS.1PL\"),\n",
        "    pynini.cross(\"umuz\", \"+POSS.1PL\"), pynini.cross(\"ümüz\", \"+POSS.1PL\"),\n",
        "    pynini.cross(\"iniz\", \"+POSS.2PL\"), pynini.cross(\"ınız\", \"+POSS.2PL\"),\n",
        "    pynini.cross(\"unuz\", \"+POSS.2PL\"), pynini.cross(\"ünüz\", \"+POSS.2PL\"),\n",
        "    pynini.cross(\"leri\", \"+POSS.3PL\"), pynini.cross(\"ları\", \"+POSS.3PL\"),\n",
        "    pynini.cross(\"im\", \"+POSS.1SG\"), pynini.cross(\"ım\", \"+POSS.1SG\"),\n",
        "    pynini.cross(\"um\", \"+POSS.1SG\"), pynini.cross(\"üm\", \"+POSS.1SG\"),\n",
        "    pynini.cross(\"in\", \"+POSS.2SG\"), pynini.cross(\"ın\", \"+POSS.2SG\"),\n",
        "    pynini.cross(\"un\", \"+POSS.2SG\"), pynini.cross(\"ün\", \"+POSS.2SG\"),\n",
        "    pynini.cross(\"si\", \"+POSS.3SG\"), pynini.cross(\"sı\", \"+POSS.3SG\"),\n",
        "    pynini.cross(\"su\", \"+POSS.3SG\"), pynini.cross(\"sü\", \"+POSS.3SG\")\n",
        ")\n",
        "\n",
        "case_after_poss = pynini.union(\n",
        "    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n",
        "    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n",
        "    pynini.cross(\"ndan\", \"+ABL\"), pynini.cross(\"nden\", \"+ABL\"),\n",
        "    pynini.cross(\"ntan\", \"+ABL\"), pynini.cross(\"nten\", \"+ABL\"),\n",
        "    pynini.cross(\"nın\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n",
        "    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nün\", \"+GEN\"),\n",
        "    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n",
        "    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n",
        "    pynini.cross(\"nda\", \"+LOC\"), pynini.cross(\"nde\", \"+LOC\"),\n",
        "    pynini.cross(\"nta\", \"+LOC\"), pynini.cross(\"nte\", \"+LOC\"),\n",
        "    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n",
        "    pynini.cross(\"na\", \"+DAT\"), pynini.cross(\"ne\", \"+DAT\"),\n",
        "    pynini.cross(\"yı\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n",
        "    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yü\", \"+ACC\"),\n",
        "    pynini.cross(\"nı\", \"+ACC\"), pynini.cross(\"ni\", \"+ACC\"),\n",
        "    pynini.cross(\"nu\", \"+ACC\"), pynini.cross(\"nü\", \"+ACC\"),\n",
        "    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n",
        "    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ki_suffix = pynini.union(\n",
        "    pynini.cross(\"ki\", \"+KI\"),\n",
        "    pynini.cross(\"kü\", \"+KI\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "plural_after_ki = pynini.union(\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "possessive_path = nominal_pl + possessive + case_after_poss + ki_suffix + plural_after_ki\n",
        "\n",
        "case_no_poss = pynini.union(\n",
        "    pynini.cross(\"ların\", \"+GEN\"), pynini.cross(\"lerin\", \"+GEN\"),\n",
        "    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n",
        "    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n",
        "    pynini.cross(\"nın\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n",
        "    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nün\", \"+GEN\"),\n",
        "    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n",
        "    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n",
        "    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n",
        "    pynini.cross(\"a\", \"+DAT\"), pynini.cross(\"e\", \"+DAT\"),\n",
        "    pynini.cross(\"yı\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n",
        "    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yü\", \"+ACC\"),\n",
        "    pynini.cross(\"ı\", \"+ACC\"), pynini.cross(\"i\", \"+ACC\"),\n",
        "    pynini.cross(\"u\", \"+ACC\"), pynini.cross(\"ü\", \"+ACC\"),\n",
        "    pynini.cross(\"la\", \"+INS\"), pynini.cross(\"le\", \"+INS\"),\n",
        "    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n",
        "    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ki_suffix_case = pynini.union(\n",
        "    pynini.cross(\"ki\", \"+KI\"),\n",
        "    pynini.cross(\"kü\", \"+KI\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "plural_after_ki_case = pynini.union(\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "case_only_path = nominal_pl + case_no_poss + ki_suffix_case + plural_after_ki_case\n",
        "\n",
        "nominal_final_base = pynini.union(possessive_path, case_only_path).optimize()\n",
        "\n",
        "# 2. Update Verb Path (Noun -> Verb OR Verb -> Verb)\n",
        "noun_to_verb_stem = (nominal_roots + deriv_n_v).optimize()\n",
        "verb_to_verb_stem = (verb_roots + deriv_v_v).optimize()\n",
        "verb_stems_all = pynini.union(noun_to_verb_stem, verb_to_verb_stem)\n",
        "\n",
        "copula = pynini.union(\n",
        "    pynini.cross(\"ydi\", \"+COP.PAST\"), pynini.cross(\"ydı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ydu\", \"+COP.PAST\"), pynini.cross(\"ydü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ymış\", \"+COP.EVID\"), pynini.cross(\"ymiş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"ymuş\", \"+COP.EVID\"), pynini.cross(\"ymüş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"yse\", \"+COP.COND\"), pynini.cross(\"ysa\", \"+COP.COND\"),\n",
        "    pynini.cross(\"di\", \"+COP.PAST\"), pynini.cross(\"dı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"du\", \"+COP.PAST\"), pynini.cross(\"dü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ti\", \"+COP.PAST\"), pynini.cross(\"tı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"tu\", \"+COP.PAST\"), pynini.cross(\"tü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"miş\", \"+COP.EVID\"), pynini.cross(\"mış\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"muş\", \"+COP.EVID\"), pynini.cross(\"müş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"se\", \"+COP.COND\"), pynini.cross(\"sa\", \"+COP.COND\"),\n",
        "    pynini.cross(\"dir\", \"+COP.PRES\"), pynini.cross(\"dır\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"dur\", \"+COP.PRES\"), pynini.cross(\"dür\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"tir\", \"+COP.PRES\"), pynini.cross(\"tır\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"tur\", \"+COP.PRES\"), pynini.cross(\"tür\", \"+COP.PRES\")\n",
        ")\n",
        "\n",
        "person = pynini.union(\n",
        "    pynini.cross(\"im\", \"+1SG\"), pynini.cross(\"ım\", \"+1SG\"),\n",
        "    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"üm\", \"+1SG\"),\n",
        "    pynini.cross(\"in\", \"+2SG\"), pynini.cross(\"ın\", \"+2SG\"),\n",
        "    pynini.cross(\"un\", \"+2SG\"), pynini.cross(\"ün\", \"+2SG\"),\n",
        "    pynini.cross(\"ız\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n",
        "    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"üz\", \"+1PL\"),\n",
        "    pynini.cross(\"nız\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n",
        "    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nüz\", \"+2PL\"),\n",
        "    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "nominal_with_cop = copula + person\n",
        "nominal_without_cop = pynini.cross(\"\", \"\")\n",
        "nominal_complete = nominal_final_base + pynini.union(nominal_with_cop, nominal_without_cop).optimize()\n",
        "\n",
        "# ===== VERB MORPHOLOGY =====\n",
        "\n",
        "# Voice, Ability, Negation (all optional)\n",
        "voice = pynini.union(\n",
        "    pynini.cross(\"ıl\", \"+PASS\"), pynini.cross(\"il\", \"+PASS\"),\n",
        "    pynini.cross(\"ul\", \"+PASS\"), pynini.cross(\"ül\", \"+PASS\"),\n",
        "    pynini.cross(\"ın\", \"+REFL\"), pynini.cross(\"in\", \"+REFL\"),\n",
        "    pynini.cross(\"un\", \"+REFL\"), pynini.cross(\"ün\", \"+REFL\"),\n",
        "    pynini.cross(\"lan\", \"+REFL\"), pynini.cross(\"len\", \"+REFL\"),\n",
        "    pynini.cross(\"ış\", \"+RECIP\"), pynini.cross(\"iş\", \"+RECIP\"),\n",
        "    pynini.cross(\"uş\", \"+RECIP\"), pynini.cross(\"üş\", \"+RECIP\"),\n",
        "    pynini.cross(\"t\", \"+CAUS\"), pynini.cross(\"d\", \"+CAUS\"),\n",
        "    pynini.cross(\"dır\", \"+CAUS\"), pynini.cross(\"dir\", \"+CAUS\"),\n",
        "    pynini.cross(\"dur\", \"+CAUS\"), pynini.cross(\"dür\", \"+CAUS\"),\n",
        "    pynini.cross(\"tır\", \"+CAUS\"), pynini.cross(\"tir\", \"+CAUS\"),\n",
        "    pynini.cross(\"tur\", \"+CAUS\"), pynini.cross(\"tür\", \"+CAUS\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ability = pynini.union(\n",
        "    pynini.cross(\"ebil\", \"+ABIL\"),\n",
        "    pynini.cross(\"abil\", \"+ABIL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "negation = pynini.union(\n",
        "    pynini.cross(\"ma\", \"+NEG\"),\n",
        "    pynini.cross(\"me\", \"+NEG\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# Bildirme Kipleri (Indicative)\n",
        "indicative_tense = pynini.union(\n",
        "    pynini.cross(\"iyor\", \"+PRES.CONT\"), pynini.cross(\"ıyor\", \"+PRES.CONT\"),\n",
        "    pynini.cross(\"uyor\", \"+PRES.CONT\"), pynini.cross(\"üyor\", \"+PRES.CONT\"),\n",
        "    pynini.cross(\"ecek\", \"+FUT\"), pynini.cross(\"acak\", \"+FUT\"),\n",
        "    pynini.cross(\"ır\", \"+AOR\"), pynini.cross(\"ir\", \"+AOR\"),\n",
        "    pynini.cross(\"ur\", \"+AOR\"), pynini.cross(\"ür\", \"+AOR\"),\n",
        "    pynini.cross(\"ar\", \"+AOR\"), pynini.cross(\"er\", \"+AOR\"),\n",
        "    pynini.cross(\"r\", \"+AOR\"),\n",
        "    pynini.cross(\"dı\", \"+PAST\"), pynini.cross(\"di\", \"+PAST\"),\n",
        "    pynini.cross(\"du\", \"+PAST\"), pynini.cross(\"dü\", \"+PAST\"),\n",
        "    pynini.cross(\"tı\", \"+PAST\"), pynini.cross(\"ti\", \"+PAST\"),\n",
        "    pynini.cross(\"tu\", \"+PAST\"), pynini.cross(\"tü\", \"+PAST\"),\n",
        "    pynini.cross(\"mış\", \"+INFER\"), pynini.cross(\"miş\", \"+INFER\"),\n",
        "    pynini.cross(\"muş\", \"+INFER\"), pynini.cross(\"müş\", \"+INFER\")\n",
        ")\n",
        "\n",
        "indicative_person = pynini.union(\n",
        "    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"üm\", \"+1SG\"),\n",
        "    pynini.cross(\"ım\", \"+1SG\"), pynini.cross(\"im\", \"+1SG\"),\n",
        "    pynini.cross(\"m\", \"+1SG\"),\n",
        "    pynini.cross(\"sun\", \"+2SG\"), pynini.cross(\"sün\", \"+2SG\"),\n",
        "    pynini.cross(\"sın\", \"+2SG\"), pynini.cross(\"sin\", \"+2SG\"),\n",
        "    pynini.cross(\"n\", \"+2SG\"),\n",
        "    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"üz\", \"+1PL\"),\n",
        "    pynini.cross(\"ız\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n",
        "    pynini.cross(\"k\", \"+1PL\"),\n",
        "    pynini.cross(\"sunuz\", \"+2PL\"), pynini.cross(\"sünüz\", \"+2PL\"),\n",
        "    pynini.cross(\"sınız\", \"+2PL\"), pynini.cross(\"siniz\", \"+2PL\"),\n",
        "    pynini.cross(\"nız\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n",
        "    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nüz\", \"+2PL\"),\n",
        "    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n",
        "    pynini.cross(\"\", \"+3SG\")\n",
        ")\n",
        "\n",
        "# Dilek/Tasarlama Kipleri (Subjunctive/Optative)\n",
        "\n",
        "# İstek Kipi (Optative)\n",
        "optative_mood_person = pynini.union(\n",
        "    pynini.cross(\"ayım\", \"+OPT+1SG\"), pynini.cross(\"eyim\", \"+OPT+1SG\"),\n",
        "    pynini.cross(\"ayum\", \"+OPT+1SG\"), pynini.cross(\"eyüm\", \"+OPT+1SG\"),\n",
        "    pynini.cross(\"asın\", \"+OPT+2SG\"), pynini.cross(\"esin\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"asun\", \"+OPT+2SG\"), pynini.cross(\"esün\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"asana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"esene\", \"+OPT+2SG+EMPH\"),\n",
        "    pynini.cross(\"sana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"sene\", \"+OPT+2SG+EMPH\"),\n",
        "    pynini.cross(\"asan\", \"+OPT+2SG\"), pynini.cross(\"esen\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"a\", \"+OPT+3SG\"), pynini.cross(\"e\", \"+OPT+3SG\"),\n",
        "    pynini.cross(\"alım\", \"+OPT+1PL\"), pynini.cross(\"elim\", \"+OPT+1PL\"),\n",
        "    pynini.cross(\"alum\", \"+OPT+1PL\"), pynini.cross(\"elüm\", \"+OPT+1PL\"),\n",
        "    pynini.cross(\"asınız\", \"+OPT+2PL\"), pynini.cross(\"esiniz\", \"+OPT+2PL\"),\n",
        "    pynini.cross(\"asunuz\", \"+OPT+2PL\"), pynini.cross(\"esünüz\", \"+OPT+2PL\"),\n",
        "    pynini.cross(\"alar\", \"+OPT+3PL\"), pynini.cross(\"eler\", \"+OPT+3PL\")\n",
        ")\n",
        "\n",
        "# Dilek-Koşul Kipi (Conditional)\n",
        "conditional_mood_person = pynini.union(\n",
        "    pynini.cross(\"sam\", \"+COND+1SG\"), pynini.cross(\"sem\", \"+COND+1SG\"),\n",
        "    pynini.cross(\"san\", \"+COND+2SG\"), pynini.cross(\"sen\", \"+COND+2SG\"),\n",
        "    pynini.cross(\"sa\", \"+COND+3SG\"), pynini.cross(\"se\", \"+COND+3SG\"),\n",
        "    pynini.cross(\"sak\", \"+COND+1PL\"), pynini.cross(\"sek\", \"+COND+1PL\"),\n",
        "    pynini.cross(\"sanız\", \"+COND+2PL\"), pynini.cross(\"seniz\", \"+COND+2PL\"),\n",
        "    pynini.cross(\"sanuz\", \"+COND+2PL\"), pynini.cross(\"senüz\", \"+COND+2PL\"),\n",
        "    pynini.cross(\"salar\", \"+COND+3PL\"), pynini.cross(\"seler\", \"+COND+3PL\")\n",
        ")\n",
        "\n",
        "# ...\n",
        "\n",
        "\n",
        "# Gereklilik Kipi (Necessitative)\n",
        "necessitative_mood_person = pynini.union(\n",
        "    pynini.cross(\"malıyım\", \"+NEC+1SG\"), pynini.cross(\"meliyim\", \"+NEC+1SG\"),\n",
        "    pynini.cross(\"malıyum\", \"+NEC+1SG\"), pynini.cross(\"meliyüm\", \"+NEC+1SG\"),\n",
        "    pynini.cross(\"malısın\", \"+NEC+2SG\"), pynini.cross(\"melisin\", \"+NEC+2SG\"),\n",
        "    pynini.cross(\"malısun\", \"+NEC+2SG\"), pynini.cross(\"melisün\", \"+NEC+2SG\"),\n",
        "    pynini.cross(\"malı\", \"+NEC+3SG\"), pynini.cross(\"meli\", \"+NEC+3SG\"),\n",
        "    pynini.cross(\"malıyız\", \"+NEC+1PL\"), pynini.cross(\"meliyiz\", \"+NEC+1PL\"),\n",
        "    pynini.cross(\"malıyuz\", \"+NEC+1PL\"), pynini.cross(\"meliyüz\", \"+NEC+1PL\"),\n",
        "    pynini.cross(\"malısınız\", \"+NEC+2PL\"), pynini.cross(\"melisiniz\", \"+NEC+2PL\"),\n",
        "    pynini.cross(\"malısunuz\", \"+NEC+2PL\"), pynini.cross(\"melisünüz\", \"+NEC+2PL\"),\n",
        "    pynini.cross(\"malılar\", \"+NEC+3PL\"), pynini.cross(\"meliler\", \"+NEC+3PL\")\n",
        ")\n",
        "\n",
        "# Emir Kipi (Imperative)\n",
        "imperative_mood_person = pynini.union(\n",
        "    pynini.cross(\"sin\", \"+IMP+3SG\"), pynini.cross(\"sın\", \"+IMP+3SG\"),\n",
        "    pynini.cross(\"sun\", \"+IMP+3SG\"), pynini.cross(\"sün\", \"+IMP+3SG\"),\n",
        "    pynini.cross(\"in\", \"+IMP+2PL\"), pynini.cross(\"ın\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"un\", \"+IMP+2PL\"), pynini.cross(\"ün\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"iniz\", \"+IMP+2PL\"), pynini.cross(\"ınız\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"unuz\", \"+IMP+2PL\"), pynini.cross(\"ünüz\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"sinler\", \"+IMP+3PL\"), pynini.cross(\"sınlar\", \"+IMP+3PL\"),\n",
        "    pynini.cross(\"sunlar\", \"+IMP+3PL\"), pynini.cross(\"sünler\", \"+IMP+3PL\")\n",
        ")\n",
        "\n",
        "imperative_2sg_bare = pynini.cross(\"\", \"+IMP+2SG\")\n",
        "\n",
        "# Build verb structure\n",
        "verb_base = verb_stems_all + ability + negation\n",
        "\n",
        "# Verb paths\n",
        "verb_indicative = verb_base + indicative_tense + indicative_person\n",
        "verb_optative = verb_base + optative_mood_person\n",
        "verb_conditional = verb_base + conditional_mood_person\n",
        "verb_necessitative = verb_base + necessitative_mood_person\n",
        "verb_imperative = verb_base + pynini.union(imperative_mood_person, imperative_2sg_bare)\n",
        "\n",
        "# Combine all verb paths\n",
        "verb_complete = pynini.union(\n",
        "    verb_indicative,\n",
        "    verb_optative,\n",
        "    verb_conditional,\n",
        "    verb_necessitative,\n",
        "    verb_imperative\n",
        ").optimize()\n",
        "\n",
        "# ===== PUNCTUATION =====\n",
        "punctuation = pynini.union(\n",
        "    pynini.cross(\".\", \"+PUNCT.period\"),\n",
        "    pynini.cross(\",\", \"+PUNCT.comma\"),\n",
        "    pynini.cross(\"?\", \"+PUNCT.question\"),\n",
        "    pynini.cross(\"!\", \"+PUNCT.exclamation\"),\n",
        "    pynini.cross(\":\", \"+PUNCT.colon\"),\n",
        "    pynini.cross(\";\", \"+PUNCT.semicolon\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# ===== COMPLETE ANALYZER =====\n",
        "simple_categories = pynini.union(\n",
        "    adverb_roots,\n",
        "    postposition_roots,\n",
        "    interjection_roots,\n",
        "    conjunction_roots,\n",
        "    question_particles\n",
        ")\n",
        "\n",
        "nominal_fst = (nominal_complete + punctuation).optimize()\n",
        "verb_fst = (verb_complete + punctuation).optimize()\n",
        "simple_fst = (simple_categories + punctuation).optimize()\n",
        "\n",
        "turkish_analyzer = pynini.union(nominal_fst, verb_fst, simple_fst).optimize()\n",
        "\n",
        "def analyze(word):\n",
        "    \"\"\"\n",
        "    Analyze a word. Tries exact match first, then lowercase match.\n",
        "    This handles 'Ali' -> 'ali' lookup automatically.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Try exact match (Good for specific abbreviations if you have them)\n",
        "        lattice = pynini.compose(word, turkish_analyzer)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            # 2. If failed, try lowercase (Handles \"Ali\" -> \"ali\" in lexicon)\n",
        "            lattice = pynini.compose(word.lower(), turkish_analyzer)\n",
        "\n",
        "        analyses = []\n",
        "        seen = set()\n",
        "        try:\n",
        "            for path in lattice.paths().ostrings():\n",
        "                if path not in seen:\n",
        "                    analyses.append(path)\n",
        "                    seen.add(path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return sorted(analyses) if analyses else [f\"No analysis found for: {word}\"]\n",
        "    except Exception as e:\n",
        "        return [f\"Error: {str(e)}\"]\n",
        "# ===== CONTEXT AWARENESS & DISAMBIGUATION =====\n",
        "\n",
        "class ContextAwareDisambiguator:\n",
        "    def __init__(self):\n",
        "        # Transition Matrix\n",
        "        self.transitions = {\n",
        "            'START': {'NOUN': 0.3, 'PRON': 0.3, 'ADJ': 0.15, 'VERB': 0.1, 'ADV': 0.1},\n",
        "\n",
        "            # Adjectives transition to Nouns or other Adjectives\n",
        "            'ADJ': {'NOUN': 0.6, 'ADJ': 0.35, 'VERB': 0.05},\n",
        "\n",
        "            # Nouns transition to...\n",
        "            'NOUN': {'VERB': 0.3, 'NOUN': 0.15, 'CONJ': 0.15, 'POSTP': 0.15, 'QUES': 0.2, 'PUNCT': 0.05},\n",
        "\n",
        "            # Pronouns transition to...\n",
        "            'PRON': {'VERB': 0.4, 'NOUN': 0.1, 'CONJ': 0.2, 'POSTP': 0.2, 'QUES': 0.1},\n",
        "\n",
        "            # Conjunctions (de/da) transition to Verbs or Nouns\n",
        "            'CONJ': {'NOUN': 0.3, 'VERB': 0.4, 'ADJ': 0.1, 'PRON': 0.1, 'ADV': 0.1},\n",
        "\n",
        "            # Question particles\n",
        "            'QUES': {'PUNCT': 0.9, 'VERB': 0.1},\n",
        "\n",
        "            # Verbs (End of clause/sentence)\n",
        "            'VERB': {'PUNCT': 0.6, 'CONJ': 0.1, 'QUES': 0.25, 'NOUN': 0.05},\n",
        "\n",
        "            'DEFAULT': {'NOUN': 0.2, 'VERB': 0.2}\n",
        "        }\n",
        "        self.tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'POSTP', 'CONJ', 'QUES']\n",
        "\n",
        "    def get_tag_from_analysis(self, analysis_str):\n",
        "        if \"No analysis\" in analysis_str: return \"UNKNOWN\"\n",
        "\n",
        "        # Priority order\n",
        "        if \"+QUES\" in analysis_str: return \"QUES\"\n",
        "        if \"+CONJ\" in analysis_str: return \"CONJ\"\n",
        "        if \"+POSTP\" in analysis_str: return \"POSTP\"\n",
        "        if \"+PRON\" in analysis_str: return \"PRON\"\n",
        "\n",
        "        # FIX FOR 'DAKİ': -ki suffix creates Adjectives\n",
        "        if \"+KI\" in analysis_str or \"+DER.ki\" in analysis_str:\n",
        "            return \"ADJ\"\n",
        "\n",
        "        if \"+ADV\" in analysis_str: return \"ADV\"\n",
        "\n",
        "        # Derived Nouns (verb-to-noun)\n",
        "        if \"+DER\" in analysis_str:\n",
        "            if any(x in analysis_str for x in [\"+DER.ma\", \"+DER.mak\", \"+DER.lik\", \"+DER.iş\"]):\n",
        "                return \"NOUN\"\n",
        "\n",
        "        if \"+ADJ\" in analysis_str: return \"ADJ\"\n",
        "        if \"+VERB\" in analysis_str: return \"VERB\"\n",
        "\n",
        "        # Detached suffixes (like 'daki' analyzed as '+LOC+KI')\n",
        "        if analysis_str.startswith(\"+\"):\n",
        "            # If it contains KI, it's ADJ, otherwise likely NOUN suffix chain\n",
        "            if \"KI\" in analysis_str: return \"ADJ\"\n",
        "            return \"NOUN\"\n",
        "\n",
        "        return \"NOUN\"\n",
        "\n",
        "    def get_transition_prob(self, prev_tag, current_tag):\n",
        "        if prev_tag in self.transitions:\n",
        "            return self.transitions[prev_tag].get(current_tag, 0.001)\n",
        "        return self.transitions['DEFAULT'].get(current_tag, 0.001)\n",
        "\n",
        "    def heuristic_weight(self, word, analysis, tag, position, sentence_len, next_token=None):\n",
        "        score = 0.0\n",
        "        word_lower = word.lower()\n",
        "        is_capitalized = word[0].isupper()\n",
        "\n",
        "        # Rule 1: High-confidence Pronouns\n",
        "        if word_lower in [\"ben\", \"sen\", \"o\", \"biz\", \"siz\", \"onlar\", \"bana\", \"sana\"]:\n",
        "            if tag == \"PRON\": score += 5.0\n",
        "            if tag == \"NOUN\": score -= 5.0\n",
        "\n",
        "        # Rule 2: \"Bir\" handling\n",
        "        if word_lower == \"bir\":\n",
        "            if tag == \"ADJ\": score += 2.0\n",
        "            if tag == \"ADV\": score += 1.0\n",
        "            if tag == \"NOUN\": score -= 1.0\n",
        "\n",
        "        # Rule 3: De/Da handling\n",
        "        if word_lower in [\"de\", \"da\"]:\n",
        "            if tag == \"CONJ\": score += 5.0\n",
        "            if tag == \"NOUN\": score -= 5.0\n",
        "\n",
        "        # Rule 4: Contextual Verb vs Noun (Lookahead for Question)\n",
        "        if next_token:\n",
        "            next_is_question = next_token.lower().startswith((\"mi\", \"mı\", \"mu\", \"mü\"))\n",
        "            if next_is_question and tag == \"VERB\": score += 4.0\n",
        "            if next_is_question and tag == \"NOUN\": score -= 1.0\n",
        "\n",
        "        # Rule 5: Strong Sentence End Preference\n",
        "        if position == sentence_len - 1:\n",
        "            if tag == \"VERB\": score += 3.0\n",
        "            if tag == \"NOUN\": score -= 1.0\n",
        "\n",
        "        # Rule 6: Future Tense First Person Ambiguity\n",
        "        if word_lower.endswith(\"eceğim\") or word_lower.endswith(\"acağım\"):\n",
        "            if tag == \"VERB\": score += 2.0\n",
        "\n",
        "        # Rule 7: Common Adjectives\n",
        "        if word_lower in [\"güzel\", \"kırmızı\", \"mavi\", \"büyük\"]:\n",
        "            if tag == \"ADJ\": score += 3.0\n",
        "\n",
        "        # --- NEW RULE: Proper Noun Capitalization Handling ---\n",
        "        # If word is Capitalized and NOT the first word, prefer PROPN\n",
        "        if is_capitalized and position > 0:\n",
        "            if tag == \"PROPN\": score += 4.0\n",
        "            if tag == \"NOUN\": score -= 0.5 # Slight penalty for common noun usage\n",
        "\n",
        "        # If word is Capitalized and IS the first word, we can't be sure,\n",
        "        # but if the lexicon says it's ONLY a PROPN (like 'Ahmet'), the FST analysis controls this.\n",
        "        # -----------------------------------------------------\n",
        "\n",
        "        return score\n",
        "\n",
        "    def decode_sentence(self, sentence_tokens):\n",
        "        # 1. Get all possible analyses\n",
        "        lattice = []\n",
        "        for word in sentence_tokens:\n",
        "            raw_analyses = analyze(word)\n",
        "            word_candidates = []\n",
        "\n",
        "            if not raw_analyses or \"No analysis\" in raw_analyses[0]:\n",
        "                word_candidates.append({'analysis': f\"{word}+NOUN+UNKNOWN\", 'tag': 'NOUN', 'word': word})\n",
        "            else:\n",
        "                for ana in raw_analyses:\n",
        "                    tag = self.get_tag_from_analysis(ana)\n",
        "                    word_candidates.append({'analysis': ana, 'tag': tag, 'word': word})\n",
        "            lattice.append(word_candidates)\n",
        "\n",
        "        n = len(lattice)\n",
        "        if n == 0: return []\n",
        "\n",
        "        best_scores = [ {} for _ in range(n) ]\n",
        "        backpointers = [ {} for _ in range(n) ]\n",
        "\n",
        "        # Step 0: Start\n",
        "        next_tok = lattice[1][0]['word'] if n > 1 else None\n",
        "        for i, candidate in enumerate(lattice[0]):\n",
        "            trans_prob = self.get_transition_prob('START', candidate['tag'])\n",
        "            heuristic = self.heuristic_weight(candidate['word'], candidate['analysis'], candidate['tag'], 0, n, next_tok)\n",
        "            best_scores[0][i] = math.log(trans_prob) + heuristic\n",
        "\n",
        "        # Step 1..N: Viterbi Recursion\n",
        "        for t in range(1, n):\n",
        "            next_tok = lattice[t+1][0]['word'] if t < n - 1 else None\n",
        "            for i, curr_cand in enumerate(lattice[t]):\n",
        "                max_score = -float('inf')\n",
        "                best_prev_idx = -1\n",
        "\n",
        "                heuristic = self.heuristic_weight(curr_cand['word'], curr_cand['analysis'], curr_cand['tag'], t, n, next_tok)\n",
        "\n",
        "                for j, prev_cand in enumerate(lattice[t-1]):\n",
        "                    prev_score = best_scores[t-1][j]\n",
        "                    trans_prob = self.get_transition_prob(prev_cand['tag'], curr_cand['tag'])\n",
        "                    score = prev_score + math.log(trans_prob) + heuristic\n",
        "\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        best_prev_idx = j\n",
        "\n",
        "                best_scores[t][i] = max_score\n",
        "                backpointers[t][i] = best_prev_idx\n",
        "\n",
        "        # Backtracking\n",
        "        best_last_idx = max(best_scores[n-1], key=best_scores[n-1].get)\n",
        "        result_path = []\n",
        "        curr_idx = best_last_idx\n",
        "\n",
        "        for t in range(n-1, -1, -1):\n",
        "            cand = lattice[t][curr_idx]\n",
        "            result_path.append(cand)\n",
        "            if t > 0:\n",
        "                curr_idx = backpointers[t][curr_idx]\n",
        "\n",
        "        return result_path[::-1]\n",
        "\n",
        "# Re-instantiate\n",
        "disambiguator = ContextAwareDisambiguator()\n",
        "\n",
        "def post_process_results(results):\n",
        "    \"\"\"\n",
        "    Merges information from Question Particles into the preceding Verb.\n",
        "    Example: 'yapacak' (3SG) + 'mısın' (2SG) -> Updates 'yapacak' to 2SG.\n",
        "    \"\"\"\n",
        "    # Iterate through tokens looking for VERB + QUES pairs\n",
        "    for i in range(len(results) - 1):\n",
        "        curr = results[i]\n",
        "        next_tok = results[i+1]\n",
        "\n",
        "        if curr['tag'] == 'VERB' and next_tok['tag'] == 'QUES':\n",
        "            # We found a Verb followed by a Question Particle\n",
        "\n",
        "            # 1. Detect Person in the Question Particle\n",
        "            person_marker = None\n",
        "            qs_analysis = next_tok['best_analysis']\n",
        "\n",
        "            if \"+1SG\" in qs_analysis: person_marker = \"+1SG\"\n",
        "            elif \"+2SG\" in qs_analysis: person_marker = \"+2SG\"\n",
        "            elif \"+1PL\" in qs_analysis: person_marker = \"+1PL\"\n",
        "            elif \"+2PL\" in qs_analysis: person_marker = \"+2PL\"\n",
        "            # 3SG is usually implicit, but if needed:\n",
        "            elif \"+3PL\" in qs_analysis: person_marker = \"+3PL\"\n",
        "\n",
        "            # 2. Update the Verb Analysis\n",
        "            if person_marker:\n",
        "                current_analysis = curr['best_analysis']\n",
        "\n",
        "                # If the verb currently defaults to 3SG (or has no person tag), add the correct one.\n",
        "                # Note: FST usually outputs +3SG for bare stems if configured,\n",
        "                # or just ends with Tense.\n",
        "\n",
        "                if \"+3SG\" in current_analysis:\n",
        "                    # Replace default 3SG with the actual person from the particle\n",
        "                    curr['best_analysis'] = current_analysis.replace(\"+3SG\", person_marker)\n",
        "                else:\n",
        "                    # Append the person marker\n",
        "                    curr['best_analysis'] = current_analysis + person_marker\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_sentence_context_aware(sentence):\n",
        "    \"\"\"Wrapper function to replace the old analyze_sentence\"\"\"\n",
        "    # Simple tokenization (handling punctuation loosely)\n",
        "    # In a real app, split punctuation marks effectively\n",
        "    import re\n",
        "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
        "\n",
        "    results = disambiguator.decode_sentence(tokens)\n",
        "\n",
        "    formatted_output = []\n",
        "    for item in results:\n",
        "        formatted_output.append({\n",
        "            'token': item['word'],\n",
        "            'best_analysis': item['analysis'],\n",
        "            'tag': item['tag']\n",
        "        })\n",
        "\n",
        "    formatted_output = post_process_results(formatted_output)\n",
        "\n",
        "    return formatted_output\n",
        "\n",
        "def save_fst(filename):\n",
        "    \"\"\"Save the compiled FST to a file.\"\"\"\n",
        "    turkish_analyzer.write(filename)\n",
        "    print(f\"FST saved to {filename}\")\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"LEXICON LOADED\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Nouns: {len(lexicon.get('nouns', []))} words\")\n",
        "    print(f\"Verbs: {len(lexicon.get('verbs', []))} words (infinitives → roots extracted)\")\n",
        "    print(f\"Adjectives: {len(lexicon.get('adjectives', []))} words\")\n",
        "    print(f\"Pronouns: {len(lexicon.get('pronouns', []))} words\")\n",
        "    print(f\"Adverbs: {len(lexicon.get('adverbs', []))} words\")\n",
        "    print(f\"Conjunctions: {len(lexicon.get('conjunctions', []))} words\")\n",
        "    print(f\"Postpositions: {len(lexicon.get('postpositions', []))} words\")\n",
        "    print(f\"Proper Nouns: {len(lexicon.get('proper_nouns', []))} words\")\n",
        "\n",
        "    # Show some verb root examples\n",
        "    if lexicon.get('verbs'):\n",
        "        print(\"\\nVerb root examples:\")\n",
        "        for verb in lexicon.get('verbs', [])[:5]:\n",
        "            root = extract_verb_root(verb)\n",
        "            print(f\"  {verb} → {root}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DEBUG: Testing verb compositions\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test = \"gel\"\n",
        "    try:\n",
        "        lattice = pynini.compose(test, verb_imperative)\n",
        "        print(f\"\\n'{test}' + verb_imperative:\")\n",
        "        for path in lattice.paths().ostrings():\n",
        "            print(f\"  ✓ {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    test2 = \"gelsin\"\n",
        "    try:\n",
        "        lattice = pynini.compose(test2, verb_imperative)\n",
        "        print(f\"\\n'{test2}' + verb_imperative:\")\n",
        "        for path in lattice.paths().ostrings():\n",
        "            print(f\"  ✓ {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SINGLE WORD ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_words = [\n",
        "        # Dilek Kipleri\n",
        "        \"gel\",            # come! (imperative 2sg)\n",
        "        \"gelin\",          # come! (imperative 2pl)\n",
        "        \"gelsin\",         # let him/her come (imperative 3sg)\n",
        "        \"gelsem\",         # if I come (conditional)\n",
        "        \"geleyim\",        # let me come (optative)\n",
        "        \"gelmeli\",        # must come (necessitative)\n",
        "        \"yazmalıyım\",     # I must write\n",
        "        \"okusana\",        # read then! (optative emphatic)\n",
        "        \"görseler\",       # if they see (conditional)\n",
        "        # Bildirme Kipleri\n",
        "        \"okudum\",         # I read (past)\n",
        "        \"gelebilecek\",    # will be able to come (future)\n",
        "        \"geliyorum\",      # I am coming (present continuous)\n",
        "        # Nouns\n",
        "        \"kitaplardan\",    # from books\n",
        "        \"kalemlik\",       # pencil case\n",
        "        \"evdekiler\",      # those in the house\n",
        "    ]\n",
        "\n",
        "    for word in test_words:\n",
        "        print(f\"\\n{word}:\")\n",
        "        analyses = analyze(word)\n",
        "        for analysis in analyses[:5]:\n",
        "            print(f\"  {analysis}\")\n",
        "        if len(analyses) > 5:\n",
        "            print(f\"  ... and {len(analyses) - 5} more analyses\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CONTEXT-AWARE ANALYSIS (Viterbi)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # These sentences contain ambiguous words\n",
        "    ambiguous_sentences = [\n",
        "        \"yüzü güzel\",           # yüz: Face (Noun) vs Swim (Verb) -> Expect NOUN\n",
        "        \"denizde yüz\",          # yüz: Face (Noun) vs Swim (Verb) -> Expect VERB (Imperative)\n",
        "        \"bana gül\",             # gül: Rose (Noun) vs Smile (Verb) -> Expect VERB\n",
        "        \"kırmızı gül\",          # gül: Rose (Noun) vs Smile (Verb) -> Expect NOUN (Adj modifies Noun)\n",
        "        \"okula git\",            # git: Expect VERB\n",
        "        \"güzel bir ev\",         # güzel: Adj, bir: Det, ev: Noun\n",
        "        \"evde misin\",           # misin: Expect QUES\n",
        "        \"kitap okumayı severim\" # severim: Expect VERB (End of sentence)\n",
        "    ]\n",
        "\n",
        "    for sent in ambiguous_sentences:\n",
        "        print(f\"\\nSentence: '{sent}'\")\n",
        "        results = analyze_sentence_context_aware(sent)\n",
        "\n",
        "        # Print formatted table\n",
        "        print(f\" {'Word':<15} | {'Tag':<6} | {'Selected Analysis'}\")\n",
        "        print(\"-\" * 50)\n",
        "        for res in results:\n",
        "            print(f\" {res['token']:<15} | {res['tag']:<6} | {res['best_analysis']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in [\"gelecek misin\", \"yapacak mısın\", \"gidiyorum\", \"ali çıktı\", \"gelecek\", \"ben\", \"kitapda\", \"kitap daki\", \"kitapta\", \"bende\", \"ben de\"]:\n",
        "    print(f\"\\n{word}:\")\n",
        "    raw = analyze(word)\n",
        "    filtered = analyze_sentence_context_aware(word)\n",
        "    print(f\"  RAW ({len(raw)}): {raw}\")\n",
        "    print(f\"  FILTERED ({len(filtered)}): {filtered}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLxaXXEeWzeR",
        "outputId": "92457bc6-07b7-49bb-d290-0ce3ddce4b31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gelecek misin:\n",
            "  RAW (1): ['No analysis found for: gelecek misin']\n",
            "  FILTERED (2): [{'token': 'gelecek', 'best_analysis': 'gel+VERB+FUT+2SG', 'tag': 'VERB'}, {'token': 'misin', 'best_analysis': 'mi+QUES+2SG', 'tag': 'QUES'}]\n",
            "\n",
            "yapacak mısın:\n",
            "  RAW (1): ['No analysis found for: yapacak mısın']\n",
            "  FILTERED (2): [{'token': 'yapacak', 'best_analysis': 'yapacak+NOUN+UNKNOWN', 'tag': 'NOUN'}, {'token': 'mısın', 'best_analysis': 'mı+QUES+2SG', 'tag': 'QUES'}]\n",
            "\n",
            "gidiyorum:\n",
            "  RAW (1): ['git+VERB+PRES.CONT+1SG']\n",
            "  FILTERED (1): [{'token': 'gidiyorum', 'best_analysis': 'git+VERB+PRES.CONT+1SG', 'tag': 'VERB'}]\n",
            "\n",
            "ali çıktı:\n",
            "  RAW (1): ['No analysis found for: ali çıktı']\n",
            "  FILTERED (2): [{'token': 'ali', 'best_analysis': 'ali+NOUN+UNKNOWN', 'tag': 'NOUN'}, {'token': 'çıktı', 'best_analysis': 'çıktı+NOUN+UNKNOWN', 'tag': 'NOUN'}]\n",
            "\n",
            "gelecek:\n",
            "  RAW (1): ['gel+VERB+FUT+3SG']\n",
            "  FILTERED (1): [{'token': 'gelecek', 'best_analysis': 'gel+VERB+FUT+3SG', 'tag': 'VERB'}]\n",
            "\n",
            "ben:\n",
            "  RAW (2): ['ben+PRON', 'ben+PRON+IMP+2SG']\n",
            "  FILTERED (1): [{'token': 'ben', 'best_analysis': 'ben+PRON', 'tag': 'PRON'}]\n",
            "\n",
            "kitapda:\n",
            "  RAW (1): ['kitap+NOUN+LOC']\n",
            "  FILTERED (1): [{'token': 'kitapda', 'best_analysis': 'kitap+NOUN+LOC', 'tag': 'NOUN'}]\n",
            "\n",
            "kitap daki:\n",
            "  RAW (1): ['No analysis found for: kitap daki']\n",
            "  FILTERED (2): [{'token': 'kitap', 'best_analysis': 'kitap+NOUN', 'tag': 'NOUN'}, {'token': 'daki', 'best_analysis': '+LOC+KI', 'tag': 'ADJ'}]\n",
            "\n",
            "kitapta:\n",
            "  RAW (1): ['kitap+NOUN+LOC']\n",
            "  FILTERED (1): [{'token': 'kitapta', 'best_analysis': 'kitap+NOUN+LOC', 'tag': 'NOUN'}]\n",
            "\n",
            "bende:\n",
            "  RAW (1): ['ben+PRON+LOC']\n",
            "  FILTERED (1): [{'token': 'bende', 'best_analysis': 'ben+PRON+LOC', 'tag': 'PRON'}]\n",
            "\n",
            "ben de:\n",
            "  RAW (1): ['No analysis found for: ben de']\n",
            "  FILTERED (2): [{'token': 'ben', 'best_analysis': 'ben+PRON', 'tag': 'PRON'}, {'token': 'de', 'best_analysis': 'de+CONJ', 'tag': 'CONJ'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_sentence_context_aware(\"bana gül\")"
      ],
      "metadata": {
        "id": "GwNC0Vj63whs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfa4ebe-5fd4-469e-e064-b4b16248de58"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'token': 'bana', 'best_analysis': 'ben+PRON+DAT', 'tag': 'PRON'},\n",
              " {'token': 'gül', 'best_analysis': 'gül+NOUN+UNKNOWN', 'tag': 'NOUN'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_sentence_context_aware(\"yapacak mısın\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rer0dwxpCorP",
        "outputId": "fbfdd4de-46d5-4cfb-afe4-000138286331"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'token': 'yapacak', 'best_analysis': 'yapacak+NOUN+UNKNOWN', 'tag': 'NOUN'},\n",
              " {'token': 'mısın', 'best_analysis': 'mı+QUES+2SG', 'tag': 'QUES'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Case for Conjunctions\n",
        "print(analyze_sentence_context_aware(\"kitap da masada\"))\n",
        "# Expected:\n",
        "# kitap -> NOUN\n",
        "# da -> CONJ (because 'da' is in lexicon['conjunctions'])\n",
        "# masada -> NOUN (masa + LOC)\n",
        "\n",
        "# Test Case for Questions\n",
        "print(analyze_sentence_context_aware(\"gelecek misin\"))\n",
        "# Expected:\n",
        "# gelecek -> VERB (FUT) or NOUN (Future) -> Context prefers VERB if at end?\n",
        "# misin -> QUES (mi + 2SG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtfwAkzk6udm",
        "outputId": "da8917f5-57b9-4a92-ce7d-4e70920ce3d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'token': 'kitap', 'best_analysis': 'kitap+NOUN', 'tag': 'NOUN'}, {'token': 'da', 'best_analysis': 'da+CONJ', 'tag': 'CONJ'}, {'token': 'masada', 'best_analysis': 'masa+NOUN+LOC', 'tag': 'NOUN'}]\n",
            "[{'token': 'gelecek', 'best_analysis': 'gel+VERB+FUT+2SG', 'tag': 'VERB'}, {'token': 'misin', 'best_analysis': 'mi+QUES+2SG', 'tag': 'QUES'}]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}