{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6txQNlZvQ8P",
        "outputId": "47c03294-ca84-4c6e-9fb5-7bb3750b883c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pynini\n",
            "  Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (165.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.5/165.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynini\n",
            "Successfully installed pynini-2.1.7\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install --only-binary :all: pynini\n",
        "!pip install wurlitzer\n",
        "import pynini\n",
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_HcAl-2_lcJ",
        "outputId": "f01a22c9-9114-45c3-ae4f-e5769d693e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LEXICON LOADED\n",
            "============================================================\n",
            "Nouns: 56691 words\n",
            "Verbs: 3455 words (infinitives → roots extracted)\n",
            "Adjectives: 5504 words\n",
            "Pronouns: 50 words\n",
            "Adverbs: 1300 words\n",
            "Conjunctions: 53 words\n",
            "Postpositions: 0 words\n",
            "Proper Nouns: 0 words\n",
            "\n",
            "Verb root examples:\n",
            "  polenlemek → polenle\n",
            "  kubaşmak → kubaş\n",
            "  sayıklamak → sayıkla\n",
            "  ölçeklemek → ölçekle\n",
            "  atkılamak → atkıla\n",
            "\n",
            "============================================================\n",
            "DEBUG: Testing verb compositions\n",
            "============================================================\n",
            "\n",
            "'gel' + verb_imperative:\n",
            "  ✓ gel+VERB+IMP+2SG\n",
            "\n",
            "'gelsin' + verb_imperative:\n",
            "  ✓ gel+VERB+IMP+3SG\n",
            "\n",
            "============================================================\n",
            "SINGLE WORD ANALYSIS\n",
            "============================================================\n",
            "\n",
            "gel:\n",
            "  gel+VERB+IMP+2SG\n",
            "\n",
            "gelin:\n",
            "  gel+VERB+IMP+2PL\n",
            "  gel+VERB+REFL+IMP+2SG\n",
            "  gelin+NOUN\n",
            "\n",
            "gelsin:\n",
            "  gel+VERB+IMP+3SG\n",
            "\n",
            "gelsem:\n",
            "  gel+VERB+COND+1SG\n",
            "\n",
            "geleyim:\n",
            "  gel+VERB+OPT+1SG\n",
            "\n",
            "gelmeli:\n",
            "  gel+VERB+NEC+3SG\n",
            "\n",
            "yazmalıyım:\n",
            "  yaz+VERB+NEC+1SG\n",
            "\n",
            "okusana:\n",
            "  oku+VERB+OPT+2SG+EMPH\n",
            "\n",
            "görseler:\n",
            "  gör+VERB+COND+3PL\n",
            "\n",
            "okudum:\n",
            "  oku+VERB+PAST+1SG\n",
            "\n",
            "gelebilecek:\n",
            "  gel+VERB+ABIL+FUT+3SG\n",
            "\n",
            "geliyorum:\n",
            "  gel+VERB+PRES.CONT+1SG\n",
            "\n",
            "kitaplardan:\n",
            "  kitap+NOUN+PL+ABL\n",
            "\n",
            "kalemlik:\n",
            "  kalem+NOUN+DER.lik\n",
            "\n",
            "evdekiler:\n",
            "  ev+NOUN+LOC+KI+PL\n",
            "\n",
            "============================================================\n",
            "MULTI-WORD ANALYSIS\n",
            "============================================================\n",
            "\n",
            "'evde misin?':\n",
            "  evde:\n",
            "    ev+NOUN+LOC\n",
            "  misin?:\n",
            "    mi+QUES+2SG+PUNCT.question\n",
            "\n",
            "'kitap da güzel':\n",
            "  kitap:\n",
            "    kitap+NOUN\n",
            "  da:\n",
            "    +LOC\n",
            "    da+CONJ\n",
            "  güzel:\n",
            "    güzel+ADJ\n",
            "    güzel+ADV\n",
            "    güzel+NOUN\n",
            "\n",
            "'gel buraya!':\n",
            "  gel:\n",
            "    gel+VERB+IMP+2SG\n",
            "  buraya!:\n",
            "    bura+NOUN+DAT+PUNCT.exclamation\n",
            "\n",
            "'oraya git':\n",
            "  oraya:\n",
            "    ora+NOUN+DAT\n",
            "  git:\n",
            "    git+VERB+IMP+2SG\n",
            "\n",
            "'yukarı çık':\n",
            "  yukarı:\n",
            "    yukarı+ADJ\n",
            "    yukarı+ADV\n",
            "    yukarı+NOUN\n",
            "  çık:\n",
            "    çık+VERB+IMP+2SG\n",
            "\n",
            "'ben yemeğe gidiyorum gelecek misin?':\n",
            "  ben:\n",
            "    ben+NOUN\n",
            "    ben+PRON\n",
            "    ben+PRON\n",
            "  yemeğe:\n",
            "    yemek+NOUN+DAT\n",
            "  gidiyorum:\n",
            "    git+VERB+PRES.CONT+1SG\n",
            "  gelecek:\n",
            "    gel+VERB+FUT+3SG\n",
            "    gel+VERB+FUT+3SG\n",
            "    gelecek+ADJ\n",
            "    ... and 3 more\n",
            "  misin?:\n",
            "    mi+QUES+2SG+PUNCT.question\n",
            "\n",
            "'kitabı aldım':\n",
            "  kitabı:\n",
            "    kitap+NOUN+ACC\n",
            "  aldım:\n",
            "    al+VERB+PAST+1SG\n",
            "\n",
            "'ağaca çıktı':\n",
            "  ağaca:\n",
            "    ağa+ADJ+EQU\n",
            "    ağa+NOUN+EQU\n",
            "    ağaç+ADJ+DAT\n",
            "    ... and 1 more\n",
            "  çıktı:\n",
            "    çık+VERB+PAST+3SG\n",
            "    çık+VERB+PAST+3SG\n",
            "    çıktı+NOUN\n",
            "\n",
            "============================================================\n",
            "✅ TURKISH MORPHOLOGICAL ANALYZER - COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Lexical Categories:\n",
            "  ✓ Ad (Noun), Sıfat (Adjective), Zamir (Pronoun)\n",
            "  ✓ Fiil (Verb) with all moods\n",
            "  ✓ Zarf (Adverb), Edat (Postposition)\n",
            "  ✓ Ünlem (Interjection), Bağlaç (Conjunction)\n",
            "  ✓ Question Particles\n",
            "\n",
            "Verb Moods (Bildirme + Dilek Kipleri):\n",
            "  ✓ Present Continuous, Future, Aorist, Past, Inferential\n",
            "  ✓ Optative (İstek), Conditional (Şart), Necessitative (Gereklilik)\n",
            "  ✓ Imperative (Emir)\n",
            "\n",
            "Features:\n",
            "  ✓ Derivational suffixes (-lık, -lik, etc.)\n",
            "  ✓ Voice (passive, reflexive, reciprocal, causative)\n",
            "  ✓ Ability (-ebil/-abil)\n",
            "  ✓ Negation (-ma/-me)\n",
            "  ✓ Punctuation handling\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import pynini\n",
        "import json\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# ===== LOAD LEXICON FROM JSON =====\n",
        "def load_lexicon(json_file='turkish_lexicon.json'):\n",
        "    \"\"\"Load Turkish lexicon from JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {json_file} not found. Using default minimal lexicon.\")\n",
        "        return {\n",
        "            \"nouns\": [\"ev\", \"kitap\", \"masa\"],\n",
        "            \"verbs\": [\"gel\", \"git\", \"oku\"],\n",
        "            \"adjectives\": [\"güzel\", \"iyi\"],\n",
        "            \"pronouns\": [\"ben\", \"sen\", \"o\"],\n",
        "            \"adverbs\": [\"çok\", \"az\"],\n",
        "            \"conjunctions\": [\"ve\", \"da\", \"de\"],\n",
        "            \"postpositions\": [\"gibi\", \"için\"],\n",
        "            \"proper_nouns\": []\n",
        "        }\n",
        "\n",
        "# Load lexicon\n",
        "lexicon = load_lexicon()\n",
        "\n",
        "# ===== HELPER FUNCTIONS =====\n",
        "def extract_verb_root(verb_infinitive):\n",
        "    \"\"\"Extract verb root from infinitive form (remove -mak/-mek).\"\"\"\n",
        "    if verb_infinitive.endswith('mak'):\n",
        "        return verb_infinitive[:-3]\n",
        "    elif verb_infinitive.endswith('mek'):\n",
        "        return verb_infinitive[:-3]\n",
        "    else:\n",
        "        # If it doesn't end with -mak/-mek, return as is (might be already a root)\n",
        "        return verb_infinitive\n",
        "\n",
        "def create_alternating_roots(words, tag):\n",
        "    \"\"\"\n",
        "    Create FST roots with consonant softening (ünsüz yumuşaması).\n",
        "    p→b, ç→c, t→d, k→g/ğ when followed by vowel-initial suffix.\n",
        "    \"\"\"\n",
        "    roots = []\n",
        "\n",
        "    for word in words:\n",
        "        # Original form (before consonant-initial suffixes)\n",
        "        roots.append(pynini.cross(word, f\"{word}+{tag}\"))\n",
        "\n",
        "        # Check if word ends with p, ç, t, k (sert ünsüz)\n",
        "        if len(word) > 1 and word[-1] in ['p', 'ç', 't', 'k']:\n",
        "            # Get the stem without final consonant\n",
        "            stem = word[:-1]\n",
        "            final = word[-1]\n",
        "\n",
        "            # Determine softened form (yumuşak ünsüz)\n",
        "            if final == 'p':\n",
        "                softened = stem + 'b'\n",
        "            elif final == 'ç':\n",
        "                softened = stem + 'c'\n",
        "            elif final == 't':\n",
        "                softened = stem + 'd'\n",
        "            elif final == 'k':\n",
        "                # k → ğ after vowels, k → g after consonants\n",
        "                if len(stem) > 0 and stem[-1] in 'aeıioöuü':\n",
        "                    softened = stem + 'ğ'\n",
        "                else:\n",
        "                    softened = stem + 'g'\n",
        "\n",
        "            # Add softened form (before vowel-initial suffixes)\n",
        "            roots.append(pynini.cross(softened, f\"{word}+{tag}\"))\n",
        "\n",
        "    return pynini.union(*roots) if roots else pynini.cross(\"\", \"\")\n",
        "\n",
        "# ===== ROOTS BY LEXICAL CATEGORY =====\n",
        "\n",
        "# Nouns (Ad) - with consonant softening\n",
        "noun_roots = create_alternating_roots(\n",
        "    lexicon.get('nouns', []),\n",
        "    'NOUN'\n",
        ") if lexicon.get('nouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Adjectives (Sıfat) - with consonant softening\n",
        "adj_roots = create_alternating_roots(\n",
        "    lexicon.get('adjectives', []),\n",
        "    'ADJ'\n",
        ") if lexicon.get('adjectives') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Verbs (Fiil) - extract roots and apply consonant softening\n",
        "verb_root_list = [extract_verb_root(word) for word in lexicon.get('verbs', [])]\n",
        "verb_roots = create_alternating_roots(\n",
        "    verb_root_list,\n",
        "    'VERB'\n",
        ") if verb_root_list else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Pronouns (Zamir) - usually don't undergo consonant softening, but include for completeness\n",
        "pronoun_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+PRON\")\n",
        "    for word in lexicon.get('pronouns', [])\n",
        "]) if lexicon.get('pronouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Adverbs (Zarf) - no inflection\n",
        "adverb_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+ADV\")\n",
        "    for word in lexicon.get('adverbs', [])\n",
        "]) if lexicon.get('adverbs') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Postpositions (Edat) - no inflection\n",
        "postposition_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+POSTP\")\n",
        "    for word in lexicon.get('postpositions', [])\n",
        "]) if lexicon.get('postpositions') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Interjections (Ünlem) - no inflection\n",
        "interjection_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+INTERJ\")\n",
        "    for word in lexicon.get('interjections', [])\n",
        "]) if lexicon.get('interjections') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Conjunctions (Bağlaç) - no inflection\n",
        "conjunction_roots = pynini.union(*[\n",
        "    pynini.cross(word, f\"{word}+CONJ\")\n",
        "    for word in lexicon.get('conjunctions', [])\n",
        "]) if lexicon.get('conjunctions') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Proper Nouns (Özel İsimler) - with consonant softening\n",
        "proper_noun_roots = create_alternating_roots(\n",
        "    lexicon.get('proper_nouns', []),\n",
        "    'PROPN'\n",
        ") if lexicon.get('proper_nouns') else pynini.cross(\"\", \"\")\n",
        "\n",
        "# Question Particles\n",
        "question_particles = pynini.union(\n",
        "    pynini.cross(\"mi\", \"mi+QUES\"),\n",
        "    pynini.cross(\"mı\", \"mı+QUES\"),\n",
        "    pynini.cross(\"mu\", \"mu+QUES\"),\n",
        "    pynini.cross(\"mü\", \"mü+QUES\"),\n",
        "    pynini.cross(\"misin\", \"mi+QUES+2SG\"),\n",
        "    pynini.cross(\"mısın\", \"mı+QUES+2SG\"),\n",
        "    pynini.cross(\"musun\", \"mu+QUES+2SG\"),\n",
        "    pynini.cross(\"müsün\", \"mü+QUES+2SG\"),\n",
        "    pynini.cross(\"miyim\", \"mi+QUES+1SG\"),\n",
        "    pynini.cross(\"mıyım\", \"mı+QUES+1SG\"),\n",
        "    pynini.cross(\"muyum\", \"mu+QUES+1SG\"),\n",
        "    pynini.cross(\"müyüm\", \"mü+QUES+1SG\"),\n",
        "    pynini.cross(\"miyiz\", \"mi+QUES+1PL\"),\n",
        "    pynini.cross(\"mıyız\", \"mı+QUES+1PL\"),\n",
        "    pynini.cross(\"muyuz\", \"mu+QUES+1PL\"),\n",
        "    pynini.cross(\"müyüz\", \"mü+QUES+1PL\"),\n",
        "    pynini.cross(\"misiniz\", \"mi+QUES+2PL\"),\n",
        "    pynini.cross(\"mısınız\", \"mı+QUES+2PL\"),\n",
        "    pynini.cross(\"musunuz\", \"mu+QUES+2PL\"),\n",
        "    pynini.cross(\"müsünüz\", \"mü+QUES+2PL\")\n",
        ")\n",
        "\n",
        "# ===== DERIVATIONAL SUFFIXES =====\n",
        "derivational = pynini.union(\n",
        "    pynini.cross(\"lık\", \"+DER.lık\"), pynini.cross(\"lik\", \"+DER.lik\"),\n",
        "    pynini.cross(\"luk\", \"+DER.luk\"), pynini.cross(\"lük\", \"+DER.lük\"),\n",
        "    pynini.cross(\"cı\", \"+DER.cı\"), pynini.cross(\"ci\", \"+DER.ci\"),\n",
        "    pynini.cross(\"cu\", \"+DER.cu\"), pynini.cross(\"cü\", \"+DER.cü\"),\n",
        "    pynini.cross(\"çı\", \"+DER.çı\"), pynini.cross(\"çi\", \"+DER.çi\"),\n",
        "    pynini.cross(\"çu\", \"+DER.çu\"), pynini.cross(\"çü\", \"+DER.çü\"),\n",
        "    pynini.cross(\"sız\", \"+DER.sız\"), pynini.cross(\"siz\", \"+DER.siz\"),\n",
        "    pynini.cross(\"suz\", \"+DER.suz\"), pynini.cross(\"süz\", \"+DER.süz\"),\n",
        "    pynini.cross(\"lı\", \"+DER.lı\"), pynini.cross(\"li\", \"+DER.li\"),\n",
        "    pynini.cross(\"lu\", \"+DER.lu\"), pynini.cross(\"lü\", \"+DER.lü\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# ===== NOUN/ADJECTIVE MORPHOLOGY =====\n",
        "# Include proper nouns that can take case markers\n",
        "nominal_roots = pynini.union(noun_roots, adj_roots, pronoun_roots, proper_noun_roots)\n",
        "nominal_derived = nominal_roots + derivational\n",
        "\n",
        "plural = pynini.union(\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "nominal_pl = nominal_derived + plural\n",
        "\n",
        "possessive = pynini.union(\n",
        "    pynini.cross(\"imiz\", \"+POSS.1PL\"), pynini.cross(\"ımız\", \"+POSS.1PL\"),\n",
        "    pynini.cross(\"umuz\", \"+POSS.1PL\"), pynini.cross(\"ümüz\", \"+POSS.1PL\"),\n",
        "    pynini.cross(\"iniz\", \"+POSS.2PL\"), pynini.cross(\"ınız\", \"+POSS.2PL\"),\n",
        "    pynini.cross(\"unuz\", \"+POSS.2PL\"), pynini.cross(\"ünüz\", \"+POSS.2PL\"),\n",
        "    pynini.cross(\"leri\", \"+POSS.3PL\"), pynini.cross(\"ları\", \"+POSS.3PL\"),\n",
        "    pynini.cross(\"im\", \"+POSS.1SG\"), pynini.cross(\"ım\", \"+POSS.1SG\"),\n",
        "    pynini.cross(\"um\", \"+POSS.1SG\"), pynini.cross(\"üm\", \"+POSS.1SG\"),\n",
        "    pynini.cross(\"in\", \"+POSS.2SG\"), pynini.cross(\"ın\", \"+POSS.2SG\"),\n",
        "    pynini.cross(\"un\", \"+POSS.2SG\"), pynini.cross(\"ün\", \"+POSS.2SG\"),\n",
        "    pynini.cross(\"si\", \"+POSS.3SG\"), pynini.cross(\"sı\", \"+POSS.3SG\"),\n",
        "    pynini.cross(\"su\", \"+POSS.3SG\"), pynini.cross(\"sü\", \"+POSS.3SG\")\n",
        ")\n",
        "\n",
        "case_after_poss = pynini.union(\n",
        "    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n",
        "    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n",
        "    pynini.cross(\"ndan\", \"+ABL\"), pynini.cross(\"nden\", \"+ABL\"),\n",
        "    pynini.cross(\"ntan\", \"+ABL\"), pynini.cross(\"nten\", \"+ABL\"),\n",
        "    pynini.cross(\"nın\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n",
        "    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nün\", \"+GEN\"),\n",
        "    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n",
        "    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n",
        "    pynini.cross(\"nda\", \"+LOC\"), pynini.cross(\"nde\", \"+LOC\"),\n",
        "    pynini.cross(\"nta\", \"+LOC\"), pynini.cross(\"nte\", \"+LOC\"),\n",
        "    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n",
        "    pynini.cross(\"na\", \"+DAT\"), pynini.cross(\"ne\", \"+DAT\"),\n",
        "    pynini.cross(\"yı\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n",
        "    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yü\", \"+ACC\"),\n",
        "    pynini.cross(\"nı\", \"+ACC\"), pynini.cross(\"ni\", \"+ACC\"),\n",
        "    pynini.cross(\"nu\", \"+ACC\"), pynini.cross(\"nü\", \"+ACC\"),\n",
        "    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n",
        "    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ki_suffix = pynini.union(\n",
        "    pynini.cross(\"ki\", \"+KI\"),\n",
        "    pynini.cross(\"kü\", \"+KI\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "plural_after_ki = pynini.union(\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "possessive_path = nominal_pl + possessive + case_after_poss + ki_suffix + plural_after_ki\n",
        "\n",
        "case_no_poss = pynini.union(\n",
        "    pynini.cross(\"ların\", \"+GEN\"), pynini.cross(\"lerin\", \"+GEN\"),\n",
        "    pynini.cross(\"dan\", \"+ABL\"), pynini.cross(\"den\", \"+ABL\"),\n",
        "    pynini.cross(\"tan\", \"+ABL\"), pynini.cross(\"ten\", \"+ABL\"),\n",
        "    pynini.cross(\"nın\", \"+GEN\"), pynini.cross(\"nin\", \"+GEN\"),\n",
        "    pynini.cross(\"nun\", \"+GEN\"), pynini.cross(\"nün\", \"+GEN\"),\n",
        "    pynini.cross(\"da\", \"+LOC\"), pynini.cross(\"de\", \"+LOC\"),\n",
        "    pynini.cross(\"ta\", \"+LOC\"), pynini.cross(\"te\", \"+LOC\"),\n",
        "    pynini.cross(\"ya\", \"+DAT\"), pynini.cross(\"ye\", \"+DAT\"),\n",
        "    pynini.cross(\"a\", \"+DAT\"), pynini.cross(\"e\", \"+DAT\"),\n",
        "    pynini.cross(\"yı\", \"+ACC\"), pynini.cross(\"yi\", \"+ACC\"),\n",
        "    pynini.cross(\"yu\", \"+ACC\"), pynini.cross(\"yü\", \"+ACC\"),\n",
        "    pynini.cross(\"ı\", \"+ACC\"), pynini.cross(\"i\", \"+ACC\"),\n",
        "    pynini.cross(\"u\", \"+ACC\"), pynini.cross(\"ü\", \"+ACC\"),\n",
        "    pynini.cross(\"la\", \"+INS\"), pynini.cross(\"le\", \"+INS\"),\n",
        "    pynini.cross(\"yla\", \"+INS\"), pynini.cross(\"yle\", \"+INS\"),\n",
        "    pynini.cross(\"ca\", \"+EQU\"), pynini.cross(\"ce\", \"+EQU\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ki_suffix_case = pynini.union(\n",
        "    pynini.cross(\"ki\", \"+KI\"),\n",
        "    pynini.cross(\"kü\", \"+KI\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "plural_after_ki_case = pynini.union(\n",
        "    pynini.cross(\"ler\", \"+PL\"),\n",
        "    pynini.cross(\"lar\", \"+PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "case_only_path = nominal_pl + case_no_poss + ki_suffix_case + plural_after_ki_case\n",
        "\n",
        "nominal_base = pynini.union(possessive_path, case_only_path).optimize()\n",
        "\n",
        "copula = pynini.union(\n",
        "    pynini.cross(\"ydi\", \"+COP.PAST\"), pynini.cross(\"ydı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ydu\", \"+COP.PAST\"), pynini.cross(\"ydü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ymış\", \"+COP.EVID\"), pynini.cross(\"ymiş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"ymuş\", \"+COP.EVID\"), pynini.cross(\"ymüş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"yse\", \"+COP.COND\"), pynini.cross(\"ysa\", \"+COP.COND\"),\n",
        "    pynini.cross(\"di\", \"+COP.PAST\"), pynini.cross(\"dı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"du\", \"+COP.PAST\"), pynini.cross(\"dü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"ti\", \"+COP.PAST\"), pynini.cross(\"tı\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"tu\", \"+COP.PAST\"), pynini.cross(\"tü\", \"+COP.PAST\"),\n",
        "    pynini.cross(\"miş\", \"+COP.EVID\"), pynini.cross(\"mış\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"muş\", \"+COP.EVID\"), pynini.cross(\"müş\", \"+COP.EVID\"),\n",
        "    pynini.cross(\"se\", \"+COP.COND\"), pynini.cross(\"sa\", \"+COP.COND\"),\n",
        "    pynini.cross(\"dir\", \"+COP.PRES\"), pynini.cross(\"dır\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"dur\", \"+COP.PRES\"), pynini.cross(\"dür\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"tir\", \"+COP.PRES\"), pynini.cross(\"tır\", \"+COP.PRES\"),\n",
        "    pynini.cross(\"tur\", \"+COP.PRES\"), pynini.cross(\"tür\", \"+COP.PRES\")\n",
        ")\n",
        "\n",
        "person = pynini.union(\n",
        "    pynini.cross(\"im\", \"+1SG\"), pynini.cross(\"ım\", \"+1SG\"),\n",
        "    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"üm\", \"+1SG\"),\n",
        "    pynini.cross(\"in\", \"+2SG\"), pynini.cross(\"ın\", \"+2SG\"),\n",
        "    pynini.cross(\"un\", \"+2SG\"), pynini.cross(\"ün\", \"+2SG\"),\n",
        "    pynini.cross(\"ız\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n",
        "    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"üz\", \"+1PL\"),\n",
        "    pynini.cross(\"nız\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n",
        "    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nüz\", \"+2PL\"),\n",
        "    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "nominal_with_cop = copula + person\n",
        "nominal_without_cop = pynini.cross(\"\", \"\")\n",
        "nominal_complete = nominal_base + pynini.union(nominal_with_cop, nominal_without_cop).optimize()\n",
        "\n",
        "# ===== VERB MORPHOLOGY =====\n",
        "\n",
        "# Voice, Ability, Negation (all optional)\n",
        "voice = pynini.union(\n",
        "    pynini.cross(\"ıl\", \"+PASS\"), pynini.cross(\"il\", \"+PASS\"),\n",
        "    pynini.cross(\"ul\", \"+PASS\"), pynini.cross(\"ül\", \"+PASS\"),\n",
        "    pynini.cross(\"ın\", \"+REFL\"), pynini.cross(\"in\", \"+REFL\"),\n",
        "    pynini.cross(\"un\", \"+REFL\"), pynini.cross(\"ün\", \"+REFL\"),\n",
        "    pynini.cross(\"lan\", \"+REFL\"), pynini.cross(\"len\", \"+REFL\"),\n",
        "    pynini.cross(\"ış\", \"+RECIP\"), pynini.cross(\"iş\", \"+RECIP\"),\n",
        "    pynini.cross(\"uş\", \"+RECIP\"), pynini.cross(\"üş\", \"+RECIP\"),\n",
        "    pynini.cross(\"t\", \"+CAUS\"), pynini.cross(\"d\", \"+CAUS\"),\n",
        "    pynini.cross(\"dır\", \"+CAUS\"), pynini.cross(\"dir\", \"+CAUS\"),\n",
        "    pynini.cross(\"dur\", \"+CAUS\"), pynini.cross(\"dür\", \"+CAUS\"),\n",
        "    pynini.cross(\"tır\", \"+CAUS\"), pynini.cross(\"tir\", \"+CAUS\"),\n",
        "    pynini.cross(\"tur\", \"+CAUS\"), pynini.cross(\"tür\", \"+CAUS\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "ability = pynini.union(\n",
        "    pynini.cross(\"ebil\", \"+ABIL\"),\n",
        "    pynini.cross(\"abil\", \"+ABIL\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "negation = pynini.union(\n",
        "    pynini.cross(\"ma\", \"+NEG\"),\n",
        "    pynini.cross(\"me\", \"+NEG\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# Bildirme Kipleri (Indicative)\n",
        "indicative_tense = pynini.union(\n",
        "    pynini.cross(\"iyor\", \"+PRES.CONT\"), pynini.cross(\"ıyor\", \"+PRES.CONT\"),\n",
        "    pynini.cross(\"uyor\", \"+PRES.CONT\"), pynini.cross(\"üyor\", \"+PRES.CONT\"),\n",
        "    pynini.cross(\"ecek\", \"+FUT\"), pynini.cross(\"acak\", \"+FUT\"),\n",
        "    pynini.cross(\"ır\", \"+AOR\"), pynini.cross(\"ir\", \"+AOR\"),\n",
        "    pynini.cross(\"ur\", \"+AOR\"), pynini.cross(\"ür\", \"+AOR\"),\n",
        "    pynini.cross(\"ar\", \"+AOR\"), pynini.cross(\"er\", \"+AOR\"),\n",
        "    pynini.cross(\"r\", \"+AOR\"),\n",
        "    pynini.cross(\"dı\", \"+PAST\"), pynini.cross(\"di\", \"+PAST\"),\n",
        "    pynini.cross(\"du\", \"+PAST\"), pynini.cross(\"dü\", \"+PAST\"),\n",
        "    pynini.cross(\"tı\", \"+PAST\"), pynini.cross(\"ti\", \"+PAST\"),\n",
        "    pynini.cross(\"tu\", \"+PAST\"), pynini.cross(\"tü\", \"+PAST\"),\n",
        "    pynini.cross(\"mış\", \"+INFER\"), pynini.cross(\"miş\", \"+INFER\"),\n",
        "    pynini.cross(\"muş\", \"+INFER\"), pynini.cross(\"müş\", \"+INFER\")\n",
        ")\n",
        "\n",
        "indicative_person = pynini.union(\n",
        "    pynini.cross(\"um\", \"+1SG\"), pynini.cross(\"üm\", \"+1SG\"),\n",
        "    pynini.cross(\"ım\", \"+1SG\"), pynini.cross(\"im\", \"+1SG\"),\n",
        "    pynini.cross(\"m\", \"+1SG\"),\n",
        "    pynini.cross(\"sun\", \"+2SG\"), pynini.cross(\"sün\", \"+2SG\"),\n",
        "    pynini.cross(\"sın\", \"+2SG\"), pynini.cross(\"sin\", \"+2SG\"),\n",
        "    pynini.cross(\"n\", \"+2SG\"),\n",
        "    pynini.cross(\"uz\", \"+1PL\"), pynini.cross(\"üz\", \"+1PL\"),\n",
        "    pynini.cross(\"ız\", \"+1PL\"), pynini.cross(\"iz\", \"+1PL\"),\n",
        "    pynini.cross(\"k\", \"+1PL\"),\n",
        "    pynini.cross(\"sunuz\", \"+2PL\"), pynini.cross(\"sünüz\", \"+2PL\"),\n",
        "    pynini.cross(\"sınız\", \"+2PL\"), pynini.cross(\"siniz\", \"+2PL\"),\n",
        "    pynini.cross(\"nız\", \"+2PL\"), pynini.cross(\"niz\", \"+2PL\"),\n",
        "    pynini.cross(\"nuz\", \"+2PL\"), pynini.cross(\"nüz\", \"+2PL\"),\n",
        "    pynini.cross(\"lar\", \"+3PL\"), pynini.cross(\"ler\", \"+3PL\"),\n",
        "    pynini.cross(\"\", \"+3SG\")\n",
        ")\n",
        "\n",
        "# Dilek/Tasarlama Kipleri (Subjunctive/Optative)\n",
        "\n",
        "# İstek Kipi (Optative)\n",
        "optative_mood_person = pynini.union(\n",
        "    pynini.cross(\"ayım\", \"+OPT+1SG\"), pynini.cross(\"eyim\", \"+OPT+1SG\"),\n",
        "    pynini.cross(\"ayum\", \"+OPT+1SG\"), pynini.cross(\"eyüm\", \"+OPT+1SG\"),\n",
        "    pynini.cross(\"asın\", \"+OPT+2SG\"), pynini.cross(\"esin\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"asun\", \"+OPT+2SG\"), pynini.cross(\"esün\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"asana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"esene\", \"+OPT+2SG+EMPH\"),\n",
        "    pynini.cross(\"sana\", \"+OPT+2SG+EMPH\"), pynini.cross(\"sene\", \"+OPT+2SG+EMPH\"),\n",
        "    pynini.cross(\"asan\", \"+OPT+2SG\"), pynini.cross(\"esen\", \"+OPT+2SG\"),\n",
        "    pynini.cross(\"a\", \"+OPT+3SG\"), pynini.cross(\"e\", \"+OPT+3SG\"),\n",
        "    pynini.cross(\"alım\", \"+OPT+1PL\"), pynini.cross(\"elim\", \"+OPT+1PL\"),\n",
        "    pynini.cross(\"alum\", \"+OPT+1PL\"), pynini.cross(\"elüm\", \"+OPT+1PL\"),\n",
        "    pynini.cross(\"asınız\", \"+OPT+2PL\"), pynini.cross(\"esiniz\", \"+OPT+2PL\"),\n",
        "    pynini.cross(\"asunuz\", \"+OPT+2PL\"), pynini.cross(\"esünüz\", \"+OPT+2PL\"),\n",
        "    pynini.cross(\"alar\", \"+OPT+3PL\"), pynini.cross(\"eler\", \"+OPT+3PL\")\n",
        ")\n",
        "\n",
        "# Dilek-Koşul Kipi (Conditional)\n",
        "conditional_mood_person = pynini.union(\n",
        "    pynini.cross(\"sam\", \"+COND+1SG\"), pynini.cross(\"sem\", \"+COND+1SG\"),\n",
        "    pynini.cross(\"san\", \"+COND+2SG\"), pynini.cross(\"sen\", \"+COND+2SG\"),\n",
        "    pynini.cross(\"sa\", \"+COND+3SG\"), pynini.cross(\"se\", \"+COND+3SG\"),\n",
        "    pynini.cross(\"sak\", \"+COND+1PL\"), pynini.cross(\"sek\", \"+COND+1PL\"),\n",
        "    pynini.cross(\"sanız\", \"+COND+2PL\"), pynini.cross(\"seniz\", \"+COND+2PL\"),\n",
        "    pynini.cross(\"sanuz\", \"+COND+2PL\"), pynini.cross(\"senüz\", \"+COND+2PL\"),\n",
        "    pynini.cross(\"salar\", \"+COND+3PL\"), pynini.cross(\"seler\", \"+COND+3PL\")\n",
        ")\n",
        "\n",
        "# ...\n",
        "\n",
        "\n",
        "# Gereklilik Kipi (Necessitative)\n",
        "necessitative_mood_person = pynini.union(\n",
        "    pynini.cross(\"malıyım\", \"+NEC+1SG\"), pynini.cross(\"meliyim\", \"+NEC+1SG\"),\n",
        "    pynini.cross(\"malıyum\", \"+NEC+1SG\"), pynini.cross(\"meliyüm\", \"+NEC+1SG\"),\n",
        "    pynini.cross(\"malısın\", \"+NEC+2SG\"), pynini.cross(\"melisin\", \"+NEC+2SG\"),\n",
        "    pynini.cross(\"malısun\", \"+NEC+2SG\"), pynini.cross(\"melisün\", \"+NEC+2SG\"),\n",
        "    pynini.cross(\"malı\", \"+NEC+3SG\"), pynini.cross(\"meli\", \"+NEC+3SG\"),\n",
        "    pynini.cross(\"malıyız\", \"+NEC+1PL\"), pynini.cross(\"meliyiz\", \"+NEC+1PL\"),\n",
        "    pynini.cross(\"malıyuz\", \"+NEC+1PL\"), pynini.cross(\"meliyüz\", \"+NEC+1PL\"),\n",
        "    pynini.cross(\"malısınız\", \"+NEC+2PL\"), pynini.cross(\"melisiniz\", \"+NEC+2PL\"),\n",
        "    pynini.cross(\"malısunuz\", \"+NEC+2PL\"), pynini.cross(\"melisünüz\", \"+NEC+2PL\"),\n",
        "    pynini.cross(\"malılar\", \"+NEC+3PL\"), pynini.cross(\"meliler\", \"+NEC+3PL\")\n",
        ")\n",
        "\n",
        "# Emir Kipi (Imperative)\n",
        "imperative_mood_person = pynini.union(\n",
        "    pynini.cross(\"sin\", \"+IMP+3SG\"), pynini.cross(\"sın\", \"+IMP+3SG\"),\n",
        "    pynini.cross(\"sun\", \"+IMP+3SG\"), pynini.cross(\"sün\", \"+IMP+3SG\"),\n",
        "    pynini.cross(\"in\", \"+IMP+2PL\"), pynini.cross(\"ın\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"un\", \"+IMP+2PL\"), pynini.cross(\"ün\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"iniz\", \"+IMP+2PL\"), pynini.cross(\"ınız\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"unuz\", \"+IMP+2PL\"), pynini.cross(\"ünüz\", \"+IMP+2PL\"),\n",
        "    pynini.cross(\"sinler\", \"+IMP+3PL\"), pynini.cross(\"sınlar\", \"+IMP+3PL\"),\n",
        "    pynini.cross(\"sunlar\", \"+IMP+3PL\"), pynini.cross(\"sünler\", \"+IMP+3PL\")\n",
        ")\n",
        "\n",
        "imperative_2sg_bare = pynini.cross(\"\", \"+IMP+2SG\")\n",
        "\n",
        "# Build verb structure\n",
        "verb_base = verb_roots + voice + ability + negation\n",
        "\n",
        "# Verb paths\n",
        "verb_indicative = verb_base + indicative_tense + indicative_person\n",
        "verb_optative = verb_base + optative_mood_person\n",
        "verb_conditional = verb_base + conditional_mood_person\n",
        "verb_necessitative = verb_base + necessitative_mood_person\n",
        "verb_imperative = verb_base + pynini.union(imperative_mood_person, imperative_2sg_bare)\n",
        "\n",
        "# Combine all verb paths\n",
        "verb_complete = pynini.union(\n",
        "    verb_indicative,\n",
        "    verb_optative,\n",
        "    verb_conditional,\n",
        "    verb_necessitative,\n",
        "    verb_imperative\n",
        ").optimize()\n",
        "\n",
        "# ===== PUNCTUATION =====\n",
        "punctuation = pynini.union(\n",
        "    pynini.cross(\".\", \"+PUNCT.period\"),\n",
        "    pynini.cross(\",\", \"+PUNCT.comma\"),\n",
        "    pynini.cross(\"?\", \"+PUNCT.question\"),\n",
        "    pynini.cross(\"!\", \"+PUNCT.exclamation\"),\n",
        "    pynini.cross(\":\", \"+PUNCT.colon\"),\n",
        "    pynini.cross(\";\", \"+PUNCT.semicolon\"),\n",
        "    pynini.cross(\"\", \"\")\n",
        ")\n",
        "\n",
        "# ===== COMPLETE ANALYZER =====\n",
        "simple_categories = pynini.union(\n",
        "    adverb_roots,\n",
        "    postposition_roots,\n",
        "    interjection_roots,\n",
        "    conjunction_roots,\n",
        "    question_particles\n",
        ")\n",
        "\n",
        "nominal_fst = (nominal_complete + punctuation).optimize()\n",
        "verb_fst = (verb_complete + punctuation).optimize()\n",
        "simple_fst = (simple_categories + punctuation).optimize()\n",
        "\n",
        "turkish_analyzer = pynini.union(nominal_fst, verb_fst, simple_fst).optimize()\n",
        "\n",
        "def analyze(word):\n",
        "    \"\"\"Analyze a Turkish word and return all possible analyses.\"\"\"\n",
        "    try:\n",
        "        lattice = pynini.compose(word, turkish_analyzer)\n",
        "        analyses = []\n",
        "        seen = set()\n",
        "        try:\n",
        "            for path in lattice.paths().ostrings():\n",
        "                if path not in seen:\n",
        "                    analyses.append(path)\n",
        "                    seen.add(path)\n",
        "        except:\n",
        "            pass\n",
        "        return sorted(analyses) if analyses else [f\"No analysis found for: {word}\"]\n",
        "    except Exception as e:\n",
        "        return [f\"Error: {str(e)}\"]\n",
        "# ===== CONTEXT AWARENESS & DISAMBIGUATION =====\n",
        "\n",
        "class ContextAwareDisambiguator:\n",
        "    def __init__(self):\n",
        "        # 1. Define Transition Probabilities (Bigram Model)\n",
        "        # What is the probability of Tag B following Tag A?\n",
        "        # Higher number = more likely.\n",
        "        self.transitions = {\n",
        "            # Start of sentence usually starts with Noun, Pronoun, or Adverb\n",
        "            'START': {'NOUN': 0.4, 'PRON': 0.3, 'ADV': 0.1, 'VERB': 0.1, 'ADJ': 0.1},\n",
        "            \n",
        "            # Adjectives almost always modify Nouns\n",
        "            'ADJ': {'NOUN': 0.9, 'ADJ': 0.1, 'VERB': 0.01},\n",
        "            \n",
        "            # Nouns can be followed by almost anything, but often Postpositions or Verbs\n",
        "            'NOUN': {'VERB': 0.4, 'NOUN': 0.2, 'CONJ': 0.1, 'POSTP': 0.2, 'ADV': 0.1},\n",
        "            \n",
        "            # Pronouns act like Nouns\n",
        "            'PRON': {'VERB': 0.5, 'NOUN': 0.2, 'POSTP': 0.2, 'ADJ': 0.1},\n",
        "            \n",
        "            # Adverbs modify Verbs or Adjectives\n",
        "            'ADV': {'VERB': 0.6, 'ADJ': 0.3, 'ADV': 0.1},\n",
        "            \n",
        "            # Numbers (if you had them) precede Nouns\n",
        "            'NUM': {'NOUN': 0.95},\n",
        "            \n",
        "            # Verbs usually end clauses, followed by Punctuation or Conjunctions\n",
        "            'VERB': {'PUNCT': 0.8, 'CONJ': 0.1, 'NOUN': 0.05, 'PRON': 0.05},\n",
        "            \n",
        "            # Question particles\n",
        "            'QUES': {'PUNCT': 0.9, 'VERB': 0.1},\n",
        "            \n",
        "            # Default fallback\n",
        "            'DEFAULT': {'NOUN': 0.3, 'VERB': 0.3, 'ADJ': 0.1, 'ADV': 0.1, 'PRON': 0.1, 'PUNCT': 0.1}\n",
        "        }\n",
        "        \n",
        "        # Tags we care about extracting from the analysis string\n",
        "        self.tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'POSTP', 'CONJ', 'QUES', 'INTERJ']\n",
        "\n",
        "    def get_tag_from_analysis(self, analysis_str):\n",
        "        \"\"\"Extracts the primary POS tag from an analysis string.\"\"\"\n",
        "        if \"No analysis\" in analysis_str:\n",
        "            return \"UNKNOWN\"\n",
        "            \n",
        "        # Check specific special cases first\n",
        "        if \"+QUES\" in analysis_str: return \"QUES\"\n",
        "        if \"+PUNCT\" in analysis_str: return \"PUNCT\"\n",
        "        \n",
        "        # Search for standard tags\n",
        "        for tag in self.tags:\n",
        "            if f\"+{tag}\" in analysis_str:\n",
        "                return tag\n",
        "        return \"NOUN\" # Default fallback\n",
        "\n",
        "    def get_transition_prob(self, prev_tag, current_tag):\n",
        "        \"\"\"Returns the probability of current_tag following prev_tag.\"\"\"\n",
        "        if prev_tag in self.transitions:\n",
        "            return self.transitions[prev_tag].get(current_tag, 0.001) # Return small prob instead of 0\n",
        "        return self.transitions['DEFAULT'].get(current_tag, 0.001)\n",
        "\n",
        "    def heuristic_weight(self, word, analysis, tag, position, sentence_len):\n",
        "        \"\"\"\n",
        "        Apply rule-based boosts (Stronger Heuristics).\n",
        "        Returns a score boost (log probability).\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "        \n",
        "        # Rule 1: Suffix matching (Morphology hints)\n",
        "        if tag == \"VERB\":\n",
        "            if word.endswith((\"yor\", \"yorum\", \"yorsun\", \"dı\", \"di\", \"du\", \"dü\", \"acak\", \"ecek\", \"malı\", \"meli\")):\n",
        "                score += 2.0\n",
        "        elif tag == \"NOUN\":\n",
        "            if word.endswith((\"lar\", \"ler\", \"in\", \"un\", \"nın\", \"nin\", \"da\", \"de\", \"dan\", \"den\")):\n",
        "                score += 1.5\n",
        "        elif tag == \"QUES\":\n",
        "            if word.lower().startswith(\"mi\") or word.lower().startswith(\"mı\"):\n",
        "                score += 5.0 # Massive boost, almost certainly a question particle\n",
        "\n",
        "        # Rule 2: Sentence Position\n",
        "        # Turkish is SOV (Subject-Object-Verb). Verbs strictly preferred at end.\n",
        "        if position == sentence_len - 1:\n",
        "            if tag == \"VERB\": score += 1.5\n",
        "            if tag == \"QUES\": score += 1.5\n",
        "            if tag == \"NOUN\": score -= 0.5 # Sentences rarely end in raw nouns (unless copula)\n",
        "        \n",
        "        # Rule 3: Length heuristic (very short words usually aren't verbs unless imperative)\n",
        "        if len(word) <= 2 and tag == \"VERB\" and position != sentence_len - 1:\n",
        "            score -= 1.0\n",
        "\n",
        "        return score\n",
        "\n",
        "    def decode_sentence(self, sentence_tokens):\n",
        "        \"\"\"\n",
        "        Viterbi Algorithm for finding the most likely sequence of analyses.\n",
        "        \"\"\"\n",
        "        # 1. Get all possible analyses for every word\n",
        "        # structure: [ [{'analysis': '...', 'tag': 'NOUN'}, ...], ... ]\n",
        "        lattice = []\n",
        "        for word in sentence_tokens:\n",
        "            raw_analyses = analyze(word)\n",
        "            word_candidates = []\n",
        "            \n",
        "            # If FST fails, treat as Unknown Noun\n",
        "            if not raw_analyses or \"No analysis\" in raw_analyses[0]:\n",
        "                word_candidates.append({'analysis': f\"{word}+NOUN+UNKNOWN\", 'tag': 'NOUN', 'word': word})\n",
        "            else:\n",
        "                for ana in raw_analyses:\n",
        "                    tag = self.get_tag_from_analysis(ana)\n",
        "                    word_candidates.append({'analysis': ana, 'tag': tag, 'word': word})\n",
        "            lattice.append(word_candidates)\n",
        "\n",
        "        n = len(lattice)\n",
        "        if n == 0: return []\n",
        "\n",
        "        # 2. Viterbi Initialization\n",
        "        # best_scores[i][candidate_index] = max_score\n",
        "        # backpointers[i][candidate_index] = index_of_best_prev_candidate\n",
        "        best_scores = [ {} for _ in range(n) ]\n",
        "        backpointers = [ {} for _ in range(n) ]\n",
        "\n",
        "        # Step 0: Start of sentence\n",
        "        for i, candidate in enumerate(lattice[0]):\n",
        "            trans_prob = self.get_transition_prob('START', candidate['tag'])\n",
        "            heuristic = self.heuristic_weight(candidate['word'], candidate['analysis'], candidate['tag'], 0, n)\n",
        "            \n",
        "            # Use Log probability to prevent underflow\n",
        "            best_scores[0][i] = math.log(trans_prob) + heuristic\n",
        "\n",
        "        # 3. Viterbi Recursion (Forward Pass)\n",
        "        for t in range(1, n):\n",
        "            for i, curr_cand in enumerate(lattice[t]):\n",
        "                max_score = -float('inf')\n",
        "                best_prev_idx = -1\n",
        "                \n",
        "                # Try coming from every possible analysis of the previous word\n",
        "                for j, prev_cand in enumerate(lattice[t-1]):\n",
        "                    # Score = Previous Score + Transition(Prev->Curr) + Heuristic(Curr)\n",
        "                    prev_score = best_scores[t-1][j]\n",
        "                    trans_prob = self.get_transition_prob(prev_cand['tag'], curr_cand['tag'])\n",
        "                    heuristic = self.heuristic_weight(curr_cand['word'], curr_cand['analysis'], curr_cand['tag'], t, n)\n",
        "                    \n",
        "                    score = prev_score + math.log(trans_prob) + heuristic\n",
        "                    \n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        best_prev_idx = j\n",
        "                \n",
        "                best_scores[t][i] = max_score\n",
        "                backpointers[t][i] = best_prev_idx\n",
        "\n",
        "        # 4. Backtracking (Backward Pass)\n",
        "        # Find best end state\n",
        "        best_last_idx = max(best_scores[n-1], key=best_scores[n-1].get)\n",
        "        \n",
        "        result_path = []\n",
        "        curr_idx = best_last_idx\n",
        "        \n",
        "        for t in range(n-1, -1, -1):\n",
        "            cand = lattice[t][curr_idx]\n",
        "            result_path.append(cand)\n",
        "            if t > 0:\n",
        "                curr_idx = backpointers[t][curr_idx]\n",
        "                \n",
        "        return result_path[::-1] # Reverse to get correct order\n",
        "\n",
        "# Create instance\n",
        "disambiguator = ContextAwareDisambiguator()\n",
        "\n",
        "def analyze_sentence_context_aware(sentence):\n",
        "    \"\"\"Wrapper function to replace the old analyze_sentence\"\"\"\n",
        "    # Simple tokenization (handling punctuation loosely)\n",
        "    # In a real app, split punctuation marks effectively\n",
        "    import re\n",
        "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
        "    \n",
        "    results = disambiguator.decode_sentence(tokens)\n",
        "    \n",
        "    formatted_output = []\n",
        "    for item in results:\n",
        "        formatted_output.append({\n",
        "            'token': item['word'],\n",
        "            'best_analysis': item['analysis'],\n",
        "            'tag': item['tag']\n",
        "        })\n",
        "    return formatted_output\n",
        "\n",
        "def save_fst(filename):\n",
        "    \"\"\"Save the compiled FST to a file.\"\"\"\n",
        "    turkish_analyzer.write(filename)\n",
        "    print(f\"FST saved to {filename}\")\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"LEXICON LOADED\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Nouns: {len(lexicon.get('nouns', []))} words\")\n",
        "    print(f\"Verbs: {len(lexicon.get('verbs', []))} words (infinitives → roots extracted)\")\n",
        "    print(f\"Adjectives: {len(lexicon.get('adjectives', []))} words\")\n",
        "    print(f\"Pronouns: {len(lexicon.get('pronouns', []))} words\")\n",
        "    print(f\"Adverbs: {len(lexicon.get('adverbs', []))} words\")\n",
        "    print(f\"Conjunctions: {len(lexicon.get('conjunctions', []))} words\")\n",
        "    print(f\"Postpositions: {len(lexicon.get('postpositions', []))} words\")\n",
        "    print(f\"Proper Nouns: {len(lexicon.get('proper_nouns', []))} words\")\n",
        "\n",
        "    # Show some verb root examples\n",
        "    if lexicon.get('verbs'):\n",
        "        print(\"\\nVerb root examples:\")\n",
        "        for verb in lexicon.get('verbs', [])[:5]:\n",
        "            root = extract_verb_root(verb)\n",
        "            print(f\"  {verb} → {root}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DEBUG: Testing verb compositions\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test = \"gel\"\n",
        "    try:\n",
        "        lattice = pynini.compose(test, verb_imperative)\n",
        "        print(f\"\\n'{test}' + verb_imperative:\")\n",
        "        for path in lattice.paths().ostrings():\n",
        "            print(f\"  ✓ {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    test2 = \"gelsin\"\n",
        "    try:\n",
        "        lattice = pynini.compose(test2, verb_imperative)\n",
        "        print(f\"\\n'{test2}' + verb_imperative:\")\n",
        "        for path in lattice.paths().ostrings():\n",
        "            print(f\"  ✓ {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SINGLE WORD ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_words = [\n",
        "        # Dilek Kipleri\n",
        "        \"gel\",            # come! (imperative 2sg)\n",
        "        \"gelin\",          # come! (imperative 2pl)\n",
        "        \"gelsin\",         # let him/her come (imperative 3sg)\n",
        "        \"gelsem\",         # if I come (conditional)\n",
        "        \"geleyim\",        # let me come (optative)\n",
        "        \"gelmeli\",        # must come (necessitative)\n",
        "        \"yazmalıyım\",     # I must write\n",
        "        \"okusana\",        # read then! (optative emphatic)\n",
        "        \"görseler\",       # if they see (conditional)\n",
        "        # Bildirme Kipleri\n",
        "        \"okudum\",         # I read (past)\n",
        "        \"gelebilecek\",    # will be able to come (future)\n",
        "        \"geliyorum\",      # I am coming (present continuous)\n",
        "        # Nouns\n",
        "        \"kitaplardan\",    # from books\n",
        "        \"kalemlik\",       # pencil case\n",
        "        \"evdekiler\",      # those in the house\n",
        "    ]\n",
        "\n",
        "    for word in test_words:\n",
        "        print(f\"\\n{word}:\")\n",
        "        analyses = analyze(word)\n",
        "        for analysis in analyses[:5]:\n",
        "            print(f\"  {analysis}\")\n",
        "        if len(analyses) > 5:\n",
        "            print(f\"  ... and {len(analyses) - 5} more analyses\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MULTI-WORD ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_sentences = [\n",
        "        \"evde misin?\",\n",
        "        \"kitap da güzel\",\n",
        "        \"gel buraya!\",\n",
        "        \"oraya git\",\n",
        "        \"yukarı çık\",\n",
        "        \"ben yemeğe gidiyorum gelecek misin?\",\n",
        "        \"kitabı aldım\",\n",
        "        \"ağaca çıktı\",\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        print(f\"\\n'{sentence}':\")\n",
        "        results = analyze_sentence(sentence)\n",
        "        for result in results:\n",
        "            print(f\"  {result['token']}:\")\n",
        "            for analysis in result['analyses'][:3]:\n",
        "                print(f\"    {analysis}\")\n",
        "            if len(result['analyses']) > 3:\n",
        "                print(f\"    ... and {len(result['analyses']) - 3} more\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CONTEXT-AWARE ANALYSIS (Viterbi)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # These sentences contain ambiguous words\n",
        "    ambiguous_sentences = [\n",
        "        \"yüzü güzel\",           # yüz: Face (Noun) vs Swim (Verb) -> Expect NOUN\n",
        "        \"denizde yüz\",          # yüz: Face (Noun) vs Swim (Verb) -> Expect VERB (Imperative)\n",
        "        \"bana gül\",             # gül: Rose (Noun) vs Smile (Verb) -> Expect VERB\n",
        "        \"kırmızı gül\",          # gül: Rose (Noun) vs Smile (Verb) -> Expect NOUN (Adj modifies Noun)\n",
        "        \"okula git\",            # git: Expect VERB\n",
        "        \"güzel bir ev\",         # güzel: Adj, bir: Det, ev: Noun\n",
        "        \"evde misin\",           # misin: Expect QUES\n",
        "        \"kitap okumayı severim\" # severim: Expect VERB (End of sentence)\n",
        "    ]\n",
        "\n",
        "    for sent in ambiguous_sentences:\n",
        "        print(f\"\\nSentence: '{sent}'\")\n",
        "        results = analyze_sentence_context_aware(sent)\n",
        "        \n",
        "        # Print formatted table\n",
        "        print(f\" {'Word':<15} | {'Tag':<6} | {'Selected Analysis'}\")\n",
        "        print(\"-\" * 50)\n",
        "        for res in results:\n",
        "            print(f\" {res['token']:<15} | {res['tag']:<6} | {res['best_analysis']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLxaXXEeWzeR",
        "outputId": "dfd23546-cc83-4a63-f15f-403586e13f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "gelecek misin:\n",
            "  RAW (1): ['No analysis found for: gelecek misin']\n",
            "  FILTERED (1): ['No analysis found for: gelecek misin']\n",
            "\n",
            "yapacak mısın:\n",
            "  RAW (1): ['No analysis found for: yapacak mısın']\n",
            "  FILTERED (1): ['No analysis found for: yapacak mısın']\n",
            "\n",
            "gidiyorum:\n",
            "  RAW (1): ['git+VERB+PRES.CONT+1SG']\n",
            "  FILTERED (1): ['git+VERB+PRES.CONT+1SG']\n",
            "\n",
            "çıktı:\n",
            "  RAW (2): ['çık+VERB+PAST+3SG', 'çıktı+NOUN']\n",
            "  FILTERED (3): ['çık+VERB+PAST+3SG', 'çık+VERB+PAST+3SG', 'çıktı+NOUN']\n",
            "\n",
            "gelecek:\n",
            "  RAW (3): ['gel+VERB+FUT+3SG', 'gelecek+ADJ', 'gelecek+NOUN']\n",
            "  FILTERED (6): ['gel+VERB+FUT+3SG', 'gel+VERB+FUT+3SG', 'gelecek+ADJ', 'gelecek+ADJ', 'gelecek+NOUN', 'gelecek+NOUN']\n",
            "\n",
            "ben:\n",
            "  RAW (2): ['ben+NOUN', 'ben+PRON']\n",
            "  FILTERED (3): ['ben+NOUN', 'ben+PRON', 'ben+PRON']\n"
          ]
        }
      ],
      "source": [
        "for word in [\"gelecek misin\", \"yapacak mısın\", \"gidiyorum\", \"çıktı\", \"gelecek\", \"ben\"]:\n",
        "    print(f\"\\n{word}:\")\n",
        "    raw = analyze(word)\n",
        "    filtered = analyze_with_disambiguation(word)\n",
        "    print(f\"  RAW ({len(raw)}): {raw}\")\n",
        "    print(f\"  FILTERED ({len(filtered)}): {filtered}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
