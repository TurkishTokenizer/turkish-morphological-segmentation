#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Turkish Word Classifier with GPU Support
-----------------------------------------
Bu script, cleaned_corpus.txt dosyasÄ±ndan kelimeleri okuyarak
GPU destekli TÃ¼rkÃ§e POS (Part-of-Speech) tagging modeli ile
kelimeleri grammatik sÄ±nÄ±flara ayÄ±rÄ±r.

SÄ±nÄ±flar:
- NOUN (Ä°sim)
- VERB (Fiil)
- ADJ (SÄ±fat)
- ADV (Zarf)
- CONJ (BaÄŸlaÃ§)
- NUM (SayÄ±)
- PRON (Zamir)
- OTHER (DiÄŸer)

KullanÄ±m:
    python classify_words.py
"""

import os
import re
import json
import torch
from collections import defaultdict
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Sabitler
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
CORPUS_PATH = os.path.join(PROJECT_ROOT, "wikimedia_data", "cleaned_corpus.txt")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "classified_words")

# TÃ¼rkÃ§e POS modeli - dbmdz/bert-base-turkish-cased modelini kullanÄ±yoruz
# Bu model TÃ¼rkÃ§e iÃ§in eÄŸitilmiÅŸ BERT tabanlÄ± bir model
MODEL_NAME = "akdeniz27/bert-base-turkish-cased-ner"  # NER model, POS iÃ§in alternatif kullanacaÄŸÄ±z

# POS etiketlerini kategorilere eÅŸleÅŸtirme
POS_MAPPING = {
    # Ä°simler
    "NOUN": "noun",
    "PROPN": "noun",  # Ã–zel isimler de noun kategorisine
    
    # Fiiller
    "VERB": "verb",
    "AUX": "verb",  # YardÄ±mcÄ± fiiller
    
    # SÄ±fatlar
    "ADJ": "adjective",
    
    # Zarflar
    "ADV": "adverb",
    
    # BaÄŸlaÃ§lar
    "CCONJ": "conjunction",
    "SCONJ": "conjunction",
    
    # SayÄ±lar
    "NUM": "num",
    
    # Zamirler
    "PRON": "pronoun",
    "DET": "pronoun",  # Belirleyiciler de pronoun olarak
    
    # DiÄŸer
    "ADP": "other",
    "INTJ": "other",
    "PART": "other",
    "PUNCT": "other",
    "SYM": "other",
    "X": "other",
}


def check_gpu():
    """GPU kullanÄ±labilirliÄŸini kontrol et"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f"âœ“ GPU bulundu: {device_name}")
        print(f"  CUDA Version: {torch.version.cuda}")
        return 0  # GPU device index
    else:
        print("âš  GPU bulunamadÄ±, CPU kullanÄ±lacak")
        return -1  # CPU


def load_words_from_corpus(corpus_path, max_words=None):
    """Corpus dosyasÄ±ndan benzersiz kelimeleri yÃ¼kle"""
    print(f"\nðŸ“– Corpus yÃ¼kleniyor: {corpus_path}")
    
    if not os.path.exists(corpus_path):
        raise FileNotFoundError(f"Corpus dosyasÄ± bulunamadÄ±: {corpus_path}")
    
    words = set()
    turkish_pattern = re.compile(r'^[a-zA-ZÃ§Ã‡ÄŸÄžÄ±Ä°Ã¶Ã–ÅŸÅžÃ¼ÃœÃ¢Ã‚Ã®ÃŽÃ»Ã›]+$')
    
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(tqdm(f, desc="SatÄ±rlar okunuyor"), 1):
            # SatÄ±rdaki kelimeleri ayÄ±r
            tokens = line.strip().lower().split()
            for token in tokens:
                # Sadece TÃ¼rkÃ§e karakterler iÃ§eren kelimeleri al
                if turkish_pattern.match(token) and len(token) >= 2:
                    words.add(token)
            
            # max_words limitine ulaÅŸÄ±ldÄ±ysa dur
            if max_words and len(words) >= max_words:
                break
    
    print(f"âœ“ Toplam {len(words)} benzersiz kelime bulundu")
    return list(words)


def classify_with_spacy_stanza(words, device):
    """
    Stanza kÃ¼tÃ¼phanesi ile TÃ¼rkÃ§e POS tagging
    Stanza, Stanford NLP'nin Python wrapper'Ä± ve Ã§ok dilli destek sunar
    """
    try:
        import stanza
        
        print("\nðŸ”§ Stanza TÃ¼rkÃ§e modeli yÃ¼kleniyor...")
        
        # GPU kullanÄ±mÄ±nÄ± ayarla
        use_gpu = device >= 0
        
        # TÃ¼rkÃ§e modeli indir (ilk Ã§alÄ±ÅŸtÄ±rmada)
        try:
            stanza.download('tr', verbose=False)
        except:
            pass  # Zaten indirilmiÅŸ olabilir
        
        # Pipeline oluÅŸtur
        nlp = stanza.Pipeline('tr', processors='tokenize,pos', use_gpu=use_gpu, verbose=False)
        
        # SÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ kelimeleri tut
        classified = defaultdict(set)
        
        # Batch iÅŸleme iÃ§in kelimeleri grupla
        batch_size = 100
        total_batches = (len(words) + batch_size - 1) // batch_size
        
        print(f"\nðŸ·ï¸  {len(words)} kelime sÄ±nÄ±flandÄ±rÄ±lÄ±yor...")
        
        for i in tqdm(range(0, len(words), batch_size), desc="SÄ±nÄ±flandÄ±rÄ±lÄ±yor", total=total_batches):
            batch = words[i:i+batch_size]
            
            # Her kelimeyi ayrÄ± cÃ¼mle olarak iÅŸle
            text = " . ".join(batch)
            
            try:
                doc = nlp(text)
                
                for sentence in doc.sentences:
                    for word in sentence.words:
                        if word.text.lower() in batch or word.text in batch:
                            pos = word.upos  # Universal POS tag
                            category = POS_MAPPING.get(pos, "other")
                            classified[category].add(word.text.lower())
            except Exception as e:
                # Hata durumunda kelimeleri other'a ekle
                for w in batch:
                    classified["other"].add(w)
        
        return classified
        
    except ImportError:
        print("âš  Stanza yÃ¼klÃ¼ deÄŸil, alternatif yÃ¶ntem deneniyor...")
        return None


def classify_with_simple_rules(words):
    """
    Basit kural tabanlÄ± sÄ±nÄ±flandÄ±rma
    TÃ¼rkÃ§e morfolojik Ã¶zelliklere dayalÄ±
    """
    print("\nðŸ“ Kural tabanlÄ± sÄ±nÄ±flandÄ±rma yapÄ±lÄ±yor...")
    
    classified = defaultdict(set)
    
    # YaygÄ±n TÃ¼rkÃ§e eklere gÃ¶re sÄ±nÄ±flandÄ±rma kurallarÄ±
    verb_suffixes = ['mak', 'mek', 'yor', 'dÄ±', 'di', 'du', 'dÃ¼', 'tÄ±', 'ti', 'tu', 'tÃ¼',
                     'acak', 'ecek', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'malÄ±', 'meli']
    
    adj_suffixes = ['lÄ±', 'li', 'lu', 'lÃ¼', 'sÄ±z', 'siz', 'suz', 'sÃ¼z', 
                    'lÄ±k', 'lik', 'luk', 'lÃ¼k', 'sal', 'sel', 'cÄ±', 'ci', 'cu', 'cÃ¼']
    
    adv_suffixes = ['ca', 'ce', 'Ã§a', 'Ã§e']
    
    # YaygÄ±n baÄŸlaÃ§lar
    conjunctions = {'ve', 'veya', 'ama', 'fakat', 'ancak', 'lakin', 'yani', 'Ã§Ã¼nkÃ¼', 
                   'halbuki', 'oysa', 'ya', 'yahut', 'hem', 'ne', 'ise', 'ki', 'dahi',
                   'ile', 'iÃ§in', 'gibi', 'kadar', 'dolayÄ±', 'raÄŸmen', 'karÅŸÄ±n'}
    
    # YaygÄ±n zamirler
    pronouns = {'ben', 'sen', 'o', 'biz', 'siz', 'onlar', 'bu', 'ÅŸu', 'bunlar', 'ÅŸunlar',
               'kim', 'ne', 'hangi', 'hangisi', 'nere', 'nerede', 'burasÄ±', 'orasÄ±',
               'kendi', 'kendisi', 'hepsi', 'hiÃ§biri', 'bazÄ±sÄ±', 'birisi', 'herkes',
               'kimse', 'biraz', 'bir', 'iki', 'Ã¼Ã§', 'dÃ¶rt', 'beÅŸ', 'altÄ±', 'yedi',
               'sekiz', 'dokuz', 'on', 'yirmi', 'otuz', 'kÄ±rk', 'elli', 'altmÄ±ÅŸ',
               'yetmiÅŸ', 'seksen', 'doksan', 'yÃ¼z', 'bin', 'milyon', 'milyar'}
    
    for word in tqdm(words, desc="Kurallar uygulanÄ±yor"):
        word_lower = word.lower()
        
        # SayÄ±larÄ± kontrol et
        if word_lower.isdigit():
            classified["num"].add(word_lower)
            continue
        
        # BaÄŸlaÃ§larÄ± kontrol et
        if word_lower in conjunctions:
            classified["conjunction"].add(word_lower)
            continue
        
        # Zamirleri kontrol et
        if word_lower in pronouns:
            classified["pronoun"].add(word_lower)
            continue
        
        # Fiil eklerini kontrol et
        is_verb = False
        for suffix in verb_suffixes:
            if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 1:
                classified["verb"].add(word_lower)
                is_verb = True
                break
        if is_verb:
            continue
        
        # Zarf eklerini kontrol et (sÄ±fattan Ã¶nce)
        is_adv = False
        for suffix in adv_suffixes:
            if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:
                classified["adverb"].add(word_lower)
                is_adv = True
                break
        if is_adv:
            continue
        
        # SÄ±fat eklerini kontrol et
        is_adj = False
        for suffix in adj_suffixes:
            if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 1:
                classified["adjective"].add(word_lower)
                is_adj = True
                break
        if is_adj:
            continue
        
        # Geri kalanlarÄ± isim olarak varsay (TÃ¼rkÃ§e'de en yaygÄ±n kategori)
        if len(word_lower) >= 3:
            classified["noun"].add(word_lower)
        else:
            classified["other"].add(word_lower)
    
    return classified


def save_classified_words(classified, output_dir):
    """SÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ kelimeleri dosyalara kaydet"""
    print(f"\nðŸ’¾ SonuÃ§lar kaydediliyor: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    categories = ['adjective', 'adverb', 'conjunction', 'noun', 'num', 'pronoun', 'verb', 'other']
    
    summary = {}
    
    for category in categories:
        words = sorted(classified.get(category, set()))
        summary[category] = len(words)
        
        # Her kategori iÃ§in ayrÄ± dosya
        output_path = os.path.join(output_dir, f"{category}.txt")
        with open(output_path, 'w', encoding='utf-8') as f:
            for word in words:
                f.write(f"{word}\n")
        
        print(f"  âœ“ {category}: {len(words)} kelime")
    
    # Ã–zet JSON dosyasÄ±
    summary_path = os.path.join(output_dir, "summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # TÃ¼m kelimeleri tek bir JSON dosyasÄ±na da kaydet
    all_words_path = os.path.join(output_dir, "all_classified_words.json")
    all_data = {cat: sorted(list(words)) for cat, words in classified.items()}
    with open(all_words_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ Ã–zet: {summary_path}")
    print(f"âœ“ TÃ¼m veriler: {all_words_path}")
    
    return summary


def main():
    print("=" * 60)
    print("       TÃœRKÃ‡E KELÄ°ME SINIFLANDIRICI (GPU DESTEKLÄ°)")
    print("=" * 60)
    
    # GPU kontrolÃ¼
    device = check_gpu()
    
    # Kelimeleri yÃ¼kle
    words = load_words_from_corpus(CORPUS_PATH)
    
    if not words:
        print("âŒ Kelime bulunamadÄ±!")
        return
    
    # Ã–nce Stanza ile dene (daha doÄŸru sonuÃ§)
    classified = classify_with_spacy_stanza(words, device)
    
    # Stanza baÅŸarÄ±sÄ±z olursa kural tabanlÄ± yÃ¶ntemi kullan
    if classified is None or len(classified) == 0:
        classified = classify_with_simple_rules(words)
    
    # SonuÃ§larÄ± kaydet
    summary = save_classified_words(classified, OUTPUT_DIR)
    
    # Ã–zet gÃ¶ster
    print("\n" + "=" * 60)
    print("                      Ã–ZET")
    print("=" * 60)
    total = sum(summary.values())
    for category, count in sorted(summary.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        print(f"  {category:15} : {count:8,} kelime ({percentage:.1f}%)")
    print("-" * 60)
    print(f"  {'TOPLAM':15} : {total:8,} kelime")
    print("=" * 60)
    
    print(f"\nðŸŽ‰ Ä°ÅŸlem tamamlandÄ±! SonuÃ§lar: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
