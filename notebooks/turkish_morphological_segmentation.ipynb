{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6txQNlZvQ8P",
        "outputId": "099cc9e0-f4aa-4f87-b03a-9197adcdb785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynini\n",
            "  Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (165.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.5/165.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynini\n",
            "Successfully installed pynini-2.1.7\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install --only-binary :all: pynini\n",
        "!pip install wurlitzer\n",
        "import pynini\n",
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_HcAl-2_lcJ",
        "outputId": "86756d4a-c81d-484e-dc59-9053f56cefb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOStream.flush timed out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SINGLE WORD ANALYSIS\n",
            "------------------------------------------------------------\n",
            "\n",
            "git:\n",
            "  git+VERB+IMP+2SG\n",
            "\n",
            "gitsin:\n",
            "  git+VERB+IMP+3SG\n",
            "\n",
            "giderim:\n",
            "  gider+NOUN+POSS.1SG\n",
            "  git+VERB+AOR+1SG\n",
            "\n",
            "gittim:\n",
            "  git+VERB+PAST+1SG\n",
            "\n",
            "gideceğim:\n",
            "  git+VERB+FUT+1SG+3SG\n",
            "\n",
            "gidiyorum:\n",
            "  git+VERB+PRES.CONT+1SG\n",
            "\n",
            "gitmeliyim:\n",
            "  git+VERB+NEC+1SG\n",
            "\n",
            "yapabilir:\n",
            "  yap+VERB+ABIL+AOR+3SG\n",
            "\n",
            "yapacaksın:\n",
            "  yap+VERB+FUT+2SG\n",
            "\n",
            "anlasana:\n",
            "  anla+VERB+OPT+2SG+EMPH\n",
            "\n",
            "duysalar:\n",
            "  d+NOUN+ACC+COP.COND+3PL\n",
            "  d+NOUN+POSS.3SG+COP.COND+3PL\n",
            "  d+PROPN+ACC+COP.COND+3PL\n",
            "  d+PROPN+POSS.3SG+COP.COND+3PL\n",
            "  duy+NOUN+COP.COND+3PL\n",
            "\n",
            "baksaydım:\n",
            "  No analysis found for: baksaydım\n",
            "\n",
            "çocuklardan:\n",
            "  çocuk+NOUN+PL+ABL\n",
            "\n",
            "çantamdan:\n",
            "  çanta+NOUN+POSS.1SG+ABL\n",
            "\n",
            "okuldakiler:\n",
            "  okul+NOUN+LOC+KI+PL\n",
            "\n",
            "CONTEXT-AWARE ANALYSIS (Viterbi)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Sentence: 'Çay demledim ama çay kenarında yürümeyi de seviyorum; hangisini önce yapalım diye düşünürken zaman geçti.'\n",
            " Word            | Tag    | Selected Analysis\n",
            "----------------------------------------------------------------------\n",
            " Çay             | UNKNOWN | Çay+UNK+OOV\n",
            " demledim        | UNKNOWN | demledim+UNK+OOV\n",
            " ama             | ADJ    | ama+ADJ\n",
            " çay             | NOUN   | çay+NOUN\n",
            " kenarında       | NOUN   | kenar+NOUN+POSS.2SG+LOC\n",
            " yürümeyi        | UNKNOWN | yürümeyi+UNK+OOV\n",
            " de              | NOUN   | d+NOUN+DAT\n",
            " seviyorum       | VERB   | sev+VERB+PRES.CONT+1SG\n",
            " ;               | PUNCT  | ;+PUNCT.semicolon\n",
            " hangisini       | PRON   | hangi+PRON+POSS.3SG+ACC\n",
            " önce            | NOUN   | ön+NOUN+EQU\n",
            " yapalım         | VERB   | yap+VERB+OPT+1PL\n",
            " diye            | CONJ   | diye+CONJ\n",
            " düşünürken      | UNKNOWN | düşünürken+UNK+OOV\n",
            " zaman           | NOUN   | zaman+NOUN\n",
            " geçti           | VERB   | geç+VERB+PAST+3SG\n",
            " .               | PUNCT  | .+PUNCT.period\n",
            "\n",
            "Sentence: 'Bana gül deyince çiçekten mi bahsediyorsun yoksa sadece gülümsememi mi istiyorsun, karar veremedim.'\n",
            " Word            | Tag    | Selected Analysis\n",
            "----------------------------------------------------------------------\n",
            " Bana            | UNKNOWN | Bana+UNK+OOV\n",
            " gül             | NOUN   | gül+NOUN\n",
            " deyince         | VERB   | de+VERB+CVB.INCA\n",
            " çiçekten        | NOUN   | çiçek+NOUN+ABL\n",
            " mi              | QUES   | mi+QUES\n",
            " bahsediyorsun   | VERB   | bahset+VERB+PRES.CONT+2SG\n",
            " yoksa           | CONJ   | yoksa+CONJ\n",
            " sadece          | ADJ    | sade+ADJ+EQU\n",
            " gülümsememi     | UNKNOWN | gülümsememi+UNK+OOV\n",
            " mi              | QUES   | mi+QUES\n",
            " istiyorsun      | UNKNOWN | istiyorsun+UNK+OOV\n",
            " ,               | PUNCT  | ,+PUNCT.comma\n",
            " karar           | VERB   | kar+VERB+AOR+3SG\n",
            " veremedim       | UNKNOWN | veremedim+UNK+OOV\n",
            " .               | PUNCT  | .+PUNCT.period\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import pynini\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "EPS = pynini.cross(\"\", \"\")\n",
        "VOWELS = \"aeıioöuü\"\n",
        "TR_LETTERS = \"abcçdefgğhıijklmnoöprsştuüvyz\"\n",
        "\n",
        "# ----------------------------\n",
        "# Normalization helpers (For turkish correct lowercate transitions)\n",
        "# ----------------------------\n",
        "_TR_LOWER_MAP = str.maketrans({\"I\": \"ı\", \"İ\": \"i\"})\n",
        "\n",
        "def tr_lower(s: str) -> str:\n",
        "    return s.translate(_TR_LOWER_MAP).lower()\n",
        "\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = s.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
        "    return tr_lower(s.strip())\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Lexicon Loading / Normalization\n",
        "# -----------------------------------------------------------------------------\n",
        "def load_lexicon(json_file: str = \"turkish_lexicon.json\") -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Load Turkish lexicon from a JSON file.\n",
        "\n",
        "    Expected schema (recommended):\n",
        "      {\n",
        "        \"nouns\": [...],\n",
        "        \"verbs\": [...],        # may be stems or infinitives\n",
        "        \"adjectives\": [...],\n",
        "        ...\n",
        "      }\n",
        "\n",
        "    Also supports:\n",
        "      { \"NOUN\": [...], \"VERB\": [...], ... }\n",
        "    \"\"\"\n",
        "    path = Path(json_file)\n",
        "    if not path.exists():\n",
        "        logger.warning(\"%s not found. Using fallback minimal lexicon.\", json_file)\n",
        "        # Slightly richer fallback so your demo sentences behave better even w/out JSON.\n",
        "        return {\n",
        "            \"nouns\": [\n",
        "                \"ev\", \"kitap\", \"masa\", \"sabah\", \"ayna\", \"yüz\", \"gül\", \"çiçek\",\n",
        "                \"bahçe\", \"yaz\", \"mevsim\", \"kelime\", \"okul\", \"çocuk\", \"çanta\",\n",
        "                \"deniz\", \"çay\", \"gün\", \"plan\", \"zaman\", \"bulut\", \"hava\", \"cam\",\n",
        "                \"söz\", \"karar\", \"emir\", \"kenar\", \"bütün\", \"dolu\",\n",
        "            ],\n",
        "            \"verbs\": [\n",
        "                \"gelmek\", \"gitmek\", \"okumak\", \"bakmak\", \"söylemek\", \"demek\",\n",
        "                \"sanmak\", \"anlamak\", \"sevmek\", \"bahsetmek\", \"dikmek\", \"seçmek\",\n",
        "                \"geçirmek\", \"yürümek\", \"düşünmek\", \"çıkmak\", \"yağmak\",\n",
        "                \"bozmak\", \"yapmak\", \"karıştırmak\", \"kapanmak\", \"almak\",\n",
        "                \"kastetmek\", \"gülümsemek\",\n",
        "            ],\n",
        "            \"adjectives\": [\"güzel\", \"iyi\", \"kırmızı\", \"romantik\", \"baskın\", \"ciddi\", \"kara\"],\n",
        "            \"pronouns\": [\"ben\", \"sen\", \"o\", \"biz\", \"siz\", \"onlar\", \"herkes\", \"hangi\", \"ne\"],\n",
        "            \"adverbs\": [\"çok\", \"az\", \"hâlâ\", \"bile\", \"aslında\", \"gerçekten\", \"sonra\", \"önce\", \"daha\"],\n",
        "            \"conjunctions\": [\"ve\", \"ama\", \"yoksa\", \"da\", \"de\"],\n",
        "            \"postpositions\": [\"gibi\", \"için\", \"yerine\", \"yüzünden\"],\n",
        "            \"proper_nouns\": [],\n",
        "            \"interjections\": [],\n",
        "        }\n",
        "\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def normalize_lexicon(lex: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
        "    mapping = {\n",
        "        \"NOUN\": \"nouns\",\n",
        "        \"VERB\": \"verbs\",\n",
        "        \"ADJ\": \"adjectives\",\n",
        "        \"ADV\": \"adverbs\",\n",
        "        \"PRON\": \"pronouns\",\n",
        "        \"CONJ\": \"conjunctions\",\n",
        "        \"POSTP\": \"postpositions\",\n",
        "        \"PROPN\": \"proper_nouns\",\n",
        "        \"INTERJ\": \"interjections\",\n",
        "        \"NUM\": \"numbers\",\n",
        "    }\n",
        "\n",
        "    out: Dict[str, List[str]] = defaultdict(list)\n",
        "    for k, v in lex.items():\n",
        "        kk = mapping.get(k, k)\n",
        "        if not isinstance(v, list):\n",
        "            continue\n",
        "        for w in v:\n",
        "            if isinstance(w, str) and w.strip():\n",
        "                out[kk].append(normalize_text(w))\n",
        "    # de-dup while keeping order\n",
        "    for kk in list(out.keys()):\n",
        "        seen = set()\n",
        "        deduped = []\n",
        "        for w in out[kk]:\n",
        "            if w not in seen:\n",
        "                seen.add(w)\n",
        "                deduped.append(w)\n",
        "        out[kk] = deduped\n",
        "    return dict(out)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Root Helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "def extract_verb_root(verb_infinitive: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract verb root from infinitive form by removing -mak/-mek if present.\n",
        "    \"\"\"\n",
        "    verb_infinitive = normalize_text(verb_infinitive)\n",
        "    if verb_infinitive.endswith((\"mak\", \"mek\")) and len(verb_infinitive) > 3:\n",
        "        return verb_infinitive[:-3]\n",
        "    return verb_infinitive\n",
        "\n",
        "\n",
        "def create_alternating_roots(words: List[str], tag: str) -> pynini.Fst:\n",
        "    \"\"\"\n",
        "    Create root FSTs with consonant softening:\n",
        "\n",
        "    p → b\n",
        "    ç → c\n",
        "    t → d\n",
        "    k → g / ğ\n",
        "\n",
        "    Softened form maps back to original lemma in output analysis.\n",
        "    \"\"\"\n",
        "    roots = []\n",
        "    for w in words:\n",
        "        word = normalize_text(w)\n",
        "        if not word:\n",
        "            continue\n",
        "\n",
        "        # Base form\n",
        "        roots.append(pynini.cross(word, f\"{word}+{tag}\"))\n",
        "\n",
        "        # Softened stem variants\n",
        "        if len(word) > 1 and word[-1] in {\"p\", \"ç\", \"t\", \"k\"}:\n",
        "            stem = word[:-1]\n",
        "            final = word[-1]\n",
        "            softened: Optional[str] = None\n",
        "\n",
        "            if final == \"p\":\n",
        "                softened = stem + \"b\"\n",
        "            elif final == \"ç\":\n",
        "                softened = stem + \"c\"\n",
        "            elif final == \"t\":\n",
        "                softened = stem + \"d\"\n",
        "            elif final == \"k\":\n",
        "                softened = stem + (\"ğ\" if (stem and stem[-1] in VOWELS) else \"g\")\n",
        "\n",
        "            if softened:\n",
        "                roots.append(pynini.cross(softened, f\"{word}+{tag}\"))\n",
        "\n",
        "    if not roots:\n",
        "        # IMPORTANT: do NOT return EPS here, or you'll create epsilon-roots in unions.\n",
        "        # Return an \"empty language\" acceptor by intersecting disjoint acceptors.\n",
        "        return pynini.intersect(pynini.accep(\"a\"), pynini.accep(\"b\"))\n",
        "    return pynini.union(*roots)\n",
        "\n",
        "def union_nonempty(fsts: List[pynini.Fst]) -> pynini.Fst:\n",
        "    \"\"\"\n",
        "    Union only non-empty FSTs; avoids accidentally introducing epsilon-roots.\n",
        "    \"\"\"\n",
        "    kept = []\n",
        "    for fst in fsts:\n",
        "        # There's no perfect \"is empty\" API; this heuristic works for our constructed empties.\n",
        "        try:\n",
        "            # If it has no start state, it's empty; but Pynini objects vary.\n",
        "            _ = fst.start()\n",
        "            kept.append(fst)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not kept:\n",
        "        return pynini.intersect(pynini.accep(\"a\"), pynini.accep(\"b\"))\n",
        "    return pynini.union(*kept)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Build Morphology FST\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_analyzer(lexicon: Dict[str, List[str]]) -> pynini.Fst:\n",
        "    \"\"\"\n",
        "    Build the full Turkish analyzer FST from lexicon + suffix grammars.\n",
        "\n",
        "    Quick-fix changes vs original:\n",
        "    - Avoid EPS roots in root unions (prevents suffix-only parses like 'de' -> '+LOC')\n",
        "    - Add fused FUT 1SG/1PL: -acağım/-eceğim, -acağız/-eceğiz (e.g., 'gideceğim')\n",
        "    - Add common converbs: -ınca/-yince, -ken, -ip (e.g., 'bakınca', 'deyince')\n",
        "    - Add vowel-final possessive shortcuts (e.g., yüz+ü for 3SG, ev+im for 1SG, etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Helper: \"empty language\" FST (NOT epsilon)\n",
        "    # -------------------------------------------------------------------------\n",
        "    def EMPTY_FST() -> pynini.Fst:\n",
        "        # Language with no strings. Useful as a safe \"nothing here\" placeholder.\n",
        "        return pynini.intersect(pynini.accep(\"a\"), pynini.accep(\"b\"))\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Roots (IMPORTANT: never use EPS here)\n",
        "    # -------------------------------------------------------------------------\n",
        "    noun_roots = (\n",
        "        create_alternating_roots(lexicon.get(\"nouns\", []), \"NOUN\")\n",
        "        if lexicon.get(\"nouns\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    adj_roots = (\n",
        "        create_alternating_roots(lexicon.get(\"adjectives\", []), \"ADJ\")\n",
        "        if lexicon.get(\"adjectives\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    verb_root_list = [extract_verb_root(w) for w in lexicon.get(\"verbs\", [])]\n",
        "    verb_roots = (\n",
        "        create_alternating_roots(verb_root_list, \"VERB\")\n",
        "        if verb_root_list\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    pronoun_roots = (\n",
        "        pynini.union(\n",
        "            *[pynini.cross(w, f\"{w}+PRON\") for w in lexicon.get(\"pronouns\", [])]\n",
        "        )\n",
        "        if lexicon.get(\"pronouns\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    adverb_roots = (\n",
        "        pynini.union(*[pynini.cross(w, f\"{w}+ADV\") for w in lexicon.get(\"adverbs\", [])])\n",
        "        if lexicon.get(\"adverbs\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    postposition_roots = (\n",
        "        pynini.union(\n",
        "            *[pynini.cross(w, f\"{w}+POSTP\") for w in lexicon.get(\"postpositions\", [])]\n",
        "        )\n",
        "        if lexicon.get(\"postpositions\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    interjection_roots = (\n",
        "        pynini.union(\n",
        "            *[pynini.cross(w, f\"{w}+INTERJ\") for w in lexicon.get(\"interjections\", [])]\n",
        "        )\n",
        "        if lexicon.get(\"interjections\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    conjunction_roots = (\n",
        "        pynini.union(\n",
        "            *[pynini.cross(w, f\"{w}+CONJ\") for w in lexicon.get(\"conjunctions\", [])]\n",
        "        )\n",
        "        if lexicon.get(\"conjunctions\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    proper_noun_roots = (\n",
        "        create_alternating_roots(lexicon.get(\"proper_nouns\", []), \"PROPN\")\n",
        "        if lexicon.get(\"proper_nouns\")\n",
        "        else EMPTY_FST()\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Question particles\n",
        "    # -------------------------------------------------------------------------\n",
        "    question_particles = pynini.union(\n",
        "        pynini.cross(\"mi\", \"mi+QUES\"),\n",
        "        pynini.cross(\"mı\", \"mı+QUES\"),\n",
        "        pynini.cross(\"mu\", \"mu+QUES\"),\n",
        "        pynini.cross(\"mü\", \"mü+QUES\"),\n",
        "        pynini.cross(\"misin\", \"mi+QUES+2SG\"),\n",
        "        pynini.cross(\"mısın\", \"mı+QUES+2SG\"),\n",
        "        pynini.cross(\"musun\", \"mu+QUES+2SG\"),\n",
        "        pynini.cross(\"müsün\", \"mü+QUES+2SG\"),\n",
        "        pynini.cross(\"miyim\", \"mi+QUES+1SG\"),\n",
        "        pynini.cross(\"mıyım\", \"mı+QUES+1SG\"),\n",
        "        pynini.cross(\"muyum\", \"mu+QUES+1SG\"),\n",
        "        pynini.cross(\"müyüm\", \"mü+QUES+1SG\"),\n",
        "        pynini.cross(\"miyiz\", \"mi+QUES+1PL\"),\n",
        "        pynini.cross(\"mıyız\", \"mı+QUES+1PL\"),\n",
        "        pynini.cross(\"muyuz\", \"mu+QUES+1PL\"),\n",
        "        pynini.cross(\"müyüz\", \"mü+QUES+1PL\"),\n",
        "        pynini.cross(\"misiniz\", \"mi+QUES+2PL\"),\n",
        "        pynini.cross(\"mısınız\", \"mı+QUES+2PL\"),\n",
        "        pynini.cross(\"musunuz\", \"mu+QUES+2PL\"),\n",
        "        pynini.cross(\"müsünüz\", \"mü+QUES+2PL\"),\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Derivational suffixes\n",
        "    # -------------------------------------------------------------------------\n",
        "    derivational = pynini.union(\n",
        "        pynini.cross(\"lık\", \"+DER.lık\"),\n",
        "        pynini.cross(\"lik\", \"+DER.lik\"),\n",
        "        pynini.cross(\"luk\", \"+DER.luk\"),\n",
        "        pynini.cross(\"lük\", \"+DER.lük\"),\n",
        "        pynini.cross(\"cı\", \"+DER.cı\"),\n",
        "        pynini.cross(\"ci\", \"+DER.ci\"),\n",
        "        pynini.cross(\"cu\", \"+DER.cu\"),\n",
        "        pynini.cross(\"cü\", \"+DER.cü\"),\n",
        "        pynini.cross(\"çı\", \"+DER.çı\"),\n",
        "        pynini.cross(\"çi\", \"+DER.çi\"),\n",
        "        pynini.cross(\"çu\", \"+DER.çu\"),\n",
        "        pynini.cross(\"çü\", \"+DER.çü\"),\n",
        "        pynini.cross(\"sız\", \"+DER.sız\"),\n",
        "        pynini.cross(\"siz\", \"+DER.siz\"),\n",
        "        pynini.cross(\"suz\", \"+DER.suz\"),\n",
        "        pynini.cross(\"süz\", \"+DER.süz\"),\n",
        "        pynini.cross(\"lı\", \"+DER.lı\"),\n",
        "        pynini.cross(\"li\", \"+DER.li\"),\n",
        "        pynini.cross(\"lu\", \"+DER.lu\"),\n",
        "        pynini.cross(\"lü\", \"+DER.lü\"),\n",
        "        pynini.cross(\"\", \"\"),  # optional\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Nominal morphology\n",
        "    # -------------------------------------------------------------------------\n",
        "    nominal_roots = pynini.union(noun_roots, adj_roots, pronoun_roots, proper_noun_roots)\n",
        "    nominal_derived = nominal_roots + derivational\n",
        "\n",
        "    plural = pynini.union(\n",
        "        pynini.cross(\"lar\", \"+PL\"),\n",
        "        pynini.cross(\"ler\", \"+PL\"),\n",
        "        pynini.cross(\"\", \"\"),  # optional\n",
        "    )\n",
        "    nominal_pl = nominal_derived + plural\n",
        "\n",
        "    # Possessive (add vowel-final shortcuts so things like 'yüzü' don't get forced to ACC)\n",
        "    possessive = pynini.union(\n",
        "        # plurals\n",
        "        pynini.cross(\"imiz\", \"+POSS.1PL\"),\n",
        "        pynini.cross(\"ımız\", \"+POSS.1PL\"),\n",
        "        pynini.cross(\"umuz\", \"+POSS.1PL\"),\n",
        "        pynini.cross(\"ümüz\", \"+POSS.1PL\"),\n",
        "        pynini.cross(\"iniz\", \"+POSS.2PL\"),\n",
        "        pynini.cross(\"ınız\", \"+POSS.2PL\"),\n",
        "        pynini.cross(\"unuz\", \"+POSS.2PL\"),\n",
        "        pynini.cross(\"ünüz\", \"+POSS.2PL\"),\n",
        "        pynini.cross(\"leri\", \"+POSS.3PL\"),\n",
        "        pynini.cross(\"ları\", \"+POSS.3PL\"),\n",
        "        # 1sg (consonant-final + vowel-final)\n",
        "        pynini.cross(\"im\", \"+POSS.1SG\"),\n",
        "        pynini.cross(\"ım\", \"+POSS.1SG\"),\n",
        "        pynini.cross(\"um\", \"+POSS.1SG\"),\n",
        "        pynini.cross(\"üm\", \"+POSS.1SG\"),\n",
        "        pynini.cross(\"m\", \"+POSS.1SG\"),\n",
        "        # 2sg (consonant-final + vowel-final)\n",
        "        pynini.cross(\"in\", \"+POSS.2SG\"),\n",
        "        pynini.cross(\"ın\", \"+POSS.2SG\"),\n",
        "        pynini.cross(\"un\", \"+POSS.2SG\"),\n",
        "        pynini.cross(\"ün\", \"+POSS.2SG\"),\n",
        "        pynini.cross(\"n\", \"+POSS.2SG\"),\n",
        "        # 3sg (vowel-final with -sI and consonant-final with bare vowel)\n",
        "        pynini.cross(\"si\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"sı\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"su\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"sü\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"i\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"ı\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"u\", \"+POSS.3SG\"),\n",
        "        pynini.cross(\"ü\", \"+POSS.3SG\"),\n",
        "    )\n",
        "\n",
        "    # Cases (keep yours, but optional epsilon must be \"\", \"\" not EPS)\n",
        "    case_after_poss = pynini.union(\n",
        "        pynini.cross(\"dan\", \"+ABL\"),\n",
        "        pynini.cross(\"den\", \"+ABL\"),\n",
        "        pynini.cross(\"tan\", \"+ABL\"),\n",
        "        pynini.cross(\"ten\", \"+ABL\"),\n",
        "        pynini.cross(\"ndan\", \"+ABL\"),\n",
        "        pynini.cross(\"nden\", \"+ABL\"),\n",
        "        pynini.cross(\"ntan\", \"+ABL\"),\n",
        "        pynini.cross(\"nten\", \"+ABL\"),\n",
        "        pynini.cross(\"nın\", \"+GEN\"),\n",
        "        pynini.cross(\"nin\", \"+GEN\"),\n",
        "        pynini.cross(\"nun\", \"+GEN\"),\n",
        "        pynini.cross(\"nün\", \"+GEN\"),\n",
        "        pynini.cross(\"da\", \"+LOC\"),\n",
        "        pynini.cross(\"de\", \"+LOC\"),\n",
        "        pynini.cross(\"ta\", \"+LOC\"),\n",
        "        pynini.cross(\"te\", \"+LOC\"),\n",
        "        pynini.cross(\"nda\", \"+LOC\"),\n",
        "        pynini.cross(\"nde\", \"+LOC\"),\n",
        "        pynini.cross(\"nta\", \"+LOC\"),\n",
        "        pynini.cross(\"nte\", \"+LOC\"),\n",
        "        pynini.cross(\"ya\", \"+DAT\"),\n",
        "        pynini.cross(\"ye\", \"+DAT\"),\n",
        "        pynini.cross(\"na\", \"+DAT\"),\n",
        "        pynini.cross(\"ne\", \"+DAT\"),\n",
        "        pynini.cross(\"yı\", \"+ACC\"),\n",
        "        pynini.cross(\"yi\", \"+ACC\"),\n",
        "        pynini.cross(\"yu\", \"+ACC\"),\n",
        "        pynini.cross(\"yü\", \"+ACC\"),\n",
        "        pynini.cross(\"nı\", \"+ACC\"),\n",
        "        pynini.cross(\"ni\", \"+ACC\"),\n",
        "        pynini.cross(\"nu\", \"+ACC\"),\n",
        "        pynini.cross(\"nü\", \"+ACC\"),\n",
        "        pynini.cross(\"yla\", \"+INS\"),\n",
        "        pynini.cross(\"yle\", \"+INS\"),\n",
        "        pynini.cross(\"ca\", \"+EQU\"),\n",
        "        pynini.cross(\"ce\", \"+EQU\"),\n",
        "        pynini.cross(\"\", \"\"),  # optional\n",
        "    )\n",
        "\n",
        "    ki_suffix = pynini.union(\n",
        "        pynini.cross(\"ki\", \"+KI\"),\n",
        "        pynini.cross(\"kü\", \"+KI\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    plural_after_ki = pynini.union(\n",
        "        pynini.cross(\"ler\", \"+PL\"),\n",
        "        pynini.cross(\"lar\", \"+PL\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    possessive_path = nominal_pl + possessive + case_after_poss + ki_suffix + plural_after_ki\n",
        "\n",
        "    case_no_poss = pynini.union(\n",
        "        pynini.cross(\"ların\", \"+GEN\"),\n",
        "        pynini.cross(\"lerin\", \"+GEN\"),\n",
        "        pynini.cross(\"dan\", \"+ABL\"),\n",
        "        pynini.cross(\"den\", \"+ABL\"),\n",
        "        pynini.cross(\"tan\", \"+ABL\"),\n",
        "        pynini.cross(\"ten\", \"+ABL\"),\n",
        "        pynini.cross(\"nın\", \"+GEN\"),\n",
        "        pynini.cross(\"nin\", \"+GEN\"),\n",
        "        pynini.cross(\"nun\", \"+GEN\"),\n",
        "        pynini.cross(\"nün\", \"+GEN\"),\n",
        "        pynini.cross(\"da\", \"+LOC\"),\n",
        "        pynini.cross(\"de\", \"+LOC\"),\n",
        "        pynini.cross(\"ta\", \"+LOC\"),\n",
        "        pynini.cross(\"te\", \"+LOC\"),\n",
        "        pynini.cross(\"ya\", \"+DAT\"),\n",
        "        pynini.cross(\"ye\", \"+DAT\"),\n",
        "        pynini.cross(\"a\", \"+DAT\"),\n",
        "        pynini.cross(\"e\", \"+DAT\"),\n",
        "        pynini.cross(\"yı\", \"+ACC\"),\n",
        "        pynini.cross(\"yi\", \"+ACC\"),\n",
        "        pynini.cross(\"yu\", \"+ACC\"),\n",
        "        pynini.cross(\"yü\", \"+ACC\"),\n",
        "        pynini.cross(\"ı\", \"+ACC\"),\n",
        "        pynini.cross(\"i\", \"+ACC\"),\n",
        "        pynini.cross(\"u\", \"+ACC\"),\n",
        "        pynini.cross(\"ü\", \"+ACC\"),\n",
        "        pynini.cross(\"la\", \"+INS\"),\n",
        "        pynini.cross(\"le\", \"+INS\"),\n",
        "        pynini.cross(\"yla\", \"+INS\"),\n",
        "        pynini.cross(\"yle\", \"+INS\"),\n",
        "        pynini.cross(\"ca\", \"+EQU\"),\n",
        "        pynini.cross(\"ce\", \"+EQU\"),\n",
        "        pynini.cross(\"\", \"\"),  # optional\n",
        "    )\n",
        "\n",
        "    case_only_path = nominal_pl + case_no_poss + ki_suffix + plural_after_ki\n",
        "    nominal_base = pynini.union(possessive_path, case_only_path).optimize()\n",
        "\n",
        "    # Copula\n",
        "    copula = pynini.union(\n",
        "        pynini.cross(\"ydi\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"ydı\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"ydu\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"ydü\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"ymış\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"ymiş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"ymuş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"ymüş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"yse\", \"+COP.COND\"),\n",
        "        pynini.cross(\"ysa\", \"+COP.COND\"),\n",
        "        pynini.cross(\"di\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"dı\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"du\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"dü\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"ti\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"tı\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"tu\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"tü\", \"+COP.PAST\"),\n",
        "        pynini.cross(\"miş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"mış\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"muş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"müş\", \"+COP.EVID\"),\n",
        "        pynini.cross(\"se\", \"+COP.COND\"),\n",
        "        pynini.cross(\"sa\", \"+COP.COND\"),\n",
        "        pynini.cross(\"dir\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"dır\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"dur\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"dür\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"tir\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"tır\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"tur\", \"+COP.PRES\"),\n",
        "        pynini.cross(\"tür\", \"+COP.PRES\"),\n",
        "    )\n",
        "\n",
        "    person = pynini.union(\n",
        "        pynini.cross(\"im\", \"+1SG\"),\n",
        "        pynini.cross(\"ım\", \"+1SG\"),\n",
        "        pynini.cross(\"um\", \"+1SG\"),\n",
        "        pynini.cross(\"üm\", \"+1SG\"),\n",
        "        pynini.cross(\"in\", \"+2SG\"),\n",
        "        pynini.cross(\"ın\", \"+2SG\"),\n",
        "        pynini.cross(\"un\", \"+2SG\"),\n",
        "        pynini.cross(\"ün\", \"+2SG\"),\n",
        "        pynini.cross(\"ız\", \"+1PL\"),\n",
        "        pynini.cross(\"iz\", \"+1PL\"),\n",
        "        pynini.cross(\"uz\", \"+1PL\"),\n",
        "        pynini.cross(\"üz\", \"+1PL\"),\n",
        "        pynini.cross(\"nız\", \"+2PL\"),\n",
        "        pynini.cross(\"niz\", \"+2PL\"),\n",
        "        pynini.cross(\"nuz\", \"+2PL\"),\n",
        "        pynini.cross(\"nüz\", \"+2PL\"),\n",
        "        pynini.cross(\"lar\", \"+3PL\"),\n",
        "        pynini.cross(\"ler\", \"+3PL\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    nominal_with_cop = copula + person\n",
        "    nominal_complete = nominal_base + pynini.union(nominal_with_cop, pynini.cross(\"\", \"\")).optimize()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Verb morphology\n",
        "    # -------------------------------------------------------------------------\n",
        "    voice = pynini.union(\n",
        "        pynini.cross(\"ıl\", \"+PASS\"),\n",
        "        pynini.cross(\"il\", \"+PASS\"),\n",
        "        pynini.cross(\"ul\", \"+PASS\"),\n",
        "        pynini.cross(\"ül\", \"+PASS\"),\n",
        "        pynini.cross(\"ın\", \"+REFL\"),\n",
        "        pynini.cross(\"in\", \"+REFL\"),\n",
        "        pynini.cross(\"un\", \"+REFL\"),\n",
        "        pynini.cross(\"ün\", \"+REFL\"),\n",
        "        pynini.cross(\"lan\", \"+REFL\"),\n",
        "        pynini.cross(\"len\", \"+REFL\"),\n",
        "        pynini.cross(\"ış\", \"+RECIP\"),\n",
        "        pynini.cross(\"iş\", \"+RECIP\"),\n",
        "        pynini.cross(\"uş\", \"+RECIP\"),\n",
        "        pynini.cross(\"üş\", \"+RECIP\"),\n",
        "        pynini.cross(\"t\", \"+CAUS\"),\n",
        "        pynini.cross(\"d\", \"+CAUS\"),\n",
        "        pynini.cross(\"dır\", \"+CAUS\"),\n",
        "        pynini.cross(\"dir\", \"+CAUS\"),\n",
        "        pynini.cross(\"dur\", \"+CAUS\"),\n",
        "        pynini.cross(\"dür\", \"+CAUS\"),\n",
        "        pynini.cross(\"tır\", \"+CAUS\"),\n",
        "        pynini.cross(\"tir\", \"+CAUS\"),\n",
        "        pynini.cross(\"tur\", \"+CAUS\"),\n",
        "        pynini.cross(\"tür\", \"+CAUS\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    ability = pynini.union(\n",
        "        pynini.cross(\"ebil\", \"+ABIL\"),\n",
        "        pynini.cross(\"abil\", \"+ABIL\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    negation = pynini.union(\n",
        "        pynini.cross(\"ma\", \"+NEG\"),\n",
        "        pynini.cross(\"me\", \"+NEG\"),\n",
        "        pynini.cross(\"\", \"\"),\n",
        "    )\n",
        "\n",
        "    # Add fused FUT endings to cover gideceğim, yapacağım, gideceğiz, etc.\n",
        "    fused_future = pynini.union(\n",
        "        pynini.cross(\"acağım\", \"+FUT+1SG\"),\n",
        "        pynini.cross(\"eceğim\", \"+FUT+1SG\"),\n",
        "        pynini.cross(\"acağız\", \"+FUT+1PL\"),\n",
        "        pynini.cross(\"eceğiz\", \"+FUT+1PL\"),\n",
        "    )\n",
        "\n",
        "    indicative_tense = pynini.union(\n",
        "        pynini.cross(\"iyor\", \"+PRES.CONT\"),\n",
        "        pynini.cross(\"ıyor\", \"+PRES.CONT\"),\n",
        "        pynini.cross(\"uyor\", \"+PRES.CONT\"),\n",
        "        pynini.cross(\"üyor\", \"+PRES.CONT\"),\n",
        "        fused_future,\n",
        "        pynini.cross(\"ecek\", \"+FUT\"),\n",
        "        pynini.cross(\"acak\", \"+FUT\"),\n",
        "        pynini.cross(\"ır\", \"+AOR\"),\n",
        "        pynini.cross(\"ir\", \"+AOR\"),\n",
        "        pynini.cross(\"ur\", \"+AOR\"),\n",
        "        pynini.cross(\"ür\", \"+AOR\"),\n",
        "        pynini.cross(\"ar\", \"+AOR\"),\n",
        "        pynini.cross(\"er\", \"+AOR\"),\n",
        "        pynini.cross(\"r\", \"+AOR\"),\n",
        "        pynini.cross(\"dı\", \"+PAST\"),\n",
        "        pynini.cross(\"di\", \"+PAST\"),\n",
        "        pynini.cross(\"du\", \"+PAST\"),\n",
        "        pynini.cross(\"dü\", \"+PAST\"),\n",
        "        pynini.cross(\"tı\", \"+PAST\"),\n",
        "        pynini.cross(\"ti\", \"+PAST\"),\n",
        "        pynini.cross(\"tu\", \"+PAST\"),\n",
        "        pynini.cross(\"tü\", \"+PAST\"),\n",
        "        pynini.cross(\"mış\", \"+INFER\"),\n",
        "        pynini.cross(\"miş\", \"+INFER\"),\n",
        "        pynini.cross(\"muş\", \"+INFER\"),\n",
        "        pynini.cross(\"müş\", \"+INFER\"),\n",
        "    )\n",
        "\n",
        "    indicative_person = pynini.union(\n",
        "        pynini.cross(\"um\", \"+1SG\"),\n",
        "        pynini.cross(\"üm\", \"+1SG\"),\n",
        "        pynini.cross(\"ım\", \"+1SG\"),\n",
        "        pynini.cross(\"im\", \"+1SG\"),\n",
        "        pynini.cross(\"m\", \"+1SG\"),\n",
        "        pynini.cross(\"sun\", \"+2SG\"),\n",
        "        pynini.cross(\"sün\", \"+2SG\"),\n",
        "        pynini.cross(\"sın\", \"+2SG\"),\n",
        "        pynini.cross(\"sin\", \"+2SG\"),\n",
        "        pynini.cross(\"n\", \"+2SG\"),\n",
        "        pynini.cross(\"uz\", \"+1PL\"),\n",
        "        pynini.cross(\"üz\", \"+1PL\"),\n",
        "        pynini.cross(\"ız\", \"+1PL\"),\n",
        "        pynini.cross(\"iz\", \"+1PL\"),\n",
        "        pynini.cross(\"k\", \"+1PL\"),\n",
        "        pynini.cross(\"sunuz\", \"+2PL\"),\n",
        "        pynini.cross(\"sünüz\", \"+2PL\"),\n",
        "        pynini.cross(\"sınız\", \"+2PL\"),\n",
        "        pynini.cross(\"siniz\", \"+2PL\"),\n",
        "        pynini.cross(\"nız\", \"+2PL\"),\n",
        "        pynini.cross(\"niz\", \"+2PL\"),\n",
        "        pynini.cross(\"nuz\", \"+2PL\"),\n",
        "        pynini.cross(\"nüz\", \"+2PL\"),\n",
        "        pynini.cross(\"lar\", \"+3PL\"),\n",
        "        pynini.cross(\"ler\", \"+3PL\"),\n",
        "        pynini.cross(\"\", \"+3SG\"),\n",
        "    )\n",
        "\n",
        "    # Converbs (so bakınca/deyince/alınca don't go UNKNOWN)\n",
        "    converbs = pynini.union(\n",
        "        pynini.cross(\"ınca\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"ince\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"unca\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"ünce\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"yınca\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"yince\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"yunca\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"yünce\", \"+CVB.INCA\"),\n",
        "        pynini.cross(\"ken\", \"+CVB.KEN\"),\n",
        "        pynini.cross(\"ip\", \"+CVB.IP\"),\n",
        "        pynini.cross(\"ıp\", \"+CVB.IP\"),\n",
        "        pynini.cross(\"up\", \"+CVB.IP\"),\n",
        "        pynini.cross(\"üp\", \"+CVB.IP\"),\n",
        "        pynini.cross(\"madan\", \"+CVB.MADAN\"),\n",
        "        pynini.cross(\"meden\", \"+CVB.MEDEN\"),\n",
        "    )\n",
        "\n",
        "    optative_mood_person = pynini.union(\n",
        "        pynini.cross(\"ayım\", \"+OPT+1SG\"),\n",
        "        pynini.cross(\"eyim\", \"+OPT+1SG\"),\n",
        "        pynini.cross(\"ayum\", \"+OPT+1SG\"),\n",
        "        pynini.cross(\"eyüm\", \"+OPT+1SG\"),\n",
        "        pynini.cross(\"asın\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"esin\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"asun\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"esün\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"asana\", \"+OPT+2SG+EMPH\"),\n",
        "        pynini.cross(\"esene\", \"+OPT+2SG+EMPH\"),\n",
        "        pynini.cross(\"sana\", \"+OPT+2SG+EMPH\"),\n",
        "        pynini.cross(\"sene\", \"+OPT+2SG+EMPH\"),\n",
        "        pynini.cross(\"asan\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"esen\", \"+OPT+2SG\"),\n",
        "        pynini.cross(\"a\", \"+OPT+3SG\"),\n",
        "        pynini.cross(\"e\", \"+OPT+3SG\"),\n",
        "        pynini.cross(\"alım\", \"+OPT+1PL\"),\n",
        "        pynini.cross(\"elim\", \"+OPT+1PL\"),\n",
        "        pynini.cross(\"alum\", \"+OPT+1PL\"),\n",
        "        pynini.cross(\"elüm\", \"+OPT+1PL\"),\n",
        "        pynini.cross(\"asınız\", \"+OPT+2PL\"),\n",
        "        pynini.cross(\"esiniz\", \"+OPT+2PL\"),\n",
        "        pynini.cross(\"asunuz\", \"+OPT+2PL\"),\n",
        "        pynini.cross(\"esünüz\", \"+OPT+2PL\"),\n",
        "        pynini.cross(\"alar\", \"+OPT+3PL\"),\n",
        "        pynini.cross(\"eler\", \"+OPT+3PL\"),\n",
        "    )\n",
        "\n",
        "    conditional_mood_person = pynini.union(\n",
        "        pynini.cross(\"sam\", \"+COND+1SG\"),\n",
        "        pynini.cross(\"sem\", \"+COND+1SG\"),\n",
        "        pynini.cross(\"san\", \"+COND+2SG\"),\n",
        "        pynini.cross(\"sen\", \"+COND+2SG\"),\n",
        "        pynini.cross(\"sa\", \"+COND+3SG\"),\n",
        "        pynini.cross(\"se\", \"+COND+3SG\"),\n",
        "        pynini.cross(\"sak\", \"+COND+1PL\"),\n",
        "        pynini.cross(\"sek\", \"+COND+1PL\"),\n",
        "        pynini.cross(\"sanız\", \"+COND+2PL\"),\n",
        "        pynini.cross(\"seniz\", \"+COND+2PL\"),\n",
        "        pynini.cross(\"sanuz\", \"+COND+2PL\"),\n",
        "        pynini.cross(\"senüz\", \"+COND+2PL\"),\n",
        "        pynini.cross(\"salar\", \"+COND+3PL\"),\n",
        "        pynini.cross(\"seler\", \"+COND+3PL\"),\n",
        "    )\n",
        "\n",
        "    necessitative_mood_person = pynini.union(\n",
        "        pynini.cross(\"malıyım\", \"+NEC+1SG\"),\n",
        "        pynini.cross(\"meliyim\", \"+NEC+1SG\"),\n",
        "        pynini.cross(\"malıyum\", \"+NEC+1SG\"),\n",
        "        pynini.cross(\"meliyüm\", \"+NEC+1SG\"),\n",
        "        pynini.cross(\"malısın\", \"+NEC+2SG\"),\n",
        "        pynini.cross(\"melisin\", \"+NEC+2SG\"),\n",
        "        pynini.cross(\"malısun\", \"+NEC+2SG\"),\n",
        "        pynini.cross(\"melisün\", \"+NEC+2SG\"),\n",
        "        pynini.cross(\"malı\", \"+NEC+3SG\"),\n",
        "        pynini.cross(\"meli\", \"+NEC+3SG\"),\n",
        "        pynini.cross(\"malıyız\", \"+NEC+1PL\"),\n",
        "        pynini.cross(\"meliyiz\", \"+NEC+1PL\"),\n",
        "        pynini.cross(\"malıyuz\", \"+NEC+1PL\"),\n",
        "        pynini.cross(\"meliyüz\", \"+NEC+1PL\"),\n",
        "        pynini.cross(\"malısınız\", \"+NEC+2PL\"),\n",
        "        pynini.cross(\"melisiniz\", \"+NEC+2PL\"),\n",
        "        pynini.cross(\"malısunuz\", \"+NEC+2PL\"),\n",
        "        pynini.cross(\"melisünüz\", \"+NEC+2PL\"),\n",
        "        pynini.cross(\"malılar\", \"+NEC+3PL\"),\n",
        "        pynini.cross(\"meliler\", \"+NEC+3PL\"),\n",
        "    )\n",
        "\n",
        "    imperative_mood_person = pynini.union(\n",
        "        pynini.cross(\"sin\", \"+IMP+3SG\"),\n",
        "        pynini.cross(\"sın\", \"+IMP+3SG\"),\n",
        "        pynini.cross(\"sun\", \"+IMP+3SG\"),\n",
        "        pynini.cross(\"sün\", \"+IMP+3SG\"),\n",
        "        pynini.cross(\"in\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"ın\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"un\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"ün\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"iniz\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"ınız\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"unuz\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"ünüz\", \"+IMP+2PL\"),\n",
        "        pynini.cross(\"sinler\", \"+IMP+3PL\"),\n",
        "        pynini.cross(\"sınlar\", \"+IMP+3PL\"),\n",
        "        pynini.cross(\"sunlar\", \"+IMP+3PL\"),\n",
        "        pynini.cross(\"sünler\", \"+IMP+3PL\"),\n",
        "    )\n",
        "\n",
        "    imperative_2sg_bare = pynini.cross(\"\", \"+IMP+2SG\")\n",
        "\n",
        "    verb_base = verb_roots + voice + ability + negation\n",
        "\n",
        "    # Add converbs as another valid verb continuation\n",
        "    verb_converb = verb_base + converbs\n",
        "\n",
        "    verb_indicative = verb_base + indicative_tense + indicative_person\n",
        "    verb_optative = verb_base + optative_mood_person\n",
        "    verb_conditional = verb_base + conditional_mood_person\n",
        "    verb_necessitative = verb_base + necessitative_mood_person\n",
        "    verb_imperative = verb_base + pynini.union(imperative_mood_person, imperative_2sg_bare)\n",
        "\n",
        "    verb_complete = pynini.union(\n",
        "        verb_indicative,\n",
        "        verb_optative,\n",
        "        verb_conditional,\n",
        "        verb_necessitative,\n",
        "        verb_imperative,\n",
        "        verb_converb,\n",
        "    ).optimize()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Punctuation\n",
        "    # -------------------------------------------------------------------------\n",
        "    punctuation_suffix = pynini.union(\n",
        "        pynini.cross(\".\", \"+PUNCT.period\"),\n",
        "        pynini.cross(\",\", \"+PUNCT.comma\"),\n",
        "        pynini.cross(\"?\", \"+PUNCT.question\"),\n",
        "        pynini.cross(\"!\", \"+PUNCT.exclamation\"),\n",
        "        pynini.cross(\":\", \"+PUNCT.colon\"),\n",
        "        pynini.cross(\";\", \"+PUNCT.semicolon\"),\n",
        "        pynini.cross(\"\", \"\"),  # optional suffix AFTER a word\n",
        "    )\n",
        "\n",
        "    # Punctuation as a standalone token (NO epsilon here)\n",
        "    punctuation_only = pynini.union(\n",
        "        pynini.cross(\".\", \".+PUNCT.period\"),\n",
        "        pynini.cross(\",\", \",+PUNCT.comma\"),\n",
        "        pynini.cross(\"?\", \"?+PUNCT.question\"),\n",
        "        pynini.cross(\"!\", \"!+PUNCT.exclamation\"),\n",
        "        pynini.cross(\":\", \":+PUNCT.colon\"),\n",
        "        pynini.cross(\";\", \";+PUNCT.semicolon\"),\n",
        "    ).optimize()\n",
        "\n",
        "    # Define simple_categories here\n",
        "    simple_categories = pynini.union(\n",
        "        adverb_roots,\n",
        "        conjunction_roots,\n",
        "        interjection_roots,\n",
        "        question_particles,\n",
        "        postposition_roots\n",
        "    ).optimize()\n",
        "\n",
        "    nominal_fst = (nominal_complete + punctuation_suffix).optimize()\n",
        "    verb_fst = (verb_complete + punctuation_suffix).optimize()\n",
        "    simple_fst = (simple_categories + punctuation_suffix).optimize()\n",
        "\n",
        "    return pynini.union(nominal_fst, verb_fst, simple_fst, punctuation_only).optimize()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Analysis API\n",
        "# -----------------------------------------------------------------------------\n",
        "def analyze_word(word: str, analyzer: pynini.Fst) -> List[str]:\n",
        "    \"\"\"\n",
        "    Analyze a word and return all possible analyses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lattice = pynini.compose(word, analyzer)\n",
        "        analyses: List[str] = []\n",
        "        seen = set()\n",
        "\n",
        "        try:\n",
        "            for path in lattice.paths().ostrings():\n",
        "                if path not in seen:\n",
        "                    analyses.append(path)\n",
        "                    seen.add(path)\n",
        "        except Exception:\n",
        "            # Paths iteration fails when there are no paths or lattice isn't enumerable\n",
        "            pass\n",
        "\n",
        "        return sorted(analyses) if analyses else [f\"No analysis found for: {word}\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        return [f\"Error: {e}\"]\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Context-Aware Disambiguation (Viterbi)\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class Candidate:\n",
        "    word: str\n",
        "    analysis: str\n",
        "    tag: str\n",
        "    source: str = \"fst\"   # \"fst\" | \"oov\"\n",
        "\n",
        "\n",
        "class ContextAwareDisambiguator:\n",
        "    \"\"\"\n",
        "    Simple Viterbi decoder using a hand-written bigram POS transition model\n",
        "    + morphology-based heuristic boosts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, analyzer: pynini.Fst):\n",
        "        self.analyzer = analyzer\n",
        "\n",
        "        self.transitions = {\n",
        "            \"START\": {\"NOUN\": 0.4, \"PRON\": 0.3, \"ADV\": 0.1, \"VERB\": 0.1, \"ADJ\": 0.1},\n",
        "            \"ADJ\": {\"NOUN\": 0.9, \"ADJ\": 0.1, \"VERB\": 0.01},\n",
        "            \"NOUN\": {\"VERB\": 0.4, \"NOUN\": 0.2, \"CONJ\": 0.1, \"POSTP\": 0.2, \"ADV\": 0.1},\n",
        "            \"PRON\": {\"VERB\": 0.5, \"NOUN\": 0.2, \"POSTP\": 0.2, \"ADJ\": 0.1},\n",
        "            \"ADV\": {\"VERB\": 0.6, \"ADJ\": 0.3, \"ADV\": 0.1},\n",
        "            \"NUM\": {\"NOUN\": 0.95},\n",
        "            \"VERB\": {\"PUNCT\": 0.8, \"CONJ\": 0.1, \"NOUN\": 0.05, \"PRON\": 0.05},\n",
        "            \"QUES\": {\"PUNCT\": 0.9, \"VERB\": 0.1},\n",
        "            \"UNKNOWN\": {\"NOUN\": 0.3, \"VERB\": 0.3, \"ADJ\": 0.1, \"ADV\": 0.1, \"PRON\": 0.1, \"PUNCT\": 0.1},\n",
        "            \"DEFAULT\": {\"NOUN\": 0.3, \"VERB\": 0.3, \"ADJ\": 0.1, \"ADV\": 0.1, \"PRON\": 0.1, \"PUNCT\": 0.1},\n",
        "        }\n",
        "\n",
        "        self.tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"POSTP\", \"CONJ\", \"QUES\", \"INTERJ\", \"UNKNOWN\"]\n",
        "\n",
        "    def get_tag_from_analysis(self, analysis: str) -> str:\n",
        "        if \"No analysis\" in analysis:\n",
        "            return \"UNKNOWN\"\n",
        "        if \"+UNK\" in analysis or \"+OOV\" in analysis:\n",
        "            return \"UNKNOWN\"\n",
        "        if \"+QUES\" in analysis:\n",
        "            return \"QUES\"\n",
        "        if \"+PUNCT\" in analysis:\n",
        "            return \"PUNCT\"\n",
        "\n",
        "        for tag in self.tags:\n",
        "            if f\"+{tag}\" in analysis:\n",
        "                return tag\n",
        "        return \"NOUN\"\n",
        "\n",
        "    def get_transition_prob(self, prev_tag: str, current_tag: str) -> float:\n",
        "        if prev_tag in self.transitions:\n",
        "            return self.transitions[prev_tag].get(current_tag, 0.001)\n",
        "        return self.transitions[\"DEFAULT\"].get(current_tag, 0.001)\n",
        "\n",
        "    def heuristic_weight(self, word: str, analysis: str, tag: str, position: int, sentence_len: int) -> float:\n",
        "        score = 0.0\n",
        "        if tag == \"UNKNOWN\":\n",
        "          score -= 3.0\n",
        "        # Morphology hints\n",
        "        if tag == \"VERB\":\n",
        "            if word.endswith((\"yor\", \"yorum\", \"yorsun\", \"dı\", \"di\", \"du\", \"dü\", \"acak\", \"ecek\", \"malı\", \"meli\")):\n",
        "                score += 2.0\n",
        "        elif tag == \"NOUN\":\n",
        "            if word.endswith((\"lar\", \"ler\", \"in\", \"un\", \"nın\", \"nin\", \"da\", \"de\", \"dan\", \"den\")):\n",
        "                score += 1.5\n",
        "        elif tag == \"QUES\":\n",
        "            if word.lower().startswith((\"mi\", \"mı\", \"mu\", \"mü\")):\n",
        "                score += 5.0\n",
        "\n",
        "        # Turkish is often SOV; verbs are likely at end\n",
        "        if position == sentence_len - 1:\n",
        "            if tag in {\"VERB\", \"QUES\"}:\n",
        "                score += 1.5\n",
        "            if tag == \"NOUN\":\n",
        "                score -= 0.5\n",
        "\n",
        "        # Very short tokens rarely function as verbs mid-sentence\n",
        "        if len(word) <= 2 and tag == \"VERB\" and position != sentence_len - 1:\n",
        "            score -= 1.0\n",
        "\n",
        "        return score\n",
        "\n",
        "    def decode_sentence(self, sentence_tokens: List[str]) -> List[Candidate]:\n",
        "        lattice: List[List[Candidate]] = []\n",
        "\n",
        "        for word in sentence_tokens:\n",
        "            raw_analyses = analyze_word(word, self.analyzer)\n",
        "            candidates: List[Candidate] = []\n",
        "\n",
        "            no_path = (not raw_analyses) or (len(raw_analyses) == 1 and raw_analyses[0].startswith(\"No analysis found for:\"))\n",
        "            if no_path:\n",
        "              candidates.append(Candidate(word=word, analysis=f\"{word}+UNK+OOV\", tag=\"UNKNOWN\", source=\"oov\"))\n",
        "            else:\n",
        "                for ana in raw_analyses:\n",
        "                    tag = self.get_tag_from_analysis(ana)\n",
        "                    candidates.append(Candidate(word=word, analysis=ana, tag=tag, source=\"fst\"))\n",
        "\n",
        "            lattice.append(candidates)\n",
        "\n",
        "        n = len(lattice)\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        best_scores: List[Dict[int, float]] = [{} for _ in range(n)]\n",
        "        backpointers: List[Dict[int, int]] = [{} for _ in range(n)]\n",
        "\n",
        "        # Initialization\n",
        "        for i, cand in enumerate(lattice[0]):\n",
        "            trans_prob = self.get_transition_prob(\"START\", cand.tag)\n",
        "            heuristic = self.heuristic_weight(cand.word, cand.analysis, cand.tag, 0, n)\n",
        "            best_scores[0][i] = math.log(trans_prob) + heuristic\n",
        "\n",
        "        # Forward pass\n",
        "        for t in range(1, n):\n",
        "            for i, curr in enumerate(lattice[t]):\n",
        "                max_score = -float(\"inf\")\n",
        "                best_prev = -1\n",
        "\n",
        "                for j, prev in enumerate(lattice[t - 1]):\n",
        "                    prev_score = best_scores[t - 1][j]\n",
        "                    trans_prob = self.get_transition_prob(prev.tag, curr.tag)\n",
        "                    heuristic = self.heuristic_weight(curr.word, curr.analysis, curr.tag, t, n)\n",
        "\n",
        "                    score = prev_score + math.log(trans_prob) + heuristic\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        best_prev = j\n",
        "\n",
        "                best_scores[t][i] = max_score\n",
        "                backpointers[t][i] = best_prev\n",
        "\n",
        "        # Backtracking\n",
        "        best_last_idx = max(best_scores[n - 1], key=best_scores[n - 1].get)\n",
        "\n",
        "        path: List[Candidate] = []\n",
        "        curr_idx = best_last_idx\n",
        "\n",
        "        for t in range(n - 1, -1, -1):\n",
        "            path.append(lattice[t][curr_idx])\n",
        "            if t > 0:\n",
        "                curr_idx = backpointers[t][curr_idx]\n",
        "\n",
        "        return list(reversed(path))\n",
        "\n",
        "\n",
        "def tokenize(sentence: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic tokenizer: words + punctuation.\n",
        "    \"\"\"\n",
        "    return re.findall(r\"[\\w']+|[.,!?;:]\", sentence)\n",
        "\n",
        "\n",
        "def analyze_sentence_context_aware(sentence: str, disambiguator: ContextAwareDisambiguator):\n",
        "    tokens = tokenize(sentence)\n",
        "    best_path = disambiguator.decode_sentence(tokens)\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"token\": cand.word,\n",
        "            \"best_analysis\": cand.analysis,\n",
        "            \"tag\": cand.tag,\n",
        "            \"source\": cand.source,\n",
        "            \"is_oov\": (cand.source == \"oov\")\n",
        "        }\n",
        "        for cand in best_path\n",
        "    ]\n",
        "\n",
        "\n",
        "def save_fst(analyzer: pynini.Fst, filename: str) -> None:\n",
        "    analyzer.write(filename)\n",
        "    logger.info(\"FST saved to %s\", filename)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Debug / CLI\n",
        "# -----------------------------------------------------------------------------\n",
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "\n",
        "    lex = normalize_lexicon(load_lexicon())\n",
        "    analyzer = build_analyzer(lex)\n",
        "    disambiguator = ContextAwareDisambiguator(analyzer)\n",
        "\n",
        "    logger.info(\"Lexicon loaded\")\n",
        "    logger.info(\"Nouns: %d\", len(lex.get(\"nouns\", [])))\n",
        "    logger.info(\"Verbs: %d\", len(lex.get(\"verbs\", [])))\n",
        "    logger.info(\"Adjectives: %d\", len(lex.get(\"adjectives\", [])))\n",
        "    logger.info(\"Pronouns: %d\", len(lex.get(\"pronouns\", [])))\n",
        "    logger.info(\"Adverbs: %d\", len(lex.get(\"adverbs\", [])))\n",
        "    logger.info(\"Conjunctions: %d\", len(lex.get(\"conjunctions\", [])))\n",
        "    logger.info(\"Postpositions: %d\", len(lex.get(\"postpositions\", [])))\n",
        "    logger.info(\"Proper nouns: %d\", len(lex.get(\"proper_nouns\", [])))\n",
        "\n",
        "    # Sample tests\n",
        "    test_words = [\n",
        "        \"git\",\n",
        "        \"gitsin\",\n",
        "        \"giderim\",\n",
        "        \"gittim\",\n",
        "        \"gideceğim\",\n",
        "        \"gidiyorum\",\n",
        "        \"gitmeliyim\",\n",
        "        \"yapabilir\",\n",
        "        \"yapacaksın\",\n",
        "        \"anlasana\",\n",
        "        \"duysalar\",\n",
        "        \"baksaydım\",\n",
        "        \"çocuklardan\",\n",
        "        \"çantamdan\",\n",
        "        \"okuldakiler\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSINGLE WORD ANALYSIS\")\n",
        "    print(\"-\" * 60)\n",
        "    for w in test_words:\n",
        "        print(f\"\\n{w}:\")\n",
        "        for a in analyze_word(w, analyzer)[:5]:\n",
        "            print(f\"  {a}\")\n",
        "\n",
        "    ambiguous_sentences = [\n",
        "        \"Çay demledim ama çay kenarında yürümeyi de seviyorum; hangisini önce yapalım diye düşünürken zaman geçti.\",\n",
        "        \"Bana gül deyince çiçekten mi bahsediyorsun yoksa sadece gülümsememi mi istiyorsun, karar veremedim.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nCONTEXT-AWARE ANALYSIS (Viterbi)\")\n",
        "    print(\"-\" * 60)\n",
        "    for sent in ambiguous_sentences:\n",
        "        print(f\"\\nSentence: '{sent}'\")\n",
        "        results = analyze_sentence_context_aware(sent, disambiguator)\n",
        "\n",
        "        print(f\" {'Word':<15} | {'Tag':<6} | {'Selected Analysis'}\")\n",
        "        print(\"-\" * 70)\n",
        "        for r in results:\n",
        "            print(f\" {r['token']:<15} | {r['tag']:<6} | {r['best_analysis']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fu0JhZ9nnT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}