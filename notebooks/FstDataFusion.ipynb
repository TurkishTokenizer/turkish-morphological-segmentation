{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "gpuType": "T4", "authorship_tag": "ABX9TyNF+e/n1vkKiNVkhNZzDCeZ"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "# Part A: Veri Hazirlama\n", "Zemberek/Kaikki filtreleme, Stanza isleme, kelime temizligi.\n"]}, {"cell_type": "code", "source": ["import pandas as pd\n", "import json\n", "import sqlite3\n", "import os\n", "import glob\n", "from tqdm import tqdm\n", "\n", "# Ana Proje Yolu (Senin Drive yapÄ±na gÃ¶re)\n", "BASE_PATH = '/content/drive/MyDrive/FSTurk'\n", "os.chdir(BASE_PATH)\n", "\n", "print(f\"ğŸ“‚ Ã‡alÄ±ÅŸma dizini: {os.getcwd()}\")\n", "print(\"âœ… Gerekli kÃ¼tÃ¼phaneler yÃ¼klendi.\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "tykR2Zs5kVMF", "executionInfo": {"status": "ok", "timestamp": 1765828350348, "user_tz": -180, "elapsed": 33, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "a57b2e48-92db-4ea1-db04-613a5acc2b0f"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸ“‚ Ã‡alÄ±ÅŸma dizini: /content/drive/MyDrive/FSTurk\n", "âœ… Gerekli kÃ¼tÃ¼phaneler yÃ¼klendi.\n"]}]}, {"cell_type": "code", "source": ["#\" \", -, . gibi karakterler iÃ§in Ã¶zelleÅŸmiÅŸ ZEMBEREK VE KAIKKI FÄ°LTRASYONU\n", "\n", "import json\n", "import os\n", "import glob\n", "from collections import defaultdict\n", "\n", "# --- AYARLAR ---\n", "BASE_PATH = '/content/drive/MyDrive/FSTurk'\n", "ZEMBEREK_FILE = os.path.join(BASE_PATH, \"turkish_processed_zemberek_lexicon.json\")\n", "KAIKKI_FOLDER = os.path.join(BASE_PATH, \"kaikki_processing\")\n", "OUTPUT_FILE = os.path.join(BASE_PATH, \"final_lexicon_atomic_base.json\")\n", "\n", "# TR -> EN MAPPING\n", "TAG_MAPPING = {\n", "    'ISIM': 'NOUN',     'NOUN': 'NOUN',\n", "    'FIIL': 'VERB',     'VERB': 'VERB',\n", "    'SIFAT': 'ADJ',     'ADJ': 'ADJ',\n", "    'ZARF': 'ADV',      'ADV': 'ADV',\n", "    'ZAMIR': 'PRON',    'PRON': 'PRON',\n", "    'BAGLAC': 'CONJ',   'CONJ': 'CONJ', 'CCONJ': 'CONJ',\n", "    'SAYI': 'NUM',      'NUM': 'NUM',\n", "    'EDAT': 'ADP',      'ADP': 'ADP',\n", "    'BELIRTEC': 'DET',  'DET': 'DET',\n", "    'OZEL ISIM': 'NOUN', 'PROPN': 'NOUN',\n", "    'UNK': 'OTHER',     'OTHER': 'OTHER'\n", "}\n", "\n", "def is_clean_atomic_word(word, tag):\n", "    \"\"\"\n", "    Kelimenin FST iÃ§in uygun, temiz ve atomik olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.\n", "    \"\"\"\n", "    word = word.strip()\n", "\n", "    # 1. BoÅŸluk KontrolÃ¼ (Multi-word)\n", "    if ' ' in word: return False\n", "\n", "    # 2. Tire KontrolÃ¼ (Compound words) -> \"e-devlet\" istemiyoruz\n", "    if '-' in word: return False\n", "\n", "    # 3. Uzunluk KontrolÃ¼ (Tek harf 'o' hariÃ§ genelde Ã§Ã¶p olabilir ama ÅŸimdilik tutalÄ±m)\n", "    if len(word) == 0: return False\n", "\n", "    # 4. SayÄ±/Sembol KontrolÃ¼ (NUM kategorisi hariÃ§)\n", "    # EÄŸer kelime NUM deÄŸilse ama iÃ§inde rakam varsa (Ã¶rn: \"covid19\") atalÄ±m mÄ±?\n", "    # Evet, temiz sÃ¶zlÃ¼k iÃ§in atalÄ±m.\n", "    if tag != 'NUM':\n", "        # isalpha(): Sadece harf mi? (TÃ¼rkÃ§e karakterler ve ÅŸapkalar dahildir)\n", "        # Ancak tire ve boÅŸluk zaten yukarÄ±da elendi.\n", "        # BazÄ± kÄ±saltmalar (T.C.) noktalÄ± olabilir. FST genelde noktasÄ±z sever.\n", "        if '.' in word: return False\n", "\n", "        # Ä°Ã§inde rakam var mÄ±? (Word123 -> Ã‡Ã¶p)\n", "        if any(char.isdigit() for char in word): return False\n", "\n", "    return True\n", "\n", "def create_pristine_lexicon():\n", "    print(\"ğŸ’ PRISTINE (TERTEMÄ°Z) SÃ–ZLÃœK OLUÅTURULUYOR...\")\n", "    print(\"   ğŸš« Filtreler: BoÅŸluk, Tire (-), Nokta (.), RakamlÄ± Kelimeler\")\n", "\n", "    grouped_data = defaultdict(set)\n", "    dropped_count = 0\n", "\n", "    # --- 1. ZEMBEREK ---\n", "    print(f\"1ï¸âƒ£ Zemberek taranÄ±yor...\")\n", "    if os.path.exists(ZEMBEREK_FILE):\n", "        with open(ZEMBEREK_FILE, 'r', encoding='utf-8') as f:\n", "            raw_data = json.load(f)\n", "\n", "            # Veri listeyse\n", "            if isinstance(raw_data, list):\n", "                for item in raw_data:\n", "                    if len(item) >= 2:\n", "                        word = item[0]\n", "                        tr_tag = item[1]\n", "                        en_tag = TAG_MAPPING.get(tr_tag, 'OTHER')\n", "\n", "                        if is_clean_atomic_word(word, en_tag):\n", "                            grouped_data[en_tag].add(word.strip())\n", "                        else:\n", "                            dropped_count += 1\n", "\n", "            # Veri sÃ¶zlÃ¼kse\n", "            elif isinstance(raw_data, dict):\n", "                for tag, words in raw_data.items():\n", "                    en_tag = TAG_MAPPING.get(tag, 'OTHER')\n", "                    for w in words:\n", "                        if is_clean_atomic_word(w, en_tag):\n", "                            grouped_data[en_tag].add(w.strip())\n", "                        else:\n", "                            dropped_count += 1\n", "\n", "    # --- 2. KAIKKI ---\n", "    print(f\"2ï¸âƒ£ Kaikki taranÄ±yor...\")\n", "    jsonl_files = glob.glob(os.path.join(KAIKKI_FOLDER, \"*.jsonl\"))\n", "\n", "    for filepath in jsonl_files:\n", "        filename = os.path.basename(filepath)\n", "        if filename == \"tr-extract.jsonl\": continue\n", "\n", "        raw_tag = filename.replace('.jsonl', '')\n", "        en_tag = TAG_MAPPING.get(raw_tag, raw_tag) # VarsayÄ±lan: dosya adÄ±\n", "\n", "        with open(filepath, 'r', encoding='utf-8') as f:\n", "            for line in f:\n", "                try:\n", "                    data = json.loads(line)\n", "                    word = data.get('word')\n", "                    if word:\n", "                        if is_clean_atomic_word(word, en_tag):\n", "                            grouped_data[en_tag].add(word.strip())\n", "                        else:\n", "                            dropped_count += 1\n", "                except: continue\n", "\n", "    # --- 3. KAYDETME ---\n", "    print(f\"3ï¸âƒ£ Final dosya yazÄ±lÄ±yor...\")\n", "\n", "    final_output = {}\n", "    total_words = 0\n", "\n", "    for tag, word_set in grouped_data.items():\n", "        final_output[tag] = sorted(list(word_set))\n", "        total_words += len(final_output[tag])\n", "        print(f\"   ğŸ”¹ {tag}: {len(final_output[tag])} kelime\")\n", "\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(final_output, f, ensure_ascii=False)\n", "\n", "    print(f\"\\nâœ¨ TERTEMÄ°Z BAÅLANGIÃ‡ HAZIR!\")\n", "    print(f\"   ğŸ—‘ï¸ Filtrelenen (Ã‡Ã¶p/BileÅŸik) Kelime: {dropped_count}\")\n", "    print(f\"   ğŸ’ Saf Kelime SayÄ±sÄ±: {total_words}\")\n", "    print(f\"   ğŸ“‚ Dosya: {OUTPUT_FILE}\")\n", "\n", "create_pristine_lexicon()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "1-vb-i83mUF0", "executionInfo": {"status": "ok", "timestamp": 1765836644084, "user_tz": -180, "elapsed": 4223, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "da18cbf2-9f5d-43a0-c3c8-56f632caa28a"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸ’ PRISTINE (TERTEMÄ°Z) SÃ–ZLÃœK OLUÅTURULUYOR...\n", "   ğŸš« Filtreler: BoÅŸluk, Tire (-), Nokta (.), RakamlÄ± Kelimeler\n", "1ï¸âƒ£ Zemberek taranÄ±yor...\n", "2ï¸âƒ£ Kaikki taranÄ±yor...\n", "3ï¸âƒ£ Final dosya yazÄ±lÄ±yor...\n", "   ğŸ”¹ OTHER: 90758 kelime\n", "   ğŸ”¹ PRON: 135 kelime\n", "   ğŸ”¹ NOUN: 145538 kelime\n", "   ğŸ”¹ ADJ: 10831 kelime\n", "   ğŸ”¹ NUM: 11 kelime\n", "   ğŸ”¹ VERB: 68180 kelime\n", "   ğŸ”¹ CONJ: 52 kelime\n", "   ğŸ”¹ ADV: 1703 kelime\n", "\n", "âœ¨ TERTEMÄ°Z BAÅLANGIÃ‡ HAZIR!\n", "   ğŸ—‘ï¸ Filtrelenen (Ã‡Ã¶p/BileÅŸik) Kelime: 34980\n", "   ğŸ’ Saf Kelime SayÄ±sÄ±: 317208\n", "   ğŸ“‚ Dosya: /content/drive/MyDrive/FSTurk/final_lexicon_atomic_base.json\n"]}]}, {"cell_type": "code", "source": ["#--AMAÃ‡ WIKIMEDIA VERÄ°LERÄ°NÄ° STANZA iÃ§in  TÃœRLERÄ°NE AYIRMA--\n", "import json\n", "import os\n", "import re\n", "from tqdm import tqdm\n", "\n", "# --- AYARLAR ---\n", "BASE_PATH = '/content/drive/MyDrive/FSTurk'\n", "# Åu anki en gÃ¼ncel dosyan bu (Zemberek + Kaikki + GruplanmÄ±ÅŸ):\n", "INPUT_LEXICON = os.path.join(BASE_PATH, \"final_lexicon_grouped_english.json\")\n", "CORPUS_FILE = os.path.join(BASE_PATH, \"wikimedia_data\", \"cleaned_corpus.txt\")\n", "OUTPUT_FILE = os.path.join(BASE_PATH, \"contain_zemberek_kaikki_wikimedia_with_stanza.json\")\n", "\n", "def add_corpus_words_to_waiting_room():\n", "    print(\"ğŸš€ WIKIPEDIA TARAMASI BAÅLIYOR...\")\n", "\n", "    # 1. Mevcut SÃ¶zlÃ¼ÄŸÃ¼ YÃ¼kle\n", "    if not os.path.exists(INPUT_LEXICON):\n", "        print(\"âŒ SÃ¶zlÃ¼k bulunamadÄ±!\")\n", "        return\n", "\n", "    print(f\"1ï¸âƒ£ Mevcut sÃ¶zlÃ¼k yÃ¼kleniyor...\")\n", "    with open(INPUT_LEXICON, 'r', encoding='utf-8') as f:\n", "        lexicon_data = json.load(f)\n", "\n", "    # HÄ±zlÄ± kontrol iÃ§in mevcut kelimelerin hepsini bir kÃ¼meye (set) alalÄ±m\n", "    existing_words = set()\n", "    for tag, word_list in lexicon_data.items():\n", "        for w in word_list:\n", "            existing_words.add(w)\n", "\n", "    print(f\"   âœ… HafÄ±zada zaten {len(existing_words)} kelime var.\")\n", "\n", "    # 2. Corpus DosyasÄ±nÄ± Tara\n", "    print(f\"2ï¸âƒ£ Corpus dosyasÄ±ndan YENÄ° kelimeler aranÄ±yor...\")\n", "\n", "    if not os.path.exists(CORPUS_FILE):\n", "        print(\"âŒ Corpus dosyasÄ± yok! LÃ¼tfen wikimedia_data klasÃ¶rÃ¼nÃ¼ kontrol et.\")\n", "        return\n", "\n", "    new_candidates = set()\n", "    # Regex: Sadece TÃ¼rkÃ§e karakter iÃ§eren kelimeleri al (noktalama yok, sayÄ± yok)\n", "    word_pattern = re.compile(r\"[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ]+\")\n", "\n", "    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n", "        # Tqdm ile ilerleme Ã§ubuÄŸu gÃ¶ster\n", "        for line in tqdm(f, desc=\"Wiki TaranÄ±yor\"):\n", "            words = word_pattern.findall(line)\n", "            for w in words:\n", "                # KÃ¼Ã§Ã¼k harfe Ã§evir (Lexicon standardÄ± iÃ§in)\n", "                w_clean = w.lower()\n", "\n", "                # Kurallar:\n", "                # 1. 2 harften uzun olsun (ve, de, ki gibi baÄŸlaÃ§lar zaten vardÄ±r)\n", "                # 2. Zaten sÃ¶zlÃ¼kte OLMASIN\n", "                if len(w_clean) > 2 and w_clean not in existing_words:\n", "                    new_candidates.add(w_clean)\n", "\n", "    print(f\"   âœ¨ Wikipedia'dan {len(new_candidates)} adet YEPYENÄ° kelime bulundu!\")\n", "\n", "    # 3. BunlarÄ± 'OTHER' Kategorisine Ekle (Bekleme OdasÄ±)\n", "    if 'OTHER' not in lexicon_data:\n", "        lexicon_data['OTHER'] = []\n", "\n", "    # KÃ¼meyi listeye Ã§evirip ekle\n", "    lexicon_data['OTHER'].extend(list(new_candidates))\n", "\n", "    # OTHER iÃ§indeki tekrarlarÄ± temizle\n", "    lexicon_data['OTHER'] = sorted(list(set(lexicon_data['OTHER'])))\n", "\n", "    print(f\"   ğŸ“¦ Yeni kelimeler 'OTHER' (Formatlanacaklar) listesine eklendi.\")\n", "    print(f\"   ğŸ§ Åu an 'OTHER' toplam boyutu: {len(lexicon_data['OTHER'])}\")\n", "\n", "    # 4. Kaydet\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(lexicon_data, f, ensure_ascii=False)\n", "\n", "    print(f\"\\nğŸ’¾ Dosya Kaydedildi: {OUTPUT_FILE}\")\n", "    print(\"ğŸ‘‰ SIRADAKÄ° ADIM: Stanza kodunu bu dosya Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±p 'OTHER'larÄ± eritmek!\")\n", "\n", "add_corpus_words_to_waiting_room()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Rp9c-a1o6dWR", "executionInfo": {"status": "ok", "timestamp": 1765834591066, "user_tz": -180, "elapsed": 39345, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "f9d65a67-a515-4617-aa73-6f1f75fa5ba5"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸš€ WIKIPEDIA TARAMASI BAÅLIYOR...\n", "1ï¸âƒ£ Mevcut sÃ¶zlÃ¼k yÃ¼kleniyor...\n", "   âœ… HafÄ±zada zaten 322359 kelime var.\n", "2ï¸âƒ£ Corpus dosyasÄ±ndan YENÄ° kelimeler aranÄ±yor...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Wiki TaranÄ±yor: 100000it [00:35, 2812.30it/s]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["   âœ¨ Wikipedia'dan 1033121 adet YEPYENÄ° kelime bulundu!\n", "   ğŸ“¦ Yeni kelimeler 'OTHER' (Formatlanacaklar) listesine eklendi.\n", "   ğŸ§ Åu an 'OTHER' toplam boyutu: 1129886\n", "\n", "ğŸ’¾ Dosya Kaydedildi: /content/drive/MyDrive/FSTurk/contain_zemberek_kaikki_wikimedia_with_stanza.json\n", "ğŸ‘‰ SIRADAKÄ° ADIM: Stanza kodunu bu dosya Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±p 'OTHER'larÄ± eritmek!\n"]}]}, {"cell_type": "code", "source": ["#T4 SEÃ‡TÄ°ÄÄ°MDEN EMÄ°N OLUN.\n", "from google.colab import drive\n", "drive.mount('/content/drive')"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "IaSc_AVbJhia", "executionInfo": {"status": "ok", "timestamp": 1765838955246, "user_tz": -180, "elapsed": 1238, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "8add49b7-2338-4c07-c207-7dd6e31e475d"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}, {"cell_type": "code", "source": ["!pip install stanza"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eS2MavreJtRJ", "executionInfo": {"status": "ok", "timestamp": 1765838964534, "user_tz": -180, "elapsed": 3979, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "93580faf-943e-45e4-e5f0-17d639f054c1"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Requirement already satisfied: stanza in /usr/local/lib/python3.12/dist-packages (1.11.0)\n", "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from stanza) (2.15.0)\n", "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n", "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n", "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n", "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cu126)\n", "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n", "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n", "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.5)\n", "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.3.20)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n", "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n", "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.5.0)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.11.12)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n"]}]}, {"cell_type": "code", "source": ["import torch\n", "print(\"GPU Var mÄ±?:\", torch.cuda.is_available())\n", "try:\n", "    print(\"GPU AdÄ±:\", torch.cuda.get_device_name(0))\n", "    print(\"âœ… GPU AKTÄ°F! Devam edebilirsin.\")\n", "except:\n", "    print(\"âŒ GPU YOK! AyarlarÄ± kontrol et.\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MJ5BQBfmQlVK", "executionInfo": {"status": "ok", "timestamp": 1765839893128, "user_tz": -180, "elapsed": 4485, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "f3b70a8f-4cfb-4f40-a41f-3b617f4b191a"}, "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["GPU Var mÄ±?: True\n", "GPU AdÄ±: Tesla T4\n", "âœ… GPU AKTÄ°F! Devam edebilirsin.\n"]}]}, {"cell_type": "code", "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "oAkOLmlRQphJ", "executionInfo": {"status": "ok", "timestamp": 1765839923697, "user_tz": -180, "elapsed": 17790, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "ba819aef-4cad-4ae0-dc2d-f2044bff9da2"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Mounted at /content/drive\n"]}]}, {"cell_type": "code", "source": ["!pip install stanza"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8-G5yCk4Qtgs", "executionInfo": {"status": "ok", "timestamp": 1765839938137, "user_tz": -180, "elapsed": 6830, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "0ad69023-a06b-4845-f053-3077e97111d0"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Collecting stanza\n", "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n", "Collecting emoji (from stanza)\n", "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n", "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n", "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n", "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n", "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cu126)\n", "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n", "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n", "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.5)\n", "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.3.20)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n", "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n", "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.5.0)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.11.12)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n", "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n", "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n", "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25hInstalling collected packages: emoji, stanza\n", "Successfully installed emoji-2.15.0 stanza-1.11.0\n"]}]}, {"cell_type": "code", "source": ["import stanza\n", "import json\n", "import os\n", "from tqdm import tqdm\n", "import math\n", "import torch\n", "\n", "# --- AYARLAR ---\n", "BASE_PATH = '/content/drive/MyDrive/FSTurk'\n", "INPUT_FILE = os.path.join(BASE_PATH, \"contain_zemberek_kaikki_wikimedia_with_stanza.json\")\n", "OUTPUT_FILE = os.path.join(BASE_PATH, \"MASTER_FINAL_LEXICON.json\")\n", "TEMP_SAVE_FILE = os.path.join(BASE_PATH, \"temp_stanza_progress.json\")\n", "\n", "def turbo_stanza_safe():\n", "    # 1. GPU ZORUNLU KONTROL\n", "    if not torch.cuda.is_available():\n", "        raise Exception(\"âŒ HATA: GPU algÄ±lanamadÄ±! Runtime ayarlarÄ±ndan T4 GPU seÃ§ili mi?\")\n", "\n", "    print(\"âœ… GPU AlgÄ±landÄ±, Turbo Mod BaÅŸlatÄ±lÄ±yor...\")\n", "\n", "    # 2. Stanza YÃ¼kle\n", "    stanza.download('tr', processors='tokenize,pos', verbose=False)\n", "    # use_gpu=True ZORUNLU\n", "    nlp = stanza.Pipeline('tr', processors='tokenize,pos', use_gpu=True, verbose=False, tokenize_no_ssplit=True)\n", "\n", "    # 3. Veri YÃ¼kle\n", "    if not os.path.exists(INPUT_FILE):\n", "        print(\"âŒ Dosya bulunamadÄ±!\")\n", "        return\n", "\n", "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n", "        data = json.load(f)\n", "\n", "    full_other_list = data.get('OTHER', [])\n", "    print(f\"   ğŸ“¦ Toplam 'OTHER' Kelime SayÄ±sÄ±: {len(full_other_list)}\")\n", "\n", "    if len(full_other_list) == 0:\n", "        print(\"   âœ… Ä°ÅŸlenecek veri yok!\")\n", "        return\n", "\n", "    # 4. Checkpoint KontrolÃ¼\n", "    processed_results = {}\n", "\n", "    if os.path.exists(TEMP_SAVE_FILE):\n", "        print(\"   ğŸ“‚ KayÄ±tlÄ± ilerleme bulundu! Devam ediliyor...\")\n", "        with open(TEMP_SAVE_FILE, 'r', encoding='utf-8') as f:\n", "            processed_results = json.load(f)\n", "\n", "            # Zaten yapÄ±lanlarÄ± kÃ¼me (set) iÃ§ine alÄ±p hÄ±zla eleyelim\n", "            done_set = set()\n", "            for v_list in processed_results.values():\n", "                done_set.update(v_list)\n", "\n", "            # List comprehension ile filtrele\n", "            full_other_list = [w for w in full_other_list if w not in done_set]\n", "            print(f\"   â© {len(done_set)} kelime atlandÄ±. Kalan: {len(full_other_list)}\")\n", "\n", "    else:\n", "        processed_results = {\n", "            'NOUN': [], 'VERB': [], 'ADJ': [], 'ADV': [],\n", "            'PRON': [], 'CONJ': [], 'NUM': [], 'ADP': [], 'DET': []\n", "        }\n", "\n", "    # 5. BATCH PROCESSING\n", "    BATCH_SIZE = 500\n", "    total_batches = math.ceil(len(full_other_list) / BATCH_SIZE)\n", "\n", "    print(\"   ğŸ”¥ Ä°ÅŸlem BaÅŸlÄ±yor...\")\n", "    save_counter = 0\n", "\n", "    # Tqdm ile ilerle\n", "    for i in tqdm(range(0, len(full_other_list), BATCH_SIZE), total=total_batches, desc=\"Turbo GPU\"):\n", "        batch_words = full_other_list[i : i + BATCH_SIZE]\n", "\n", "        # Kelimeleri birleÅŸtir (Stanza iÃ§in tek metin)\n", "        batch_text = \"\\n\".join(batch_words)\n", "\n", "        try:\n", "            doc = nlp(batch_text)\n", "\n", "            for sentence in doc.sentences:\n", "                for word_obj in sentence.words:\n", "                    if word_obj.upos in ['NOUN', 'PROPN']: t = 'NOUN'\n", "                    elif word_obj.upos == 'VERB': t = 'VERB'\n", "                    elif word_obj.upos == 'ADJ': t = 'ADJ'\n", "                    elif word_obj.upos == 'ADV': t = 'ADV'\n", "                    elif word_obj.upos == 'PRON': t = 'PRON'\n", "                    elif word_obj.upos in ['CCONJ', 'SCONJ']: t = 'CONJ'\n", "                    elif word_obj.upos == 'NUM': t = 'NUM'\n", "                    elif word_obj.upos == 'ADP': t = 'ADP'\n", "                    elif word_obj.upos == 'DET': t = 'DET'\n", "                    else: t = 'UNK'\n", "\n", "                    if t != 'UNK':\n", "                        processed_results[t].append(word_obj.text)\n", "        except Exception as e:\n", "            # Hata olsa bile devam et, sadece o batch'i atla\n", "            continue\n", "\n", "        # KAYDETME (Her 20 batch - 10k kelime)\n", "        save_counter += 1\n", "        if save_counter % 20 == 0:\n", "            with open(TEMP_SAVE_FILE, 'w', encoding='utf-8') as f:\n", "                json.dump(processed_results, f, ensure_ascii=False)\n", "\n", "    # 6. FÄ°NAL\n", "    print(\"\\n   ğŸ’¾ Bitti! Kaydediliyor...\")\n", "    final_data = data.copy()\n", "    final_data['OTHER'] = []\n", "\n", "    for tag, new_words in processed_results.items():\n", "        if tag in final_data:\n", "            final_data[tag].extend(new_words)\n", "            final_data[tag] = sorted(list(set(final_data[tag])))\n", "        else:\n", "            final_data[tag] = sorted(list(set(new_words)))\n", "\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(final_data, f, ensure_ascii=False)\n", "\n", "    if os.path.exists(TEMP_SAVE_FILE):\n", "        os.remove(TEMP_SAVE_FILE)\n", "\n", "    print(f\"ğŸ† FÄ°NAL DOSYA HAZIR: {OUTPUT_FILE}\")\n", "\n", "turbo_stanza_safe()"], "metadata": {"id": "T_f3KLmfNKU7", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1765843397423, "user_tz": -180, "elapsed": 3421983, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "9019de09-64f3-4e06-ebf4-49e2933b6f40"}, "execution_count": 4, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["âœ… GPU AlgÄ±landÄ±, Turbo Mod BaÅŸlatÄ±lÄ±yor...\n", "   ğŸ“¦ Toplam 'OTHER' Kelime SayÄ±sÄ±: 1129886\n", "   ğŸ“‚ KayÄ±tlÄ± ilerleme bulundu! Devam ediliyor...\n", "   â© 39686 kelime atlandÄ±. Kalan: 1091019\n", "   ğŸ”¥ Ä°ÅŸlem BaÅŸlÄ±yor...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Turbo GPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2183/2183 [56:42<00:00,  1.56s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["\n", "   ğŸ’¾ Bitti! Kaydediliyor...\n", "ğŸ† FÄ°NAL DOSYA HAZIR: /content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json\n"]}]}, {"cell_type": "code", "source": ["import json\n", "import random\n", "import os\n", "\n", "FILE_PATH = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json'\n", "\n", "def check_lexicon_health():\n", "    if not os.path.exists(FILE_PATH):\n", "        print(\"âŒ Dosya bulunamadÄ±! Yolu kontrol et.\")\n", "        return\n", "\n", "    print(\"ğŸ“‚ Dosya yÃ¼kleniyor (Biraz sÃ¼rebilir)...\")\n", "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n", "        data = json.load(f)\n", "\n", "    print(f\"âœ… Dosya baÅŸarÄ±yla yÃ¼klendi!\")\n", "    print(\"=\"*40)\n", "\n", "    total_words = 0\n", "\n", "    # Her kategoriden rastgele 5 Ã¶rnek gÃ¶ster\n", "    for category, word_list in data.items():\n", "        count = len(word_list)\n", "        total_words += count\n", "        print(f\"ğŸ”¹ {category:<10} : {count:,} kelime\")\n", "\n", "        # Ã–rnekler\n", "        if count > 0:\n", "            sample = random.sample(word_list, min(5, count))\n", "            print(f\"   Ã–rnekler: {', '.join(sample)}\")\n", "        print(\"-\" * 20)\n", "\n", "    print(\"=\"*40)\n", "    print(f\"ğŸ“Š TOPLAM KELÄ°ME SAYISI: {total_words:,}\")\n", "\n", "check_lexicon_health()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "a73JtHSjfHLF", "executionInfo": {"status": "ok", "timestamp": 1765843697767, "user_tz": -180, "elapsed": 335, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "e5ee8e13-f74b-4a26-f4c2-24493df318db"}, "execution_count": 5, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸ“‚ Dosya yÃ¼kleniyor (Biraz sÃ¼rebilir)...\n", "âœ… Dosya baÅŸarÄ±yla yÃ¼klendi!\n", "========================================\n", "ğŸ”¹ OTHER      : 0 kelime\n", "--------------------\n", "ğŸ”¹ PRON       : 685 kelime\n", "   Ã–rnekler: benzeri, sizgen, ÅŸunla, birinede, kendilerinin\n", "--------------------\n", "ğŸ”¹ NOUN       : 986,200 kelime\n", "   Ã–rnekler: relerin, lappe, canmore, gaviiformes, stanbulkarta\n", "--------------------\n", "ğŸ”¹ ADJ        : 48,022 kelime\n", "   Ã–rnekler: klimatolojik, mÃ¼cellÃ¢, gotÄ±n, sevmeyenin, edepli\n", "--------------------\n", "ğŸ”¹ NUM        : 67,259 kelime\n", "   Ã–rnekler: stivers, klavo, felÃ¢ketzede, botanic, dakara\n", "--------------------\n", "ğŸ”¹ VERB       : 207,486 kelime\n", "   Ã–rnekler: getirileceksin, baÅŸlamayan, dengeleneceÄŸine, yÃ¼kseltilebilen, uzayacaktÄ±\n", "--------------------\n", "ğŸ”¹ CONJ       : 253 kelime\n", "   Ã–rnekler: ille velÃ¢kin, kaldÄ± ki, gÃ¶rÃ¼ro, muuttaa, emde\n", "--------------------\n", "ğŸ”¹ ADV        : 5,315 kelime\n", "   Ã–rnekler: taptaze, kÄ±z kÄ±za, memnun, bedava, ciddiyken\n", "--------------------\n", "ğŸ”¹ ADP        : 3,947 kelime\n", "   Ã–rnekler: kulluÄŸumuz, musavve, milleluci, aÄŸÄ±dÄ±yken, linde\n", "--------------------\n", "ğŸ”¹ DET        : 505 kelime\n", "   Ã–rnekler: yayÄ±nlanmÄ±ÅŸtÄ±rbu, dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼ndebu, bildirdibu, biridirbu, baÄŸlanmÄ±ÅŸtÄ±rbu\n", "--------------------\n", "========================================\n", "ğŸ“Š TOPLAM KELÄ°ME SAYISI: 1,319,672\n"]}]}, {"cell_type": "code", "source": ["#GEREKSÄ°Z KELÄ°ME TEMÄ°ZLÄ°ÄÄ° 1\n", "import json\n", "import re\n", "import os\n", "from collections import Counter\n", "\n", "# Dosya YollarÄ±\n", "INPUT_FILE = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json'\n", "OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json'\n", "\n", "def refinery_logic():\n", "    print(\"ğŸ§¹ Temizlik Operasyonu BaÅŸlÄ±yor...\")\n", "\n", "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n", "        data = json.load(f)\n", "\n", "    clean_lexicon = {\n", "        'NOUN': set(), 'VERB': set(), 'ADJ': set(),\n", "        'ADV': set(), 'PRON': set(), 'CONJ': set(),\n", "        'ADP': set(), 'DET': set(), 'NUM': set(), 'PROPN': set()\n", "    }\n", "\n", "    # Ä°stemediÄŸimiz karakterler\n", "    garbage_pattern = re.compile(r'[wqx0-9_@\\.\\-\\'\\+]')\n", "\n", "    # Fiil temizleyici (Basit Kural TabanlÄ± Stemmer)\n", "    # AmacÄ±mÄ±z \"getirileceksin\" -> \"getir\" yapmak\n", "    def clean_verb(word):\n", "        # Sondan eklemeli dillerde tersten budama\n", "        suffixes = [\n", "            'mek', 'mak', 'me', 'ma', 'iyor', 'Ä±yor', 'uyor', 'Ã¼yor',\n", "            'ecek', 'acak', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'dÄ±', 'di', 'du', 'dÃ¼',\n", "            'sÄ±n', 'sin', 'sun', 'sÃ¼n', 'lar', 'ler', 'nÄ±z', 'niz'\n", "        ]\n", "\n", "        root = word\n", "        # Basit bir dÃ¶ngÃ¼yle sondaki ekleri at (Ã‡ok agresif olmadan)\n", "        # KÃ¶k en az 3 harf kalmalÄ± (yememek -> ye)\n", "        original = word\n", "        for _ in range(3): # 3 katman ek silebilir\n", "            for suf in suffixes:\n", "                if root.endswith(suf) and len(root) > len(suf) + 2:\n", "                    root = root[:-len(suf)]\n", "                    break\n", "        return root\n", "\n", "    stats = {'deleted': 0, 'kept': 0, 'trimmed': 0}\n", "\n", "    for category, words in data.items():\n", "        print(f\"   SÃ¼zÃ¼lÃ¼yor: {category} ({len(words)} aday)...\")\n", "\n", "        for word in words:\n", "            word = word.lower().strip()\n", "\n", "            # 1. Kural: Ã‡Ã¶p karakterler ve Uzunluk\n", "            if garbage_pattern.search(word) or len(word) < 2 or len(word) > 12:\n", "                stats['deleted'] += 1\n", "                continue\n", "\n", "            # 2. Kural: YapÄ±ÅŸÄ±k Kelimeler (Parser HatalarÄ±)\n", "            # \"yapÄ±lmÄ±ÅŸtÄ±rbu\" -> sonu 'bu' ile bitiyorsa ve uzunsa ÅŸÃ¼phelidir\n", "            if (word.endswith('bu') or word.endswith('ve')) and len(word) > 8:\n", "                stats['deleted'] += 1\n", "                continue\n", "\n", "            # 3. Kural: Kategoriye Ã–zel Temizlik\n", "            if category == 'VERB':\n", "                root = clean_verb(word)\n", "                if root != word:\n", "                    stats['trimmed'] += 1\n", "                clean_lexicon['VERB'].add(root)\n", "\n", "            elif category == 'NOUN':\n", "                # Ä°simlerde Ã§ok fazla Ã§ekim eki temizlemiyoruz, FST halletsin\n", "                # Ama Ã§ok uzunlarÄ± almayalÄ±m\n", "                if len(word) < 10:\n", "                    clean_lexicon['NOUN'].add(word)\n", "\n", "            elif category == 'NUM':\n", "                # SayÄ± olmayanlarÄ± at (stivers vs.)\n", "                # YazÄ±yla yazÄ±lan sayÄ±lar kalsÄ±n (bir, iki, yÃ¼z...)\n", "                valid_nums = [\"bir\", \"iki\", \"Ã¼Ã§\", \"dÃ¶rt\", \"beÅŸ\", \"altÄ±\", \"yedi\", \"sekiz\", \"dokuz\", \"on\", \"yirmi\", \"otuz\", \"kÄ±rk\", \"elli\", \"altmÄ±ÅŸ\", \"yetmiÅŸ\", \"seksen\", \"doksan\", \"yÃ¼z\", \"bin\", \"milyon\", \"milyar\", \"sÄ±fÄ±r\"]\n", "                if word in valid_nums:\n", "                    clean_lexicon['NUM'].add(word)\n", "\n", "            elif category == 'DET':\n", "                # 'yayÄ±nlanmÄ±ÅŸtÄ±rbu' gibi hatalarÄ± elemek iÃ§in sadece bilinenleri alabiliriz\n", "                # veya kÄ±sa olanlarÄ± tutabiliriz\n", "                if len(word) < 8:\n", "                    clean_lexicon['DET'].add(word)\n", "\n", "            else:\n", "                clean_lexicon[category].add(word)\n", "\n", "    # Listeye Ã§evirip kaydet\n", "    final_output = {k: sorted(list(v)) for k, v in clean_lexicon.items()}\n", "\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(final_output, f, ensure_ascii=False)\n", "\n", "    print(\"\\nâœ… TEMÄ°ZLÄ°K BÄ°TTÄ°!\")\n", "    print(f\"   ğŸ—‘ï¸ Silinen (Ã‡Ã¶p/Hata): {stats['deleted']:,}\")\n", "    print(f\"   âœ‚ï¸ Budanan (Fiil KÃ¶kÃ¼): {stats['trimmed']:,}\")\n", "\n", "    total_clean = sum(len(v) for v in final_output.values())\n", "    print(f\"   ğŸ’ Kalan Saf Kelime: {total_clean:,}\")\n", "    print(f\"   ğŸ“‚ KayÄ±t Yeri: {OUTPUT_FILE}\")\n", "\n", "    # VERB Ã–rnekleri\n", "    print(f\"\\n   Fiil Ã–rnekleri (KÃ¶k Haline DÃ¶nmÃ¼ÅŸ mÃ¼?):\")\n", "    print(f\"   {final_output['VERB'][:10]}\")\n", "\n", "refinery_logic()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "klfvukEkfp0s", "executionInfo": {"status": "ok", "timestamp": 1765843872848, "user_tz": -180, "elapsed": 3111, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "7fb36fdf-9db4-4624-e121-6d2e2ff39b2d"}, "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸ§¹ Temizlik Operasyonu BaÅŸlÄ±yor...\n", "   SÃ¼zÃ¼lÃ¼yor: OTHER (0 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: PRON (685 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: NOUN (986200 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: ADJ (48022 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: NUM (67259 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: VERB (207486 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: CONJ (253 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: ADV (5315 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: ADP (3947 aday)...\n", "   SÃ¼zÃ¼lÃ¼yor: DET (505 aday)...\n", "\n", "âœ… TEMÄ°ZLÄ°K BÄ°TTÄ°!\n", "   ğŸ—‘ï¸ Silinen (Ã‡Ã¶p/Hata): 305,336\n", "   âœ‚ï¸ Budanan (Fiil KÃ¶kÃ¼): 42,839\n", "   ğŸ’ Kalan Saf Kelime: 650,514\n", "   ğŸ“‚ KayÄ±t Yeri: /content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json\n", "\n", "   Fiil Ã–rnekleri (KÃ¶k Haline DÃ¶nmÃ¼ÅŸ mÃ¼?):\n", "   ['aaaa', 'aaaaa', 'aaaaaa', 'aaaaaaaa', 'aaaaaad', 'aaadÄ±r', 'aban', 'abanabil', 'abanabilir', 'abanacaÄŸÄ±m']\n"]}]}, {"cell_type": "code", "source": ["#GEREKSÄ°Z EKLER TEMÄ°ZLÄ°ÄÄ°\n", "import json\n", "import re\n", "import os\n", "\n", "INPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json'\n", "OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json'\n", "\n", "def acid_bath_cleaning():\n", "    print(\"ğŸ§ª Asit Banyosu BaÅŸlÄ±yor (Agresif Temizlik)...\")\n", "\n", "    if not os.path.exists(INPUT_FILE):\n", "        print(\"âŒ Ã–nceki dosya yok! LÃ¼tfen Ã¶nce Rafineri 1'i Ã§alÄ±ÅŸtÄ±r.\")\n", "        return\n", "\n", "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n", "        data = json.load(f)\n", "\n", "    final_lexicon = {k: set() for k in data.keys()}\n", "\n", "    # TÃ¼rkÃ§e Sesli Harfler\n", "    vowels = set(\"aeÄ±ioÃ¶uÃ¼\")\n", "    # 3 tane yan yana aynÄ± harf yakalayan regex (aaa, bbb)\n", "    triple_char_regex = re.compile(r'(.)\\1\\1')\n", "\n", "    # Agresif Fiil BudayÄ±cÄ±\n", "    def aggressive_verb_stemmer(word):\n", "        # Ã–nce Ã§ekim ekleri (Tense/Person)\n", "        inflectional_suffixes = [\n", "            'casÄ±na', 'cesine', 'mÄ±', 'mi', 'mu', 'mÃ¼',\n", "            'tÄ±r', 'tir', 'tur', 'tÃ¼r', 'dÄ±r', 'dir', 'dur', 'dÃ¼r',\n", "            'nÄ±z', 'niz', 'nuz', 'nÃ¼z', 'lar', 'ler', 'sÄ±n', 'sin', 'sun', 'sÃ¼n',\n", "            'Ä±z', 'iz', 'uz', 'Ã¼z', 'Ä±m', 'im', 'um', 'Ã¼m',\n", "            'dÄ±', 'di', 'du', 'dÃ¼', 'tÄ±', 'ti', 'tu', 'tÃ¼',\n", "            'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'yacak', 'yecek', 'acak', 'ecek',\n", "            'yor', 'Ä±yor', 'iyor', 'uyor', 'Ã¼yor',\n", "            'malÄ±', 'meli', 'sa', 'se', 'a', 'e'\n", "        ]\n", "\n", "        # Sonra yapÄ±m/Ã§atÄ± ekleri (Derivational) - SÄ±ra Ã¶nemli!\n", "        derivational_suffixes = [\n", "            'abil', 'ebil', 'yabil', 'yebil', # Yeterlilik\n", "            'iver', 'Ä±ver', 'uver', 'Ã¼ver',   # Tezlik\n", "            'yaz', 'eyaz',                    # YaklaÅŸma\n", "            'dur', 'edur',                    # SÃ¼erlik\n", "            'mek', 'mak', 'me', 'ma'          # Mastar\n", "        ]\n", "\n", "        root = word\n", "\n", "        # 1. Tur: Ã‡ekim eklerini at\n", "        changed = True\n", "        while changed:\n", "            changed = False\n", "            for suf in inflectional_suffixes:\n", "                if root.endswith(suf) and len(root) > len(suf) + 2: # KÃ¶k en az 2 harf kalsÄ±n (ye, de)\n", "                    root = root[:-len(suf)]\n", "                    changed = True\n", "                    break\n", "\n", "        # 2. Tur: Ã‡atÄ±/BirleÅŸik eklerini at (aban-abil -> aban)\n", "        changed = True\n", "        while changed:\n", "            changed = False\n", "            for suf in derivational_suffixes:\n", "                if root.endswith(suf) and len(root) > len(suf) + 2:\n", "                    root = root[:-len(suf)]\n", "                    changed = True\n", "                    break\n", "\n", "        return root\n", "\n", "    stats = {'deleted': 0, 'trimmed': 0}\n", "\n", "    for category, word_list in data.items():\n", "        print(f\"   Ä°ÅŸleniyor: {category} ({len(word_list)} kelime)...\")\n", "\n", "        for word in word_list:\n", "            original_word = word\n", "\n", "            # KURAL 1: 'aaaa' kontrolÃ¼\n", "            if triple_char_regex.search(word):\n", "                stats['deleted'] += 1\n", "                continue\n", "\n", "            # KURAL 2: Sesli harf kontrolÃ¼ (Sessiz harflerden oluÅŸan kÄ±saltmalarÄ± at)\n", "            if not any(char in vowels for char in word):\n", "                stats['deleted'] += 1\n", "                continue\n", "\n", "            # KURAL 3: Ã‡ok uzun ve anlamsÄ±z kelimeler\n", "            if len(word) > 15: # TÃ¼rkÃ§e kÃ¶kler bu kadar uzun olmaz\n", "                stats['deleted'] += 1\n", "                continue\n", "\n", "            # KURAL 4: Agresif Stemming\n", "            if category == 'VERB':\n", "                root = aggressive_verb_stemmer(word)\n", "                if root != word:\n", "                    stats['trimmed'] += 1\n", "                final_lexicon['VERB'].add(root)\n", "            elif category == 'NOUN':\n", "                 # Ä°simlerde de basit Ã§oÄŸul eki temizliÄŸi yapalÄ±m\n", "                 if word.endswith('lar') and len(word) > 5:\n", "                     final_lexicon['NOUN'].add(word[:-3])\n", "                     stats['trimmed'] += 1\n", "                 elif word.endswith('ler') and len(word) > 5:\n", "                     final_lexicon['NOUN'].add(word[:-3])\n", "                     stats['trimmed'] += 1\n", "                 else:\n", "                     final_lexicon['NOUN'].add(word)\n", "            else:\n", "                final_lexicon[category].add(word)\n", "\n", "    # DosyayÄ± kaydet\n", "    output_data = {k: sorted(list(v)) for k, v in final_lexicon.items()}\n", "\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(output_data, f, ensure_ascii=False)\n", "\n", "    total_clean = sum(len(v) for v in output_data.values())\n", "\n", "    print(\"\\nğŸ§ª ASÄ°T BANYOSU TAMAMLANDI!\")\n", "    print(f\"   ğŸ—‘ï¸ Eritilen (aaaa/Ã§Ã¶p): {stats['deleted']:,}\")\n", "    print(f\"   âœ‚ï¸ Kesilen (aban-abil): {stats['trimmed']:,}\")\n", "    print(f\"   ğŸ’ Kalan Saf KÃ¶k: {total_clean:,}\")\n", "    print(f\"   ğŸ“‚ Yeni Dosya: {OUTPUT_FILE}\")\n", "\n", "    print(f\"\\n   Yeni Fiil Ã–rnekleri (Ä°lk 10):\")\n", "    print(f\"   {output_data['VERB'][:10]}\")\n", "\n", "acid_bath_cleaning()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7ag-N3YygQCb", "executionInfo": {"status": "ok", "timestamp": 1765844022496, "user_tz": -180, "elapsed": 3275, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "2d5fb8a4-db47-4343-bb6d-ff1b26c26ad2"}, "execution_count": 7, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸ§ª Asit Banyosu BaÅŸlÄ±yor (Agresif Temizlik)...\n", "   Ä°ÅŸleniyor: NOUN (507171 kelime)...\n", "   Ä°ÅŸleniyor: VERB (93679 kelime)...\n", "   Ä°ÅŸleniyor: ADJ (40859 kelime)...\n", "   Ä°ÅŸleniyor: ADV (4507 kelime)...\n", "   Ä°ÅŸleniyor: PRON (571 kelime)...\n", "   Ä°ÅŸleniyor: CONJ (196 kelime)...\n", "   Ä°ÅŸleniyor: ADP (3471 kelime)...\n", "   Ä°ÅŸleniyor: DET (42 kelime)...\n", "   Ä°ÅŸleniyor: NUM (18 kelime)...\n", "   Ä°ÅŸleniyor: PROPN (0 kelime)...\n", "\n", "ğŸ§ª ASÄ°T BANYOSU TAMAMLANDI!\n", "   ğŸ—‘ï¸ Eritilen (aaaa/Ã§Ã¶p): 6,010\n", "   âœ‚ï¸ Kesilen (aban-abil): 52,463\n", "   ğŸ’ Kalan Saf KÃ¶k: 613,755\n", "   ğŸ“‚ Yeni Dosya: /content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json\n", "\n", "   Yeni Fiil Ã–rnekleri (Ä°lk 10):\n", "   ['aban', 'abanabilir', 'abanacaÄŸ', 'aband', 'abandÄ±k', 'abandÄ±n', 'abandÄ±rÄ±r', 'abanmÄ±ÅŸsÄ±', 'abanozlaÅŸ', 'abanozlaÅŸÄ±r']\n"]}]}, {"cell_type": "code", "source": ["import json\n", "import os\n", "\n", "INPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json'\n", "OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_ROOTS.json'\n", "\n", "def turbo_smart_reduction():\n", "    print(\"ğŸš€ TURBO Ä°ndirgeme BaÅŸlÄ±yor (Set Lookup Strategy)...\")\n", "\n", "    if not os.path.exists(INPUT_FILE):\n", "        print(\"âŒ Dosya bulunamadÄ±!\")\n", "        return\n", "\n", "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n", "        data = json.load(f)\n", "\n", "    # Ek listesi (AynÄ± liste)\n", "    valid_suffixes = tuple([ # Tuple yaptÄ±k ki startswith daha hÄ±zlÄ± Ã§alÄ±ÅŸsÄ±n\n", "        'mak', 'mek', 'ma', 'me', 'Ä±ÅŸ', 'iÅŸ', 'uÅŸ', 'Ã¼ÅŸ',\n", "        'yor', 'yabil', 'yebil', 'yacak', 'yecek', 'r', 'ar', 'er', 'Ä±r', 'ir', 'ur', 'Ã¼r',\n", "        'dÄ±', 'di', 'du', 'dÃ¼', 'tÄ±', 'ti', 'tu', 'tÃ¼', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ',\n", "        'sÄ±n', 'sin', 'sun', 'sÃ¼n', 'nÄ±z', 'niz', 'k', 'n', 'm',\n", "        'abil', 'ebil', 'iver', 'Ä±ver', 'dur', 'calama', 'celeme',\n", "        'lar', 'ler', 'nÄ±n', 'nin', 'nun', 'nÃ¼n', 'dan', 'den', 'tan', 'ten',\n", "        'da', 'de', 'ta', 'te', 'ya', 'ye', 'yÄ±', 'yi', 'yu', 'yÃ¼',\n", "        'lÄ±', 'li', 'lu', 'lÃ¼', 'sÄ±z', 'siz', 'suz', 'sÃ¼z', 'lÄ±k', 'lik', 'luk', 'lÃ¼k',\n", "        'cÄ±', 'ci', 'cu', 'cÃ¼', 'Ã§a', 'Ã§e', 'ca', 'ce', 'ki', 'kÃ¼', 'sÄ±', 'si',\n", "        'Ä±', 'i', 'u', 'Ã¼' # Belirtme hali eklerini de ekledim\n", "    ])\n", "\n", "    final_lexicon = {}\n", "    stats = {'deleted': 0, 'kept': 0}\n", "\n", "    for category, word_list in data.items():\n", "        print(f\"   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: {category} ({len(word_list)} kelime)...\")\n", "\n", "        # 1. Kelimeleri yine kÄ±sadan uzuna sÄ±rala\n", "        sorted_words = sorted(list(word_list), key=len)\n", "\n", "        # SET kullanÄ±yoruz (Arama hÄ±zÄ± O(1))\n", "        kept_roots = set()\n", "\n", "        category_deleted = 0\n", "\n", "        for word in sorted_words:\n", "            # Ã‡ok kÄ±sa kelimeleri direkt al\n", "            if len(word) <= 3:\n", "                kept_roots.add(word)\n", "                continue\n", "\n", "            is_derivative = False\n", "\n", "            # YENÄ° MANTIK: Kelimenin kendisi iÃ§inde \"prefix\" arÄ±yoruz\n", "            # word = \"kitaplÄ±k\" -> i=3'ten baÅŸla\n", "            # kitaplÄ±k -> 'kit', 'kita', 'kitap'...\n", "\n", "            # Kelimenin uzunluÄŸu kadar dÃ¶ngÃ¼ (Max 15-20 tur, 1 Milyon tur deÄŸil!)\n", "            for i in range(3, len(word)):\n", "                prefix = word[:i] # Potansiyel kÃ¶k\n", "                suffix = word[i:] # Geriye kalan\n", "\n", "                # 1. Bu prefix bizim elimizdeki kÃ¶klerde var mÄ±?\n", "                if prefix in kept_roots:\n", "                    # 2. Geriye kalan kÄ±sÄ±m geÃ§erli bir ek mi?\n", "                    if suffix.startswith(valid_suffixes):\n", "                        is_derivative = True\n", "                        break\n", "\n", "            if is_derivative:\n", "                category_deleted += 1\n", "                stats['deleted'] += 1\n", "            else:\n", "                kept_roots.add(word)\n", "\n", "        final_lexicon[category] = sorted(list(kept_roots))\n", "        print(f\"      -> {category} bitti. {category_deleted} kelime elendi.\")\n", "\n", "    # DosyayÄ± kaydet\n", "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n", "        json.dump(final_lexicon, f, ensure_ascii=False)\n", "\n", "    total = sum(len(v) for v in final_lexicon.values())\n", "\n", "    print(\"\\nğŸ† TURBO OPERASYON TAMAMLANDI!\")\n", "    print(f\"   ğŸ—‘ï¸ Toplam Elenen: {stats['deleted']:,}\")\n", "    print(f\"   ğŸ’ Kalan Saf KÃ¶kler: {total:,}\")\n", "    print(f\"   ğŸ“‚ Dosya: {OUTPUT_FILE}\")\n", "\n", "    # Kontrol\n", "    if 'VERB' in final_lexicon:\n", "        sample = [w for w in final_lexicon['VERB'] if w.startswith('aban')]\n", "        print(f\"\\n   'aban' KontrolÃ¼: {sample}\")\n", "\n", "turbo_smart_reduction()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "sOIHlZttgtYZ", "executionInfo": {"status": "ok", "timestamp": 1765845777643, "user_tz": -180, "elapsed": 1479, "user": {"displayName": "Atakan YÄ±lmaz", "userId": "18394375161712046586"}}, "outputId": "0718cd4e-569e-4d6e-822d-48b89161025c"}, "execution_count": 9, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["ğŸš€ TURBO Ä°ndirgeme BaÅŸlÄ±yor (Set Lookup Strategy)...\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: NOUN (492050 kelime)...\n", "      -> NOUN bitti. 348169 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: VERB (72152 kelime)...\n", "      -> VERB bitti. 54776 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADJ (40782 kelime)...\n", "      -> ADJ bitti. 12758 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADV (4502 kelime)...\n", "      -> ADV bitti. 608 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: PRON (571 kelime)...\n", "      -> PRON bitti. 385 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: CONJ (193 kelime)...\n", "      -> CONJ bitti. 8 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADP (3445 kelime)...\n", "      -> ADP bitti. 619 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: DET (42 kelime)...\n", "      -> DET bitti. 4 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: NUM (18 kelime)...\n", "      -> NUM bitti. 0 kelime elendi.\n", "   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: PROPN (0 kelime)...\n", "      -> PROPN bitti. 0 kelime elendi.\n", "\n", "ğŸ† TURBO OPERASYON TAMAMLANDI!\n", "   ğŸ—‘ï¸ Toplam Elenen: 417,327\n", "   ğŸ’ Kalan Saf KÃ¶kler: 196,428\n", "   ğŸ“‚ Dosya: /content/drive/MyDrive/FSTurk/MASTER_FINAL_ROOTS.json\n", "\n", "   'aban' KontrolÃ¼: ['aban', 'abanacaÄŸ', 'aband', 'abanozlaÅŸ']\n"]}]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "# Part B: UD -> FST Esleme\n", "Universal Dependencies (UD) Turkish-IMST verilerinin FST formatina deterministik donusumu.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ğŸ”„ Universal Dependencies (UD) â†’ FST EÅŸleme ModÃ¼lÃ¼\n", "\n", "Bu bÃ¶lÃ¼m, UD Turkish-IMST formatÄ±ndaki anotasyonlarÄ± FST formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n", "\n", "**FST formatÄ±:** `lemma+POS+FEATURE1+FEATURE2+...`\n", "\n", "**UD formatÄ±:** FORM | LEMMA | UPOS | FEATS (Key=Value|...)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 1: UD â†’ FST EÅLEMÄ° TAM TABLOLARl\n", "# ============================================================\n", "\n", "# â”€â”€ 1A. UPOS â†’ FST POS EÅŸleme â”€â”€\n", "UPOS_TO_FST_POS = {\n", "    'NOUN':  'NOUN',\n", "    'PROPN': 'PROPN',\n", "    'VERB':  'VERB',\n", "    'ADJ':   'ADJ',\n", "    'ADV':   'ADV',\n", "    'PRON':  'PRON',\n", "    'DET':   'DET',       # FST'de ayrÄ± kategori yok, PRON/ADJ'ye dÃ¼ÅŸebilir\n", "    'ADP':   'POSTP',     # TÃ¼rkÃ§e'de postposition\n", "    'CCONJ': 'CONJ',\n", "    'SCONJ': 'CONJ',\n", "    'NUM':   'NUM',\n", "    'AUX':   'AUX',       # FST'de genelde copula (+COP.*) olarak modellenir\n", "    'INTJ':  'INTERJ',\n", "    'PUNCT': 'PUNCT',\n", "    'X':     'UNK',\n", "    'SYM':   'PUNCT',\n", "}\n", "\n", "# â”€â”€ 1B. Case (Durum Eki) EÅŸleme â”€â”€\n", "CASE_MAP = {\n", "    'Nom': '',           # Ä°ÅŸaretsiz (unmarked) - FST'de tag yok\n", "    'Acc': '+ACC',\n", "    'Dat': '+DAT',\n", "    'Loc': '+LOC',\n", "    'Abl': '+ABL',\n", "    'Gen': '+GEN',\n", "    'Ins': '+INS',\n", "    'Equ': '+EQU',       # -ca/-ce eÅŸitlik\n", "}\n", "\n", "# â”€â”€ 1C. Number (SayÄ±) EÅŸleme â”€â”€\n", "NUMBER_MAP = {\n", "    'Sing': '',          # Tekil - iÅŸaretsiz\n", "    'Plur': '+PL',\n", "}\n", "\n", "# â”€â”€ 1D. Person (KiÅŸi) EÅŸleme â”€â”€\n", "# Number + Person birleÅŸtirilerek FST person tag Ã¼retilir\n", "PERSON_NUMBER_MAP = {\n", "    ('Sing', '1'): '+1SG',\n", "    ('Sing', '2'): '+2SG',\n", "    ('Sing', '3'): '+3SG',   # BazÄ± baÄŸlamlarda boÅŸ olabilir\n", "    ('Plur', '1'): '+1PL',\n", "    ('Plur', '2'): '+2PL',\n", "    ('Plur', '3'): '+3PL',\n", "}\n", "\n", "# â”€â”€ 1E. Tense (Zaman) EÅŸleme â”€â”€\n", "TENSE_MAP = {\n", "    'Past': '+PAST',\n", "    'Pres': '',          # Åimdiki zaman - Aspect ile birlikte deÄŸerlendirilir\n", "    'Fut':  '+FUT',\n", "}\n", "\n", "# â”€â”€ 1F. Aspect (GÃ¶rÃ¼nÃ¼ÅŸ) EÅŸleme â”€â”€\n", "ASPECT_MAP = {\n", "    'Perf': '',          # Perfective - genelde tense ile birlikte\n", "    'Prog': '+PRES.CONT',  # -Iyor (ÅŸimdiki zaman sÃ¼rerlik)\n", "    'Hab':  '+AOR',      # GeniÅŸ zaman\n", "    'Rapid':'+IVER',     # Tezlik fiili (nadiren)\n", "}\n", "\n", "# â”€â”€ 1G. Mood (Kip) EÅŸleme â”€â”€\n", "MOOD_MAP = {\n", "    'Ind':  '',          # Bildirme kipi - varsayÄ±lan\n", "    'Imp':  '+IMP',\n", "    'Opt':  '+OPT',      # Ä°stek kipi (-a/-e)\n", "    'Des':  '+OPT',      # Dilek kipi (UD'de Desiderative = FST'de OPT)\n", "    'Nec':  '+NEC',      # Gereklilik (-malÄ±/-meli)\n", "    'Pot':  '+ABIL',     # Yeterlilik (-abil/-ebil)\n", "    'Cnd':  '+COND',     # Åart kipi (-sa/-se)\n", "    'Gen':  '+COP.PRES', # Genelleyici -(I)dIr\n", "    'CndGen':'+COND',    # KoÅŸullu genelleyici (nadir)\n", "}\n", "\n", "# â”€â”€ 1H. Voice (Ã‡atÄ±) EÅŸleme â”€â”€\n", "VOICE_MAP = {\n", "    'Pass': '+PASS',     # Edilgen\n", "    'Cau':  '+CAUS',     # Ettirgen\n", "    'Rfl':  '+REFL',     # DÃ¶nÃ¼ÅŸlÃ¼\n", "    'Rcp':  '+RECIP',    # Ä°ÅŸteÅŸ\n", "    'CauPass':'+CAUS+PASS', # Ettirgen + Edilgen\n", "}\n", "\n", "# â”€â”€ 1I. Polarity (Olumsuzluk) EÅŸleme â”€â”€\n", "POLARITY_MAP = {\n", "    'Pos': '',           # Olumlu - iÅŸaretsiz\n", "    'Neg': '+NEG',\n", "}\n", "\n", "# â”€â”€ 1J. Possessive (Ä°yelik) EÅŸleme â”€â”€\n", "# UD: Number[psor] + Person[psor] â†’ FST: +POSS.XY\n", "POSS_MAP = {\n", "    ('Sing', '1'): '+POSS.1SG',\n", "    ('Sing', '2'): '+POSS.2SG',\n", "    ('Sing', '3'): '+POSS.3SG',\n", "    ('Plur', '1'): '+POSS.1PL',\n", "    ('Plur', '2'): '+POSS.2PL',\n", "    ('Plur', '3'): '+POSS.3PL',\n", "}\n", "\n", "# â”€â”€ 1K. Evident (KanÄ±tsallÄ±k) EÅŸleme â”€â”€\n", "EVIDENT_MAP = {\n", "    'Fh':  '',           # Birinci elden (firsthand) - varsayÄ±lan\n", "    'Nfh': '+INFER',     # Duyulan geÃ§miÅŸ (-mIÅŸ)\n", "}\n", "\n", "# â”€â”€ 1L. VerbForm (Fiil BiÃ§imi) EÅŸleme â”€â”€\n", "VERBFORM_MAP = {\n", "    'Part':  '',         # SÄ±fat-fiil - baÄŸlama gÃ¶re analiz\n", "    'Conv':  '+CVB',     # Zarf-fiil (belirli alt tip baÄŸlama gÃ¶re)\n", "    'Vnoun': '',         # Ä°sim-fiil (-mA, -mAk, -IÅŸ)\n", "    'Fin':   '',         # Ã‡ekimli fiil - varsayÄ±lan\n", "}\n", "\n", "# â”€â”€ 1M. Derivational (TÃ¼retim) EÅŸleme â”€â”€\n", "# UD'de doÄŸrudan yok, ancak bazÄ± NAdj etiketleri ipucu verir\n", "# FST: +DER.lÄ±k, +DER.cÄ±, +DER.sÄ±z, +DER.lÄ± vb.\n", "# Bu Ã¶zellikler UD'den otomatik Ã§Ä±karÄ±lamaz, lemma+XPOS'tan tahmin edilebilir\n", "\n", "print('âœ… EÅŸleme tablolarÄ± yÃ¼klendi.')\n", "print(f'   UPOS eÅŸleme: {len(UPOS_TO_FST_POS)} kategori')\n", "print(f'   Case eÅŸleme: {len(CASE_MAP)} durum')\n", "print(f'   Tense eÅŸleme: {len(TENSE_MAP)} zaman')\n", "print(f'   Mood eÅŸleme: {len(MOOD_MAP)} kip')\n", "print(f'   Voice eÅŸleme: {len(VOICE_MAP)} Ã§atÄ±')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## âš ï¸ Belirsiz / DoÄŸrudan EÅŸlenemeyen Ã–zellikler\n", "\n", "| Durum | UD TarafÄ± | FST TarafÄ± | Karar |\n", "|-------|-----------|------------|-------|\n", "| **AUX (yardÄ±mcÄ± fiil)** | `AUX` (deÄŸil, i-) | `+COP.*` (copula eki) | AUX â†’ COP altÄ± etiketlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r |\n", "| **DET (belirteÃ§)** | `DET` (bir, bu, o) | FST'de yok | DET kalÄ±r veya PRON'a atanÄ±r |\n", "| **Aspect=Perf** | BitmiÅŸlik | FST'de yok | Tense ile birlikte yorumlanÄ±r |\n", "| **Polite=Infm** | KayÄ±t dÄ±ÅŸÄ± | FST'de yok | Ä°hmal edilir (bilgi kaybÄ±) |\n", "| **PronType** | Dem/Prs/Qes... | FST'de yok | Ä°hmal, ama geri Ã§evrilebilirlik kaybÄ± |\n", "| **3SG varsayÄ±lan** | AÃ§Ä±k `Person=3` | FST bazen boÅŸ bÄ±rakÄ±r | TutarlÄ±lÄ±k iÃ§in +3SG yazÄ±lÄ±r |\n", "| **NAdj (XPOS)** | ADJ/NOUN arasÄ± | NOUN veya ADJ | UPOS'a gÃ¼venilir |\n", "| **TÃ¼retim ekleri** | UD'de `li/sÄ±z/lÄ±k/cÄ±` | `+DER.*` | Lemma farkÄ±ndan Ã§Ä±karÄ±m yapÄ±lÄ±r |\n", "| **VerbForm=Part** | SÄ±fat-fiil | FST'de explicit yok | VERB+Part notasyonu eklenir |\n", "| **VerbForm=Conv** | Zarf-fiil | `+CVB.*` | Alt tip belirsiz, genel +CVB |\n", "\n", "### ğŸ”´ FST Daha Zengin OlduÄŸu Durumlar\n", "- **+EMPH** (vurgulama): UD'de karÅŸÄ±lÄ±ÄŸÄ± yok â†’ UDâ†’FST'de Ã¼retilemez\n", "- **+KI** (relativizer): UD'de `ADP` `ki` olarak ayrÄ± token â†’ birleÅŸtirme gerekir\n", "- **+CVB.INCA/KEN/IP/MADAN**: UD sadece `Conv` der, FST alt tip belirtir\n", "- **+DER.lÄ±k/cÄ±/sÄ±z/lÄ±**: UD'de morfolojik tÃ¼retim belirtilmez\n", "\n", "### ğŸŸ¡ UD Daha Zengin OlduÄŸu Durumlar\n", "- **Polite=Infm/Form**: FST'de karÅŸÄ±lÄ±k yok â†’ ihmal\n", "- **PronType=Prs/Dem/Qes**: FST zamirleri alt tiplere ayÄ±rmaz â†’ ihmal\n", "- **NumType=Card/Ord**: FST'de NUM tek kategori â†’ ihmal\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 2: map_ud_to_fst() ANA FONKSÄ°YONU\n", "# ============================================================\n", "\n", "def parse_ud_feats(feats_str):\n", "    \"\"\"UD FEATS string'ini dict'e Ã§evirir: 'Case=Acc|Number=Sing' â†’ {'Case':'Acc', ...}\"\"\"\n", "    if not feats_str or feats_str == '_':\n", "        return {}\n", "    result = {}\n", "    for pair in feats_str.split('|'):\n", "        if '=' in pair:\n", "            k, v = pair.split('=', 1)\n", "            result[k] = v\n", "    return result\n", "\n", "\n", "def map_ud_to_fst(lemma, upos, feats_dict, xpos=None):\n", "    \"\"\"\n", "    UD anotasyonunu FST analiz string'ine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n", "    \n", "    Args:\n", "        lemma (str): UD lemma\n", "        upos (str): UD UPOS etiketi\n", "        feats_dict (dict): UD Ã¶zellik sÃ¶zlÃ¼ÄŸÃ¼ {'Case':'Acc', ...}\n", "        xpos (str): Ä°steÄŸe baÄŸlÄ± XPOS etiketi (NAdj algÄ±lama iÃ§in)\n", "    \n", "    Returns:\n", "        str: FST formatÄ±nda analiz string'i\n", "        list: UyarÄ±lar/bilgi kayÄ±plarÄ± listesi\n", "    \"\"\"\n", "    warnings = []\n", "    tags = []\n", "    \n", "    # â”€â”€ 1. POS EÅŸleme â”€â”€\n", "    fst_pos = UPOS_TO_FST_POS.get(upos, 'UNK')\n", "    if fst_pos == 'UNK':\n", "        warnings.append(f'Bilinmeyen UPOS: {upos}')\n", "    \n", "    # AUX Ã¶zel iÅŸlem: copula olarak eÅŸle\n", "    if upos == 'AUX':\n", "        return _map_aux_to_fst(lemma, feats_dict, warnings)\n", "    \n", "    # PUNCT Ã¶zel iÅŸlem\n", "    if upos == 'PUNCT':\n", "        punct_map = {'.': '+PUNCT.period', ',': '+PUNCT.comma', \n", "                     '?': '+PUNCT.question', '!': '+PUNCT.exclamation',\n", "                     ':': '+PUNCT.colon', ';': '+PUNCT.semicolon',\n", "                     '...': '+PUNCT.ellipsis', '-': '+PUNCT.dash'}\n", "        suffix = punct_map.get(lemma, '+PUNCT')\n", "        return f'{lemma}{suffix}', warnings\n", "    \n", "    # â”€â”€ 2. Fiil Morfolojisi â”€â”€\n", "    if fst_pos == 'VERB':\n", "        return _map_verb_to_fst(lemma, feats_dict, warnings)\n", "    \n", "    # â”€â”€ 3. Ä°sim/SÄ±fat/Zamir Morfolojisi â”€â”€\n", "    if fst_pos in ('NOUN', 'PROPN', 'ADJ', 'PRON', 'NUM', 'DET'):\n", "        return _map_nominal_to_fst(lemma, fst_pos, feats_dict, warnings)\n", "    \n", "    # â”€â”€ 4. Basit Kategoriler (ADV, CONJ, POSTP, INTERJ) â”€â”€\n", "    return f'{lemma}+{fst_pos}', warnings\n", "\n", "\n", "def _map_verb_to_fst(lemma, feats, warnings):\n", "    \"\"\"Fiil morfolojisini FST formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\"\"\"\n", "    parts = [f'{lemma}+VERB']\n", "    \n", "    # Ã‡atÄ± (Voice)\n", "    voice = feats.get('Voice', '')\n", "    if voice and voice in VOICE_MAP:\n", "        parts.append(VOICE_MAP[voice])\n", "    \n", "    # Yeterlilik (Mood=Pot â†’ +ABIL)\n", "    mood = feats.get('Mood', 'Ind')\n", "    if mood == 'Pot':\n", "        parts.append('+ABIL')\n", "    \n", "    # Olumsuzluk (Polarity)\n", "    polarity = feats.get('Polarity', 'Pos')\n", "    neg_tag = POLARITY_MAP.get(polarity, '')\n", "    if neg_tag:\n", "        parts.append(neg_tag)\n", "    \n", "    # VerbForm kontrol (Part, Conv, Vnoun)\n", "    verb_form = feats.get('VerbForm', '')\n", "    \n", "    if verb_form == 'Conv':\n", "        # Zarf-fiil - UD alt tip belirtmez, genel CVB\n", "        parts.append('+CVB')\n", "        warnings.append('CVB alt tipi (INCA/KEN/IP/MADAN) UD\\'den Ã§Ä±karÄ±lamaz')\n", "        return ''.join(parts), warnings\n", "    \n", "    if verb_form == 'Part':\n", "        # SÄ±fat-fiil\n", "        parts.append('+PART')\n", "    \n", "    if verb_form == 'Vnoun':\n", "        # Ä°sim-fiil\n", "        parts.append('+VNOUN')\n", "    \n", "    # Zaman + GÃ¶rÃ¼nÃ¼ÅŸ (Tense + Aspect)\n", "    tense = feats.get('Tense', '')\n", "    aspect = feats.get('Aspect', '')\n", "    evident = feats.get('Evident', '')\n", "    \n", "    # Duyulan geÃ§miÅŸ (-mIÅŸ)\n", "    if evident == 'Nfh':\n", "        parts.append('+INFER')\n", "    elif aspect == 'Prog':\n", "        parts.append('+PRES.CONT')\n", "    elif aspect == 'Hab':\n", "        parts.append('+AOR')\n", "    elif tense == 'Past':\n", "        parts.append('+PAST')\n", "    elif tense == 'Fut':\n", "        parts.append('+FUT')\n", "    elif tense == 'Pres' and aspect == 'Perf':\n", "        pass  # GeniÅŸ zaman / ÅŸimdiki baÄŸlamda zaman eki yok\n", "    \n", "    # Kip (Mood) - Pot zaten yukarÄ±da iÅŸlendi\n", "    if mood and mood not in ('Ind', 'Pot'):\n", "        mood_tag = MOOD_MAP.get(mood, '')\n", "        if mood_tag:\n", "            parts.append(mood_tag)\n", "    \n", "    # KiÅŸi + SayÄ±\n", "    number = feats.get('Number', '')\n", "    person = feats.get('Person', '')\n", "    if number and person:\n", "        pn_tag = PERSON_NUMBER_MAP.get((number, person), '')\n", "        if pn_tag:\n", "            parts.append(pn_tag)\n", "    \n", "    # Ä°yelik (possessor - fiil sÄ±fat-fiillerinde)\n", "    npsor = feats.get('Number[psor]', '')\n", "    ppsor = feats.get('Person[psor]', '')\n", "    if npsor and ppsor:\n", "        poss_tag = POSS_MAP.get((npsor, ppsor), '')\n", "        if poss_tag:\n", "            parts.append(poss_tag)\n", "    \n", "    # Durum (fiil-isimleÅŸmelerde case olabilir)\n", "    case = feats.get('Case', '')\n", "    if case and case != 'Nom':\n", "        case_tag = CASE_MAP.get(case, '')\n", "        if case_tag:\n", "            parts.append(case_tag)\n", "    \n", "    # Polite bilgi kaybÄ±\n", "    if 'Polite' in feats:\n", "        warnings.append(f'Polite={feats[\"Polite\"]} FST\\'de karÅŸÄ±lÄ±ÄŸÄ± yok')\n", "    \n", "    return ''.join(parts), warnings\n", "\n", "\n", "def _map_nominal_to_fst(lemma, fst_pos, feats, warnings):\n", "    \"\"\"Ä°sim/SÄ±fat/Zamir morfolojisini FST formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\"\"\"\n", "    parts = [f'{lemma}+{fst_pos}']\n", "    \n", "    # Ã‡oÄŸul\n", "    number = feats.get('Number', 'Sing')\n", "    if number == 'Plur':\n", "        parts.append('+PL')\n", "    \n", "    # Ä°yelik (Possessive)\n", "    npsor = feats.get('Number[psor]', '')\n", "    ppsor = feats.get('Person[psor]', '')\n", "    if npsor and ppsor:\n", "        poss_tag = POSS_MAP.get((npsor, ppsor), '')\n", "        if poss_tag:\n", "            parts.append(poss_tag)\n", "    \n", "    # Durum (Case)\n", "    case = feats.get('Case', '')\n", "    if case:\n", "        case_tag = CASE_MAP.get(case, '')\n", "        if case_tag:\n", "            parts.append(case_tag)\n", "    \n", "    # PronType bilgi kaybÄ±\n", "    if 'PronType' in feats:\n", "        warnings.append(f'PronType={feats[\"PronType\"]} FST\\'de yok')\n", "    \n", "    return ''.join(parts), warnings\n", "\n", "\n", "def _map_aux_to_fst(lemma, feats, warnings):\n", "    \"\"\"AUX (yardÄ±mcÄ± fiil/copula) eÅŸlemesi.\"\"\"\n", "    # TÃ¼rkÃ§e'de AUX genelde copula \"i-\" veya \"deÄŸil\"\n", "    parts = [f'{lemma}+AUX']\n", "    \n", "    tense = feats.get('Tense', '')\n", "    mood = feats.get('Mood', '')\n", "    evident = feats.get('Evident', '')\n", "    polarity = feats.get('Polarity', '')\n", "    \n", "    if polarity == 'Neg':\n", "        parts.append('+NEG')\n", "    \n", "    if evident == 'Nfh':\n", "        parts.append('+COP.EVID')\n", "    elif tense == 'Past':\n", "        parts.append('+COP.PAST')\n", "    elif mood == 'Gen':\n", "        parts.append('+COP.PRES')\n", "    elif mood == 'Cnd':\n", "        parts.append('+COP.COND')\n", "    else:\n", "        parts.append('+COP')\n", "    \n", "    # KiÅŸi\n", "    number = feats.get('Number', '')\n", "    person = feats.get('Person', '')\n", "    if number and person:\n", "        pn_tag = PERSON_NUMBER_MAP.get((number, person), '')\n", "        if pn_tag:\n", "            parts.append(pn_tag)\n", "    \n", "    return ''.join(parts), warnings\n", "\n", "\n", "print('âœ… map_ud_to_fst() fonksiyonu hazÄ±r.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ğŸ§ª SimÃ¼lasyon: 10 UD Token â†’ FST Ã‡Ä±ktÄ±sÄ±\n", "\n", "AÅŸaÄŸÄ±da UD Turkish-IMST'den seÃ§ilmiÅŸ 10 token Ã¼zerinde dÃ¶nÃ¼ÅŸÃ¼m simÃ¼lasyonu yapÄ±lmaktadÄ±r.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 3: 10 Ã–RNEK UD TOKEN SÄ°MÃœLASYONU\n", "# ============================================================\n", "\n", "import pandas as pd\n", "\n", "# UD Turkish-IMST'den gerÃ§ek Ã¶rnekler\n", "test_tokens = [\n", "    # (FORM, LEMMA, UPOS, FEATS_str)\n", "    ('seviyorum', 'sev', 'VERB',\n", "     'Aspect=Prog|Mood=Ind|Number=Sing|Person=1|Polarity=Pos|Polite=Infm|Tense=Pres'),\n", "    ('yaÅŸlanmayacaÄŸÄ±z', 'yaÅŸlan', 'VERB',\n", "     'Aspect=Perf|Mood=Ind|Number=Plur|Person=1|Polarity=Neg|Tense=Fut'),\n", "    ('Seni', 'sen', 'PRON',\n", "     'Case=Acc|Number=Sing|Person=2|PronType=Prs'),\n", "    ('limandan', 'liman', 'NOUN',\n", "     'Case=Abl|Number=Sing|Person=3'),\n", "    ('AÅŸkÄ±mÄ±z', 'aÅŸk', 'NOUN',\n", "     'Case=Nom|Number=Sing|Number[psor]=Plur|Person=3|Person[psor]=1'),\n", "    ('eskimeyecek', 'eski', 'VERB',\n", "     'Aspect=Perf|Mood=Ind|Number=Sing|Person=3|Polarity=Neg|Tense=Fut'),\n", "    ('demiÅŸ', 'de', 'VERB',\n", "     'Aspect=Perf|Evident=Nfh|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Past'),\n", "    ('Ã¶ÄŸrencisiyle', 'Ã¶ÄŸrenci', 'NOUN',\n", "     'Case=Ins|Number=Sing|Number[psor]=Sing|Person=3|Person[psor]=3'),\n", "    ('Ã§Ä±kÄ±nca', 'Ã§Ä±k', 'VERB',\n", "     'Aspect=Perf|Mood=Ind|Polarity=Pos|Tense=Pres|VerbForm=Conv'),\n", "    ('bakÄ±n', 'bak', 'VERB',\n", "     'Aspect=Perf|Mood=Imp|Number=Plur|Person=2|Polarity=Pos|Tense=Pres'),\n", "]\n", "\n", "print('ğŸ§ª UD â†’ FST SÄ°MÃœLASYON SONUÃ‡LARI')\n", "print('=' * 90)\n", "print(f'{\"FORM\":<20} {\"LEMMA\":<12} {\"UPOS\":<8} {\"FST Ã‡Ä±ktÄ±sÄ±\":<35} {\"UyarÄ±lar\"}')\n", "print('-' * 90)\n", "\n", "results = []\n", "for form, lemma, upos, feats_str in test_tokens:\n", "    feats_dict = parse_ud_feats(feats_str)\n", "    fst_output, warns = map_ud_to_fst(lemma, upos, feats_dict)\n", "    warn_str = '; '.join(warns) if warns else '-'\n", "    print(f'{form:<20} {lemma:<12} {upos:<8} {fst_output:<35} {warn_str}')\n", "    results.append({\n", "        'Form': form, 'Lemma': lemma, 'UPOS': upos,\n", "        'FEATS': feats_str, 'FST_Output': fst_output, 'Warnings': warn_str\n", "    })\n", "\n", "print('\\nâœ… SimÃ¼lasyon tamamlandÄ±.')\n", "\n", "# DataFrame olarak da gÃ¶ster\n", "df_sim = pd.DataFrame(results)\n", "df_sim\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 4: CoNLL-U DOSYASINI OKUYUP TOPLU FST DÃ–NÃœÅÃœMÃœ\n", "# ============================================================\n", "\n", "import os\n", "\n", "def parse_conllu(filepath):\n", "    \"\"\"CoNLL-U dosyasÄ±nÄ± parse eder, token listesi dÃ¶ndÃ¼rÃ¼r.\"\"\"\n", "    tokens = []\n", "    with open(filepath, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            line = line.strip()\n", "            if not line or line.startswith('#'):\n", "                continue\n", "            parts = line.split('\\t')\n", "            if len(parts) < 10:\n", "                continue\n", "            # Multi-word token satÄ±rlarÄ±nÄ± atla (1-2 gibi)\n", "            if '-' in parts[0] or '.' in parts[0]:\n", "                continue\n", "            tokens.append({\n", "                'id': parts[0],\n", "                'form': parts[1],\n", "                'lemma': parts[2],\n", "                'upos': parts[3],\n", "                'xpos': parts[4],\n", "                'feats': parts[5],\n", "            })\n", "    return tokens\n", "\n", "\n", "def batch_ud_to_fst(tokens):\n", "    \"\"\"Token listesini toplu olarak FST formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\"\"\"\n", "    results = []\n", "    warn_counts = {}\n", "    \n", "    for tok in tokens:\n", "        feats_dict = parse_ud_feats(tok['feats'])\n", "        fst_out, warns = map_ud_to_fst(\n", "            tok['lemma'], tok['upos'], feats_dict, tok.get('xpos')\n", "        )\n", "        results.append({\n", "            'form': tok['form'],\n", "            'lemma': tok['lemma'],\n", "            'upos': tok['upos'],\n", "            'feats': tok['feats'],\n", "            'fst_analysis': fst_out,\n", "            'warnings': warns,\n", "        })\n", "        for w in warns:\n", "            warn_counts[w] = warn_counts.get(w, 0) + 1\n", "    \n", "    return results, warn_counts\n", "\n", "\n", "# â”€â”€ Dosya yolunu ayarla â”€â”€\n", "# Yerel ortam iÃ§in:\n", "CONLLU_PATH = os.path.join('..', '..', 'tr_imst-ud-test.conllu')\n", "# Colab iÃ§in alternatif:\n", "# CONLLU_PATH = '/content/drive/MyDrive/FSTurk/tr_imst-ud-test.conllu'\n", "\n", "if os.path.exists(CONLLU_PATH):\n", "    tokens = parse_conllu(CONLLU_PATH)\n", "    print(f'ğŸ“¦ {len(tokens)} token okundu.')\n", "    \n", "    fst_results, warn_summary = batch_ud_to_fst(tokens)\n", "    \n", "    # Ä°statistikler\n", "    print(f'âœ… {len(fst_results)} token FST formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.')\n", "    print(f'\\nâš ï¸ UyarÄ± Ã–zeti ({len(warn_summary)} farklÄ± uyarÄ± tipi):')\n", "    for w, count in sorted(warn_summary.items(), key=lambda x:-x[1])[:10]:\n", "        print(f'   {count:>5}x | {w}')\n", "    \n", "    # Ä°lk 20 Ã¶rneÄŸi gÃ¶ster\n", "    print(f'\\nğŸ“‹ Ä°lk 20 DÃ¶nÃ¼ÅŸÃ¼m Ã–rneÄŸi:')\n", "    print(f'{\"FORM\":<20} {\"UPOS\":<8} {\"FST Analizi\"}')\n", "    print('-' * 60)\n", "    for r in fst_results[:20]:\n", "        print(f'{r[\"form\"]:<20} {r[\"upos\"]:<8} {r[\"fst_analysis\"]}')\n", "else:\n", "    print(f'âŒ CoNLL-U dosyasÄ± bulunamadÄ±: {CONLLU_PATH}')\n", "    print('   LÃ¼tfen dosya yolunu kontrol edin.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 5: FST ANALÄ°ZLERÄ°NÄ° DIÅA AKTARMA\n", "# ============================================================\n", "\n", "import json\n", "\n", "def export_fst_analyses(results, output_path):\n", "    \"\"\"FST analizlerini JSON formatÄ±nda kaydeder.\"\"\"\n", "    export_data = []\n", "    for r in results:\n", "        export_data.append({\n", "            'form': r['form'],\n", "            'lemma': r['lemma'],\n", "            'upos': r['upos'],\n", "            'ud_feats': r['feats'],\n", "            'fst_analysis': r['fst_analysis'],\n", "            'warnings': r['warnings'],\n", "        })\n", "    \n", "    with open(output_path, 'w', encoding='utf-8') as f:\n", "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n", "    \n", "    print(f'ğŸ’¾ {len(export_data)} analiz kaydedildi: {output_path}')\n", "\n", "\n", "def export_fst_plain(results, output_path):\n", "    \"\"\"FST analizlerini dÃ¼z metin formatÄ±nda kaydeder (FST test giriÅŸi iÃ§in).\"\"\"\n", "    with open(output_path, 'w', encoding='utf-8') as f:\n", "        for r in results:\n", "            f.write(f'{r[\"form\"]}\\t{r[\"fst_analysis\"]}\\n')\n", "    \n", "    print(f'ğŸ“ DÃ¼z metin Ã§Ä±ktÄ±sÄ±: {output_path}')\n", "\n", "\n", "# Kaydet\n", "if 'fst_results' in dir() and fst_results:\n", "    # JSON formatÄ±\n", "    export_fst_analyses(fst_results, 'ud_to_fst_mapped.json')\n", "    # DÃ¼z metin\n", "    export_fst_plain(fst_results, 'ud_to_fst_mapped.txt')\n", "    print('\\nâœ… TÃ¼m Ã§Ä±ktÄ±lar hazÄ±r!')\n", "else:\n", "    print('âš ï¸ Ã–nce CoNLL-U dosyasÄ±nÄ± yÃ¼kleyin (BÃ¶lÃ¼m 4).')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BÃ–LÃœM 6: DOÄRULAMA ve Ä°STATÄ°STÄ°KLER\n", "# ============================================================\n", "\n", "if 'fst_results' in dir() and fst_results:\n", "    from collections import Counter\n", "    \n", "    # POS daÄŸÄ±lÄ±mÄ±\n", "    pos_counter = Counter(r['upos'] for r in fst_results)\n", "    print('ğŸ“Š UPOS DaÄŸÄ±lÄ±mÄ±:')\n", "    for pos, count in pos_counter.most_common():\n", "        fst_eq = UPOS_TO_FST_POS.get(pos, '?')\n", "        print(f'   {pos:<8} â†’ {fst_eq:<8} : {count:>5} token')\n", "    \n", "    # UyarÄ±sÄ±z token oranÄ±\n", "    clean = sum(1 for r in fst_results if not r['warnings'])\n", "    total = len(fst_results)\n", "    print(f'\\nğŸ¯ Temiz DÃ¶nÃ¼ÅŸÃ¼m OranÄ±: {clean}/{total} ({100*clean/total:.1f}%)')\n", "    \n", "    # Fiil kipi daÄŸÄ±lÄ±mÄ±\n", "    verb_moods = []\n", "    for r in fst_results:\n", "        if r['upos'] == 'VERB':\n", "            fd = parse_ud_feats(r['feats'])\n", "            m = fd.get('Mood', 'Ind')\n", "            verb_moods.append(m)\n", "    mood_counter = Counter(verb_moods)\n", "    print(f'\\nğŸ“Š Fiil Kipi DaÄŸÄ±lÄ±mÄ±:')\n", "    for mood, count in mood_counter.most_common():\n", "        fst_eq = MOOD_MAP.get(mood, '?')\n", "        print(f'   {mood:<10} â†’ {fst_eq:<12} : {count:>5} fiil')\n", "    \n", "    print('\\nâœ… DoÄŸrulama tamamlandÄ±.')\n", "else:\n", "    print('âš ï¸ Ã–nce CoNLL-U dosyasÄ±nÄ± yÃ¼kleyin (BÃ¶lÃ¼m 4).')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# PATCH A: Tense x Aspect DETERMINISTIK MATRIS\n", "# ============================================================\n", "# Risk: UD'de Tense ve Aspect ayri ozellikler.\n", "#        FST'te birlestik tag (PRES.CONT, AOR, PAST vs.)\n", "#        Her kombinasyon icin tek bir FST tag uretilmeli.\n", "\n", "# Tam deterministik matris: (Tense, Aspect, Evident) -> FST tag\n", "TENSE_ASPECT_MATRIX = {\n", "    # (Tense, Aspect, Evident) -> FST tense/aspect tag\n", "    \n", "    # --- PRESENT ---\n", "    ('Pres', 'Prog',  ''):    '+PRES.CONT',   # -Iyor (simdi devam)\n", "    ('Pres', 'Hab',   ''):    '+AOR',          # -(I)r (genis zaman)\n", "    ('Pres', 'Perf',  ''):    '',              # isaretsiz (isim cumlesi vb.)\n", "    ('Pres', '',      ''):    '',              # bare present\n", "    \n", "    # --- PAST ---\n", "    ('Past', 'Perf',  ''):    '+PAST',         # -DI (gorulmus gecmis)\n", "    ('Past', 'Perf',  'Nfh'): '+INFER',        # -mIs (duyulan gecmis)\n", "    ('Past', 'Prog',  ''):    '+PAST.CONT',    # -Iyordu (gecmis devam)\n", "    ('Past', 'Hab',   ''):    '+AOR+PAST',     # -Irdi (genis zaman + gecmis)\n", "    ('Past', '',      ''):    '+PAST',         # fallback past\n", "    ('Past', '',      'Nfh'): '+INFER',        # fallback infer\n", "    \n", "    # --- FUTURE ---\n", "    ('Fut',  'Perf',  ''):    '+FUT',          # -(y)AcAk\n", "    ('Fut',  'Prog',  ''):    '+FUT',          # nadir, fallback FUT\n", "    ('Fut',  '',      ''):    '+FUT',          # fallback future\n", "    \n", "    # --- EVIDENTIAL (no explicit tense) ---\n", "    ('',     'Perf',  'Nfh'): '+INFER',        # -mIs (tense bos)\n", "    ('',     '',      'Nfh'): '+INFER',        # fallback infer\n", "    \n", "    # --- EMPTY (tense/aspect bos) ---\n", "    ('',     'Prog',  ''):    '+PRES.CONT',    # -Iyor (tense bos ama prog)\n", "    ('',     'Hab',   ''):    '+AOR',          # -(I)r (tense bos ama hab)\n", "    ('',     'Perf',  ''):    '',              # bare perf\n", "    ('',     '',      ''):    '',              # tamamen bos\n", "}\n", "\n", "\n", "def resolve_tense_aspect(feats):\n", "    \"\"\"\n", "    Tense x Aspect x Evident uclusunden deterministik FST tag uretir.\n", "    \n", "    Returns:\n", "        str: FST tense/aspect tag(lar)i\n", "        str: Uyari mesaji (bos string = uyari yok)\n", "    \"\"\"\n", "    tense = feats.get('Tense', '')\n", "    aspect = feats.get('Aspect', '')\n", "    evident = feats.get('Evident', '')\n", "    \n", "    key = (tense, aspect, evident)\n", "    \n", "    if key in TENSE_ASPECT_MATRIX:\n", "        return TENSE_ASPECT_MATRIX[key], ''\n", "    \n", "    # Fallback: bilinen parcalari dene\n", "    # Evident > Aspect > Tense oncelik sirasi\n", "    if evident == 'Nfh':\n", "        return '+INFER', f'Fallback INFER: {key}'\n", "    if aspect == 'Prog':\n", "        return '+PRES.CONT', f'Fallback PROG: {key}'\n", "    if aspect == 'Hab':\n", "        return '+AOR', f'Fallback HAB: {key}'\n", "    if tense == 'Past':\n", "        return '+PAST', f'Fallback PAST: {key}'\n", "    if tense == 'Fut':\n", "        return '+FUT', f'Fallback FUT: {key}'\n", "    \n", "    return '', f'Bilinmeyen Tense/Aspect/Evident kombinasyonu: {key}'\n", "\n", "\n", "# Test: Tum matris entries\n", "print('Tense x Aspect DETERMINISTIC MATRIS TESTI')\n", "print('=' * 70)\n", "test_combos = [\n", "    {'Tense': 'Pres', 'Aspect': 'Prog'},                    # -Iyor\n", "    {'Tense': 'Past', 'Aspect': 'Perf'},                    # -DI\n", "    {'Tense': 'Past', 'Aspect': 'Perf', 'Evident': 'Nfh'},  # -mIs\n", "    {'Tense': 'Past', 'Aspect': 'Prog'},                    # -Iyordu\n", "    {'Tense': 'Past', 'Aspect': 'Hab'},                     # -Irdi\n", "    {'Tense': 'Fut',  'Aspect': 'Perf'},                    # -(y)AcAk\n", "    {'Tense': 'Pres', 'Aspect': 'Hab'},                     # -(I)r\n", "    {'Tense': 'Pres', 'Aspect': 'Perf'},                    # bare\n", "]\n", "\n", "for combo in test_combos:\n", "    tag, warn = resolve_tense_aspect(combo)\n", "    t = combo.get('Tense','-')\n", "    a = combo.get('Aspect','-')\n", "    e = combo.get('Evident','-')\n", "    status = 'OK' if not warn else f'WARN: {warn}'\n", "    print(f'  T={t:<5} A={a:<5} E={e:<4} -> {tag or \"(bos)\":<15} {status}')\n", "\n", "print('\\nDeterministik mi? EVET - her (T,A,E) ikilisi tek sonuc verir.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# PATCH B: Person DEFAULT + GUNCELLENMIS map_ud_to_fst\n", "# ============================================================\n", "# Risk: UD'de bazen Person yok (implicit 3SG).\n", "#        FST'de +3SG zorunlu mu?\n", "#        Karar: FST motorunda +3SG bazen bos, bazen explicit.\n", "#        Tutarlilik icin: Person yoksa VE Number yoksa -> +3SG default\n", "\n", "def resolve_person_number(feats, is_verb=False):\n", "    \"\"\"\n", "    Person x Number eslesmesi, eksik degerler icin default kurallar.\n", "    \n", "    Kurallar:\n", "    - Person + Number varsa: normal esleme\n", "    - Person var, Number yok: Sing varsayilir\n", "    - Number var, Person yok: 3 varsayilir\n", "    - Ikisi de yok + fiilse: +3SG (Turkce default)\n", "    - Ikisi de yok + isimse: bos (isaretsiz)\n", "    \"\"\"\n", "    number = feats.get('Number', '')\n", "    person = feats.get('Person', '')\n", "    \n", "    # Ikisi de var -> normal\n", "    if number and person:\n", "        return PERSON_NUMBER_MAP.get((number, person), ''), ''\n", "    \n", "    # Person var, Number yok -> Sing varsayilir\n", "    if person and not number:\n", "        tag = PERSON_NUMBER_MAP.get(('Sing', person), '')\n", "        return tag, f'Number eksik, Sing varsayildi (Person={person})'\n", "    \n", "    # Number var, Person yok -> 3 varsayilir\n", "    if number and not person:\n", "        tag = PERSON_NUMBER_MAP.get((number, '3'), '')\n", "        return tag, f'Person eksik, 3 varsayildi (Number={number})'\n", "    \n", "    # Ikisi de yok\n", "    if is_verb:\n", "        return '+3SG', 'Person+Number eksik, fiil icin +3SG varsayildi'\n", "    \n", "    return '', ''  # Isimler icin isaretsiz\n", "\n", "\n", "# â”€â”€ GUNCELLENMIS FIIL ESLEME FONKSIYONU â”€â”€\n", "def _map_verb_to_fst_v2(lemma, feats, warnings):\n", "    # Fiil morfolojisi - v2: deterministik Tense*Aspect + Person default.\n", "    parts = [f'{lemma}+VERB']\n", "    \n", "    # 1. Cati (Voice)\n", "    voice = feats.get('Voice', '')\n", "    if voice and voice in VOICE_MAP:\n", "        parts.append(VOICE_MAP[voice])\n", "    \n", "    # 2. Yeterlilik (Mood=Pot -> +ABIL)\n", "    mood = feats.get('Mood', 'Ind')\n", "    if mood == 'Pot':\n", "        parts.append('+ABIL')\n", "    \n", "    # 3. Olumsuzluk\n", "    polarity = feats.get('Polarity', 'Pos')\n", "    neg_tag = POLARITY_MAP.get(polarity, '')\n", "    if neg_tag:\n", "        parts.append(neg_tag)\n", "    \n", "    # 4. VerbForm (Conv, Part, Vnoun)\n", "    verb_form = feats.get('VerbForm', '')\n", "    if verb_form == 'Conv':\n", "        parts.append('+CVB')\n", "        warnings.append('CVB alt tipi (INCA/KEN/IP/MADAN) UD\\'den cikarilmaz')\n", "        return ''.join(parts), warnings\n", "    if verb_form == 'Part':\n", "        parts.append('+PART')\n", "    if verb_form == 'Vnoun':\n", "        parts.append('+VNOUN')\n", "    \n", "    # 5. Tense x Aspect x Evident -> DETERMINISTIK MATRIS\n", "    ta_tag, ta_warn = resolve_tense_aspect(feats)\n", "    if ta_tag:\n", "        parts.append(ta_tag)\n", "    if ta_warn:\n", "        warnings.append(ta_warn)\n", "    \n", "    # 6. Kip (Mood) - Ind ve Pot zaten islendi\n", "    if mood and mood not in ('Ind', 'Pot'):\n", "        mood_tag = MOOD_MAP.get(mood, '')\n", "        if mood_tag:\n", "            parts.append(mood_tag)\n", "    \n", "    # 7. Kisi x Sayi -> DEFAULT KURALLI\n", "    pn_tag, pn_warn = resolve_person_number(feats, is_verb=True)\n", "    if pn_tag:\n", "        parts.append(pn_tag)\n", "    if pn_warn:\n", "        warnings.append(pn_warn)\n", "    \n", "    # 8. Iyelik (possessor - sifat-fiillerde)\n", "    npsor = feats.get('Number[psor]', '')\n", "    ppsor = feats.get('Person[psor]', '')\n", "    if npsor and ppsor:\n", "        poss_tag = POSS_MAP.get((npsor, ppsor), '')\n", "        if poss_tag:\n", "            parts.append(poss_tag)\n", "    \n", "    # 9. Durum (fiil-isimlesmelerinde case)\n", "    case = feats.get('Case', '')\n", "    if case and case != 'Nom':\n", "        case_tag = CASE_MAP.get(case, '')\n", "        if case_tag:\n", "            parts.append(case_tag)\n", "    \n", "    # 10. Polite bilgi kaybi\n", "    if 'Polite' in feats:\n", "        warnings.append(f'Polite={feats[\"Polite\"]} FST\\'de yok')\n", "    \n", "    return ''.join(parts), warnings\n", "\n", "\n", "# â”€â”€ ANA FONKSIYONU GUNCELLE â”€â”€\n", "def map_ud_to_fst_v2(lemma, upos, feats_dict, xpos=None):\n", "    \"\"\"map_ud_to_fst v2: deterministik T*A matrisi + Person default.\"\"\"\n", "    warnings = []\n", "    fst_pos = UPOS_TO_FST_POS.get(upos, 'UNK')\n", "    if fst_pos == 'UNK':\n", "        warnings.append(f'Bilinmeyen UPOS: {upos}')\n", "    \n", "    if upos == 'AUX':\n", "        return _map_aux_to_fst(lemma, feats_dict, warnings)\n", "    if upos == 'PUNCT':\n", "        punct_map = {'.': '+PUNCT.period', ',': '+PUNCT.comma',\n", "                     '?': '+PUNCT.question', '!': '+PUNCT.exclamation',\n", "                     ':': '+PUNCT.colon', ';': '+PUNCT.semicolon',\n", "                     '...': '+PUNCT.ellipsis', '-': '+PUNCT.dash'}\n", "        return f'{lemma}{punct_map.get(lemma, \"+PUNCT\")}', warnings\n", "    if fst_pos == 'VERB':\n", "        return _map_verb_to_fst_v2(lemma, feats_dict, warnings)\n", "    if fst_pos in ('NOUN', 'PROPN', 'ADJ', 'PRON', 'NUM', 'DET'):\n", "        return _map_nominal_to_fst(lemma, fst_pos, feats_dict, warnings)\n", "    return f'{lemma}+{fst_pos}', warnings\n", "\n", "\n", "# â”€â”€ V2 TEST â”€â”€\n", "print('\\nV2 TEST: Past+Prog ve Person-eksik durumlar')\n", "print('=' * 70)\n", "v2_tests = [\n", "    ('seviyordum', 'sev', 'VERB',\n", "     'Aspect=Prog|Mood=Ind|Number=Sing|Person=1|Polarity=Pos|Tense=Past'),\n", "    ('gelirdi', 'gel', 'VERB',\n", "     'Aspect=Hab|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Past'),\n", "    ('patlayan', 'patla', 'VERB',\n", "     'Aspect=Perf|Mood=Ind|Polarity=Pos|Tense=Pres|VerbForm=Part'),\n", "    ('cikinca', 'cik', 'VERB',\n", "     'Aspect=Perf|Mood=Ind|Polarity=Pos|Tense=Pres|VerbForm=Conv'),\n", "    ('yapmak', 'yap', 'VERB',\n", "     'Aspect=Perf|Case=Nom|Mood=Ind|Polarity=Pos|Tense=Pres|VerbForm=Vnoun'),\n", "]\n", "\n", "for form, lemma, upos, feats_str in v2_tests:\n", "    fd = parse_ud_feats(feats_str)\n", "    fst, warns = map_ud_to_fst_v2(lemma, upos, fd)\n", "    w = '; '.join(warns) if warns else '-'\n", "    print(f'  {form:<20} -> {fst:<30} | {w}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ğŸ”´ KRITIK: Derivational Boundary (Turetim Siniri) Kaybi\n", "\n", "### Problem\n", "UD Turkish derivasyonel morfolojiyi cogu zaman **acik olarak belirtmez**.\n", "\n", "| Form | UD Analizi | FST Beklenen | Sorun |\n", "|------|-----------|-------------|-------|\n", "| yuzlum | `yuz+NOUN+Poss1Sg` | `yuz+NOUN+DER.lu+POSS.1SG` | UD: DER.lu KAYIP |\n", "| kitaplik | `kitaplik+NOUN` | `kitap+NOUN+DER.lik` | UD: lemma zaten turetilmis |\n", "| isci | `isci+NOUN` | `is+NOUN+DER.ci` | UD: lemma turetilmis |\n", "| evsiz | `evsiz+ADJ` | `ev+NOUN+DER.siz` | UD: POS bile degisti |\n", "\n", "### Etki\n", "- **FST candidate ile UD gold birebir eslesMEYECEK** turetilmis kelimelerde\n", "- Bu, coverage oranini dusurur\n", "- Bigram model icin gold standard olusturmayi zorlastirir\n", "\n", "### Cozum Stratejisi\n", "1. **Lemma karsilastirmasi**: UD lemma vs FST lemma farki varsa â†’ turetim eki var \n", "2. **Suffix sezgisel tespiti**: UD lemma sonunda `-lik/-lik/-siz/-ci/-li` varsa DER tag ekle\n", "3. **Kabul**: Tam geri cevrilebilirlik turetim icin MUMKUN DEGIL, bu bilinen sinir\n", "4. **Metric**: Coverage olcumunde turetilmis kelimeleri ayri raporla\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 9: COVERAGE TESTI (Yapisal Analiz)\n", "# ============================================================\n", "# pynini gerektirmez - mapped JSON uzerinde calisir.\n", "# Gercek FST coverage icin Colab'da Bolum 7'yi kullanin.\n", "\n", "import json\n", "import os\n", "from collections import Counter, defaultdict\n", "\n", "# â”€â”€ Mapped veriyi yukle â”€â”€\n", "MAPPED_FILE = 'ud_to_fst_mapped.json'\n", "if not os.path.exists(MAPPED_FILE):\n", "    MAPPED_FILE = os.path.join(os.path.dirname(os.path.abspath('.')), MAPPED_FILE)\n", "\n", "with open(MAPPED_FILE, 'r', encoding='utf-8') as f:\n", "    mapped_data = json.load(f)\n", "\n", "print(f'Yuklenen token sayisi: {len(mapped_data)}')\n", "\n", "# â”€â”€ Coverage Metrikleri â”€â”€\n", "total = 0\n", "valid_fst = 0         # Gecerli FST format (lemma+POS+...)\n", "has_warnings = 0      # Uyari iceren\n", "punct_count = 0\n", "empty_feats = 0       # Bos features\n", "pos_coverage = Counter()    # UPOS bazinda coverage\n", "warning_types = Counter()   # Uyari tipleri\n", "fst_tag_depth = Counter()   # FST tag derinligi (+ sayisi)\n", "\n", "for tok in mapped_data:\n", "    fst = tok['fst_analysis']\n", "    warns = tok.get('warnings', [])\n", "    upos = tok.get('upos', '?')\n", "    \n", "    # PUNCT atla\n", "    if 'PUNCT' in fst:\n", "        punct_count += 1\n", "        continue\n", "    \n", "    total += 1\n", "    \n", "    # Gecerli FST kontrolu: en az lemma+POS icermeli\n", "    if '+' in fst:\n", "        valid_fst += 1\n", "        tag_count = fst.count('+')\n", "        fst_tag_depth[tag_count] += 1\n", "    \n", "    if warns:\n", "        has_warnings += 1\n", "        for w in warns:\n", "            warning_types[w] += 1\n", "    \n", "    if tok.get('ud_feats', '') in ('', '_'):\n", "        empty_feats += 1\n", "    \n", "    pos_coverage[upos] += 1\n", "\n", "# â”€â”€ RAPOR â”€â”€\n", "print('\\n' + '=' * 60)\n", "print('COVERAGE RAPORU')\n", "print('=' * 60)\n", "print(f'Toplam token (PUNCT haric): {total}')\n", "print(f'Noktalama:                  {punct_count}')\n", "print(f'Gecerli FST format:         {valid_fst} ({100*valid_fst/total:.1f}%)')\n", "print(f'Uyari iceren:               {has_warnings} ({100*has_warnings/total:.1f}%)')\n", "print(f'Temiz donusum:              {total-has_warnings} ({100*(total-has_warnings)/total:.1f}%)')\n", "print(f'Bos UD features:            {empty_feats}')\n", "\n", "print(f'\\nUPOS Bazinda Dagilim:')\n", "for upos, count in pos_coverage.most_common():\n", "    pct = 100 * count / total\n", "    bar = '#' * int(pct / 2)\n", "    print(f'  {upos:<8} {count:>5} ({pct:>5.1f}%) {bar}')\n", "\n", "print(f'\\nFST Tag Derinligi (+ sayisi):')\n", "for depth, count in sorted(fst_tag_depth.items()):\n", "    print(f'  {depth} tag: {count} token')\n", "\n", "if warning_types:\n", "    print(f'\\nEn Sik Uyarilar (ilk 10):')\n", "    for w, count in warning_types.most_common(10):\n", "        print(f'  {count:>5}x | {w}')\n", "\n", "# Sonuc degerlendirmesi\n", "coverage_pct = 100 * valid_fst / total\n", "clean_pct = 100 * (total - has_warnings) / total\n", "print(f'\\n{\"=\" * 60}')\n", "if coverage_pct > 95 and clean_pct > 70:\n", "    print(f'SONUC: Coverage IIII ({coverage_pct:.1f}%), temiz {clean_pct:.1f}%')\n", "    print(f'Bigram modele gecebilirsin!')\n", "elif coverage_pct > 90:\n", "    print(f'SONUC: Coverage ORTA ({coverage_pct:.1f}%), bazi iyilestirmeler gerekebilir')\n", "else:\n", "    print(f'SONUC: Coverage DUSUK ({coverage_pct:.1f}%), mapping gozden gecirilmeli')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "# Part C: Morfolojik Belirsizlik Giderme (Disambiguation)\n", "Interpolated Bigram HMM + Viterbi decoder.\n", "\n", "Model: **Interpolated Bigram HMM with Conditional Lexical Emission**\n", "\n", "```\n", "score = prev + trans(prev_s, curr_s) + beta * emit(surface, curr_s)\n", "trans = lam * P_bi(s|prev) + (1-lam) * P_uni(s)\n", "emit  = P(state | surface)\n", "```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Disambiguator State Abstraction\n", "\n", "FST tam analiz cok sparse:\n", "```\n", "git+VERB+NEG+FUT+1PL+DER:CAUS+DER:PASS  (unique string)\n", "sev+VERB+PRES.CONT+1SG                   (baska unique string)\n", "```\n", "\n", "Disambiguator state = **lemma-bagimsiz + DER-siz pattern**:\n", "```\n", "VERB+NEG+FUT+1PL    (ogrenilebilir pattern)\n", "VERB+PRES.CONT+1SG  (ogrenilebilir pattern)\n", "```\n", "\n", "Kural:\n", "- FST morphotactics **DEGISMEZ**\n", "- Derivational structure **SADELESSTIRILMEZ**\n", "- Sadece disambiguator icin **state abstraction** yapilir\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 10: STATE ABSTRACTION KATMANI\n", "# ============================================================\n", "# FST output -> Disambiguator State donusumu\n", "# Kural: Lemma cikar, DER.* cikar, core POS+FEATURE pattern tut\n", "\n", "import re\n", "\n", "# Cikarilacak tag pattern'lari (derivational + lemma)\n", "DER_PATTERN = re.compile(r'\\+DER\\.[a-z]+')  # +DER.lik, +DER.ci vb.\n", "\n", "def extract_disambiguator_state(fst_analysis):\n", "    # Gecerlilik kontrolu\n", "    if '+' not in fst_analysis:\n", "        return fst_analysis\n", "    \n", "    parts = fst_analysis.split('+')\n", "    \n", "    # parts[0] = lemma, parts[1] = POS, parts[2:] = features\n", "    # Lemma cikar\n", "    tags_only = parts[1:]  # POS + features\n", "    \n", "    # DER.* tag'larini cikar\n", "    filtered = [t for t in tags_only if not t.startswith('DER.')]\n", "    \n", "    # PUNCT ozel durum\n", "    if filtered and filtered[0].startswith('PUNCT'):\n", "        return 'PUNCT'\n", "    \n", "    return '+'.join(filtered) if filtered else 'UNK'\n", "\n", "\n", "# â”€â”€ Test â”€â”€\n", "test_cases = [\n", "    'sev+VERB+PRES.CONT+1SG',\n", "    'git+VERB+NEG+FUT+1PL',\n", "    'ev+NOUN+PL+ABL',\n", "    'liman+NOUN+ABL',\n", "    'kitap+NOUN+DER.lik',\n", "    'is+NOUN+DER.ci+PL+DAT',\n", "    'bak+VERB+ABIL+NEG+PAST+3SG',\n", "    'guzel+ADJ',\n", "    'sen+PRON+ACC',\n", "    'de+VERB+INFER+3SG',\n", "    '.+PUNCT.period',\n", "]\n", "\n", "print('STATE ABSTRACTION ORNEKLERI')\n", "print('=' * 65)\n", "print(f'{\"FST Tam Analiz\":<35} {\"Disambiguator State\"}')\n", "print('-' * 65)\n", "for fst in test_cases:\n", "    state = extract_disambiguator_state(fst)\n", "    print(f'{fst:<35} {state}')\n", "\n", "print(f'\\nDikkat: DER.* taglari state\\'den cikarildi.')\n", "print(f'FST output DEGISMEZ, sadece disambiguator bakar.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DUZELTME: Temiz Viterbi Recursion\n", "\n", "DoÄŸru formÃ¼l:\n", "```\n", "t=0:  score = trans(START, s)  + emit(w, s)\n", "t>0:  score = prev_score      + trans(prev_s, s) + emit(w, s)\n", "```\n", "BaÅŸka hiÃ§bir ÅŸey eklenmeyecek.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# DUZELTILMIS VITERBI: MINIMAL, MATEMATIKSEL DOGRU\n", "# ============================================================\n", "\n", "import math\n", "import json\n", "from collections import Counter, defaultdict\n", "\n", "class CleanHMMDisambiguator:\n", "    \n", "    def __init__(self, lam=0.7, smoothing=0.01):\n", "        self.lam = lam\n", "        self.smoothing = smoothing\n", "        self.bigram_counts = Counter()\n", "        self.context_counts = Counter()\n", "        self.unigram_counts = Counter()\n", "        self.total_tokens = 0\n", "        self.emission_counts = defaultdict(Counter)\n", "        self.surface_totals = Counter()\n", "        self.states = set()\n", "    \n", "    # â”€â”€ TRAINING â”€â”€\n", "    def train(self, mapped_data):\n", "        prev_state = 'START'\n", "        for tok in mapped_data:\n", "            fst = tok['fst_analysis']\n", "            surface = tok['form'].lower()\n", "            curr_state = extract_disambiguator_state(fst)\n", "            \n", "            # PUNCT: transition'a KATMA, cumle siniri olarak isle\n", "            if curr_state == 'PUNCT':\n", "                prev_state = 'START'\n", "                continue\n", "            \n", "            self.bigram_counts[(prev_state, curr_state)] += 1\n", "            self.context_counts[prev_state] += 1\n", "            self.unigram_counts[curr_state] += 1\n", "            self.total_tokens += 1\n", "            self.emission_counts[surface][curr_state] += 1\n", "            self.surface_totals[surface] += 1\n", "            self.states.add(curr_state)\n", "            prev_state = curr_state\n", "        \n", "        self.states.add('START')\n", "    \n", "    # â”€â”€ PROBABILITIES â”€â”€\n", "    def _p_uni(self, state):\n", "        V = len(self.states)\n", "        c = self.unigram_counts.get(state, 0)\n", "        return (c + self.smoothing) / (self.total_tokens + self.smoothing * V)\n", "    \n", "    def _p_bi(self, prev, curr):\n", "        V = len(self.states)\n", "        c = self.bigram_counts.get((prev, curr), 0)\n", "        ctx = self.context_counts.get(prev, 0)\n", "        return (c + self.smoothing) / (ctx + self.smoothing * V)\n", "    \n", "    def trans(self, prev, curr):\n", "        # Interpolated transition: lam * P_bi + (1-lam) * P_uni\n", "        p = self.lam * self._p_bi(prev, curr) + (1 - self.lam) * self._p_uni(curr)\n", "        return math.log(max(p, 1e-15))\n", "    \n", "    def emit(self, surface, state):\n", "        # P(state | surface)\n", "        surface = surface.lower()\n", "        if surface not in self.emission_counts:\n", "            return 0.0  # Gorulmemis surface -> uniform (log(1)=0)\n", "        V = len(self.states)\n", "        c = self.emission_counts[surface].get(state, 0)\n", "        t = self.surface_totals[surface]\n", "        return math.log(max((c + self.smoothing) / (t + self.smoothing * V), 1e-15))\n", "    \n", "    # â”€â”€ VITERBI: MINIMAL TEMIZ â”€â”€\n", "    def disambiguate(self, surfaces, candidate_lists):\n", "        # surfaces: ['cocuk', 'okula', 'gitti', '.']\n", "        # candidate_lists: [['cocuk+NOUN','cocuk+ADJ'], ['okul+NOUN+DAT'], ...]\n", "        #\n", "        # FORMUL:\n", "        #   t=0:  score[i] = trans(START, s_i)     + emit(w_0, s_i)\n", "        #   t>0:  score[i] = best[t-1][j]          + trans(s_j, s_i) + emit(w_t, s_i)\n", "        #\n", "        # BASKA HICBIR SEY EKLENMEYECEK.\n", "        \n", "        n = len(candidate_lists)\n", "        if n == 0:\n", "            return []\n", "        \n", "        best = [{} for _ in range(n)]   # best[t][i] = log_prob\n", "        back = [{} for _ in range(n)]   # back[t][i] = j (onceki index)\n", "        \n", "        # â”€â”€ t=0: INITIALIZATION â”€â”€\n", "        for i, fst in enumerate(candidate_lists[0]):\n", "            s = extract_disambiguator_state(fst)\n", "            # PUNCT tek candidate ise direkt sec\n", "            if s == 'PUNCT':\n", "                best[0][i] = 0.0\n", "            else:\n", "                best[0][i] = self.trans('START', s) + self.emit(surfaces[0], s)\n", "        \n", "        # â”€â”€ t>0: RECURSION â”€â”€\n", "        for t in range(1, n):\n", "            for i, curr_fst in enumerate(candidate_lists[t]):\n", "                curr_s = extract_disambiguator_state(curr_fst)\n", "                \n", "                # PUNCT: direkt sec, skor olarak onceki en iyiyi al\n", "                if curr_s == 'PUNCT':\n", "                    best_j = max(best[t-1], key=best[t-1].get) if best[t-1] else 0\n", "                    best[t][i] = best[t-1].get(best_j, 0.0)\n", "                    back[t][i] = best_j\n", "                    continue\n", "                \n", "                max_score = float('-inf')\n", "                best_j = 0\n", "                \n", "                for j, prev_fst in enumerate(candidate_lists[t-1]):\n", "                    prev_s = extract_disambiguator_state(prev_fst)\n", "                    \n", "                    # Onceki PUNCT ise -> prev_s = START\n", "                    if prev_s == 'PUNCT':\n", "                        prev_s = 'START'\n", "                    \n", "                    # === RECURSION: TAM BU, BASKA HICBIR SEY ===\n", "                    score = best[t-1][j] + self.trans(prev_s, curr_s) + self.emit(surfaces[t], curr_s)\n", "                    # ================================================\n", "                    \n", "                    if score > max_score:\n", "                        max_score = score\n", "                        best_j = j\n", "                \n", "                best[t][i] = max_score\n", "                back[t][i] = best_j\n", "        \n", "        # â”€â”€ BACKTRACK â”€â”€\n", "        if not best[n-1]:\n", "            return [c[0] for c in candidate_lists]\n", "        \n", "        idx = max(best[n-1], key=best[n-1].get)\n", "        path = []\n", "        for t in range(n-1, -1, -1):\n", "            path.append(candidate_lists[t][idx])\n", "            idx = back[t].get(idx, 0) if t > 0 else 0\n", "        \n", "        return list(reversed(path))\n", "    \n", "    # â”€â”€ SAVE / LOAD â”€â”€\n", "    def save(self, path):\n", "        data = {\n", "            'metadata': {'lambda': self.lam, 'smoothing': self.smoothing,\n", "                         'total_tokens': self.total_tokens,\n", "                         'states': len(self.states), 'bigrams': len(self.bigram_counts),\n", "                         'surfaces': len(self.emission_counts)},\n", "            'unigrams': dict(self.unigram_counts),\n", "            'bigrams': {f'{p}|||{c}': n for (p,c),n in self.bigram_counts.items()},\n", "            'emissions': {s: dict(d) for s,d in self.emission_counts.items()},\n", "            'states': sorted(list(self.states)),\n", "        }\n", "        with open(path, 'w', encoding='utf-8') as f:\n", "            json.dump(data, f, ensure_ascii=False, indent=2)\n", "        print(f'Model kaydedildi: {path}')\n", "    \n", "    @classmethod\n", "    def load(cls, path):\n", "        with open(path, 'r', encoding='utf-8') as f:\n", "            data = json.load(f)\n", "        m = cls(lam=data['metadata']['lambda'], smoothing=data['metadata']['smoothing'])\n", "        m.total_tokens = data['metadata']['total_tokens']\n", "        m.unigram_counts = Counter(data['unigrams'])\n", "        m.states = set(data['states'])\n", "        for key, cnt in data['bigrams'].items():\n", "            p, c = key.split('|||')\n", "            m.bigram_counts[(p, c)] = cnt\n", "            m.context_counts[p] += cnt\n", "        for surf, sd in data['emissions'].items():\n", "            for st, cnt in sd.items():\n", "                m.emission_counts[surf][st] = cnt\n", "                m.surface_totals[surf] += cnt\n", "        return m\n", "\n", "\n", "# â”€â”€ EGITIM â”€â”€\n", "clean_hmm = CleanHMMDisambiguator(lam=0.7, smoothing=0.01)\n", "clean_hmm.train(mapped_data)\n", "\n", "print(f'CleanHMMDisambiguator')\n", "print(f'  Tokens: {clean_hmm.total_tokens}')\n", "print(f'  States: {len(clean_hmm.states)}')\n", "print(f'  Bigrams: {len(clean_hmm.bigram_counts)}')\n", "print(f'  Surfaces: {len(clean_hmm.emission_counts)}')\n", "clean_hmm.save('clean_hmm_model.json')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# DUZELTILMIS VITERBI: DOGRULAMA TESTLERI\n", "# ============================================================\n", "\n", "import inspect\n", "\n", "print('RECURSION YAPISI KONTROLU')\n", "print('=' * 60)\n", "\n", "# Kaynak koddan recursion satirini bul\n", "src = inspect.getsource(clean_hmm.disambiguate)\n", "lines = src.split('\\n')\n", "\n", "# trans ve emit iceren satirlari say\n", "trans_lines = [l.strip() for l in lines if 'self.trans(' in l and not l.strip().startswith('#')]\n", "emit_lines = [l.strip() for l in lines if 'self.emit(' in l and not l.strip().startswith('#')]\n", "\n", "print(f'trans() cagri sayisi: {len(trans_lines)}')\n", "for i, l in enumerate(trans_lines):\n", "    print(f'  [{i+1}] {l}')\n", "\n", "print(f'\\nemit() cagri sayisi: {len(emit_lines)}')\n", "for i, l in enumerate(emit_lines):\n", "    print(f'  [{i+1}] {l}')\n", "\n", "# Beklenen: 2 trans (init + recursion), 2 emit (init + recursion)\n", "# Recursion icinde SADECE 1 trans + 1 emit olmali\n", "print(f'\\nBeklenen: 2 trans (t=0 ve t>0), 2 emit (t=0 ve t>0)')\n", "ok = len(trans_lines) == 2 and len(emit_lines) == 2\n", "print(f'SONUC: {\"PASS\" if ok else \"KONTROL ET\"}')\n", "\n", "# Ana recursion satirini goster\n", "print(f'\\n--- ANA RECURSION SATIRI ---')\n", "for l in lines:\n", "    if 'RECURSION: TAM BU' in l or ('best[t-1][j]' in l and 'trans' in l):\n", "        print(f'>>> {l.strip()}')\n", "\n", "# â”€â”€ DETERMINISTIK OYUNCAK TEST â”€â”€\n", "print(f'\\n\\nOYUNCAK VERI TESTI')\n", "print('=' * 60)\n", "\n", "toy = CleanHMMDisambiguator(lam=0.7, smoothing=0.001)\n", "toy.states = {'START', 'NOUN', 'VERB', 'ADJ'}\n", "toy.bigram_counts[('START', 'NOUN')] = 100\n", "toy.bigram_counts[('START', 'ADJ')] = 10\n", "toy.bigram_counts[('START', 'VERB')] = 1\n", "toy.bigram_counts[('NOUN', 'VERB')] = 80\n", "toy.bigram_counts[('NOUN', 'NOUN')] = 20\n", "toy.bigram_counts[('ADJ', 'NOUN')] = 90\n", "toy.bigram_counts[('ADJ', 'VERB')] = 10\n", "toy.context_counts = Counter({p: sum(c for (pp,_),c in toy.bigram_counts.items() if pp==p) for p in ['START','NOUN','ADJ']})\n", "toy.unigram_counts = Counter({'NOUN': 210, 'VERB': 91, 'ADJ': 10})\n", "toy.total_tokens = 311\n", "\n", "# Test 1: \"cocuk gitti\" -> NOUN VERB bekleniyor\n", "r1 = toy.disambiguate(\n", "    ['cocuk', 'gitti'],\n", "    [['cocuk+NOUN', 'cocuk+ADJ'], ['git+VERB', 'git+NOUN']]\n", ")\n", "s1 = [extract_disambiguator_state(x) for x in r1]\n", "print(f'  \"cocuk gitti\"  -> {s1}  beklenen: [NOUN, VERB]  {\"PASS\" if s1==[\"NOUN\",\"VERB\"] else \"FAIL\"}')\n", "\n", "# Test 2: \"guzel ev\" -> ADJ NOUN bekleniyor\n", "r2 = toy.disambiguate(\n", "    ['guzel', 'ev'],\n", "    [['guzel+ADJ', 'guzel+NOUN'], ['ev+NOUN', 'ev+VERB']]\n", ")\n", "s2 = [extract_disambiguator_state(x) for x in r2]\n", "print(f'  \"guzel ev\"     -> {s2}  beklenen: [ADJ, NOUN]   {\"PASS\" if s2==[\"ADJ\",\"NOUN\"] else \"KONTROL\"}')\n", "\n", "# Test 3: Tek candidate -> direkt sec\n", "r3 = toy.disambiguate(\n", "    ['okul'],\n", "    [['okul+NOUN']]\n", ")\n", "s3 = [extract_disambiguator_state(x) for x in r3]\n", "print(f'  \"okul\" (tek)   -> {s3}  beklenen: [NOUN]        {\"PASS\" if s3==[\"NOUN\"] else \"FAIL\"}')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 18: LAMBDA + BETA SWEEP\n", "# ============================================================\n", "\n", "import math\n", "import json\n", "from collections import Counter, defaultdict\n", "\n", "class TunableHMMDisambiguator:\n", "    # CleanHMM + beta (emission weight) destegi\n", "    \n", "    def __init__(self, lam=0.7, beta=1.0, smoothing=0.01):\n", "        self.lam = lam\n", "        self.beta = beta\n", "        self.smoothing = smoothing\n", "        self.bigram_counts = Counter()\n", "        self.context_counts = Counter()\n", "        self.unigram_counts = Counter()\n", "        self.total_tokens = 0\n", "        self.emission_counts = defaultdict(Counter)\n", "        self.surface_totals = Counter()\n", "        self.states = set()\n", "    \n", "    def train(self, mapped_data):\n", "        prev_state = 'START'\n", "        for tok in mapped_data:\n", "            fst = tok['fst_analysis']\n", "            surface = tok['form'].lower()\n", "            curr_state = extract_disambiguator_state(fst)\n", "            if curr_state == 'PUNCT':\n", "                prev_state = 'START'\n", "                continue\n", "            self.bigram_counts[(prev_state, curr_state)] += 1\n", "            self.context_counts[prev_state] += 1\n", "            self.unigram_counts[curr_state] += 1\n", "            self.total_tokens += 1\n", "            self.emission_counts[surface][curr_state] += 1\n", "            self.surface_totals[surface] += 1\n", "            self.states.add(curr_state)\n", "            prev_state = curr_state\n", "        self.states.add('START')\n", "    \n", "    def _p_uni(self, state):\n", "        V = len(self.states)\n", "        c = self.unigram_counts.get(state, 0)\n", "        return (c + self.smoothing) / (self.total_tokens + self.smoothing * V)\n", "    \n", "    def _p_bi(self, prev, curr):\n", "        V = len(self.states)\n", "        c = self.bigram_counts.get((prev, curr), 0)\n", "        ctx = self.context_counts.get(prev, 0)\n", "        return (c + self.smoothing) / (ctx + self.smoothing * V)\n", "    \n", "    def trans(self, prev, curr):\n", "        p = self.lam * self._p_bi(prev, curr) + (1 - self.lam) * self._p_uni(curr)\n", "        return math.log(max(p, 1e-15))\n", "    \n", "    def emit(self, surface, state):\n", "        surface = surface.lower()\n", "        if surface not in self.emission_counts:\n", "            return 0.0\n", "        V = len(self.states)\n", "        c = self.emission_counts[surface].get(state, 0)\n", "        t = self.surface_totals[surface]\n", "        return math.log(max((c + self.smoothing) / (t + self.smoothing * V), 1e-15))\n", "    \n", "    def disambiguate(self, surfaces, candidate_lists):\n", "        n = len(candidate_lists)\n", "        if n == 0:\n", "            return []\n", "        best = [{} for _ in range(n)]\n", "        back = [{} for _ in range(n)]\n", "        \n", "        # t=0\n", "        for i, fst in enumerate(candidate_lists[0]):\n", "            s = extract_disambiguator_state(fst)\n", "            if s == 'PUNCT':\n", "                best[0][i] = 0.0\n", "            else:\n", "                best[0][i] = self.trans('START', s) + self.beta * self.emit(surfaces[0], s)\n", "        \n", "        # t>0: score = prev + trans(prev_s, curr_s) + beta * emit(w, curr_s)\n", "        for t in range(1, n):\n", "            for i, curr_fst in enumerate(candidate_lists[t]):\n", "                curr_s = extract_disambiguator_state(curr_fst)\n", "                if curr_s == 'PUNCT':\n", "                    best_j = max(best[t-1], key=best[t-1].get) if best[t-1] else 0\n", "                    best[t][i] = best[t-1].get(best_j, 0.0)\n", "                    back[t][i] = best_j\n", "                    continue\n", "                max_score = float('-inf')\n", "                best_j = 0\n", "                for j, prev_fst in enumerate(candidate_lists[t-1]):\n", "                    prev_s = extract_disambiguator_state(prev_fst)\n", "                    if prev_s == 'PUNCT':\n", "                        prev_s = 'START'\n", "                    score = best[t-1][j] + self.trans(prev_s, curr_s) + self.beta * self.emit(surfaces[t], curr_s)\n", "                    if score > max_score:\n", "                        max_score = score\n", "                        best_j = j\n", "                best[t][i] = max_score\n", "                back[t][i] = best_j\n", "        \n", "        if not best[n-1]:\n", "            return [c[0] for c in candidate_lists]\n", "        idx = max(best[n-1], key=best[n-1].get)\n", "        path = []\n", "        for t in range(n-1, -1, -1):\n", "            path.append(candidate_lists[t][idx])\n", "            idx = back[t].get(idx, 0) if t > 0 else 0\n", "        return list(reversed(path))\n", "\n", "print('TunableHMMDisambiguator hazir.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DUZELTILMIS EVALUATION\n", "\n", "Onceki eval neden yanlis:\n", "- Distractor'lar random state sampling -> yapay kolay\n", "- Train = Eval -> memorization / leakage\n", "- %99.7 gercek disambiguation performansi degil\n", "\n", "Yeni eval:\n", "1. **Sentence-level train/test split** (80/20)\n", "2. **Surface-conditioned candidates**: Ayni surface form icin training'de gorulen TUM state'ler\n", "3. **Overfitting testi**: Gold'u random ile degistir, accuracy dusmeli\n", "4. **Gercek ambiguity**: `guzel` -> ADJ veya NOUN (training'den ogrenilmis)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 21: GERCEK EVALUATION SETUP\n", "# ============================================================\n", "\n", "import random\n", "import math\n", "from collections import defaultdict, Counter\n", "\n", "random.seed(42)\n", "\n", "# â”€â”€ 1. Cumle bazli train/test split â”€â”€\n", "\n", "def split_into_sentences(mapped_data):\n", "    sentences = []\n", "    curr = []\n", "    for tok in mapped_data:\n", "        if 'PUNCT' in tok['fst_analysis']:\n", "            if curr:\n", "                sentences.append(curr)\n", "                curr = []\n", "        else:\n", "            curr.append(tok)\n", "    if curr:\n", "        sentences.append(curr)\n", "    return sentences\n", "\n", "sentences = split_into_sentences(mapped_data)\n", "random.shuffle(sentences)\n", "\n", "split_idx = int(len(sentences) * 0.8)\n", "train_sents = sentences[:split_idx]\n", "test_sents = sentences[split_idx:]\n", "\n", "train_data = [tok for sent in train_sents for tok in sent]\n", "test_data = [tok for sent in test_sents for tok in sent]\n", "\n", "print(f'Cumle bazli split:')\n", "print(f'  Train: {len(train_sents)} cumle, {len(train_data)} token')\n", "print(f'  Test:  {len(test_sents)} cumle, {len(test_data)} token')\n", "\n", "# Surface overlap kontrolu\n", "train_surfaces = set(t['form'].lower() for t in train_data)\n", "test_surfaces = set(t['form'].lower() for t in test_data)\n", "overlap = train_surfaces & test_surfaces\n", "test_oov = test_surfaces - train_surfaces\n", "print(f'  Surface overlap: {len(overlap)} / {len(test_surfaces)} ({100*len(overlap)/max(len(test_surfaces),1):.0f}%)')\n", "print(f'  Test OOV: {len(test_oov)} surface ({100*len(test_oov)/max(len(test_surfaces),1):.0f}%)')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 22: SURFACE-CONDITIONED CANDIDATE URETIMI\n", "# ============================================================\n", "# Gercek ambiguity: Bir surface form training'de hangi state'lerle\n", "# gorulmusse, o state'ler candidate olur.\n", "\n", "# Training'den surface -> state haritasi olustur\n", "surface_to_states = defaultdict(set)   # surface -> {state1, state2, ...}\n", "surface_to_fst = defaultdict(dict)     # surface -> {state: fst_analysis}\n", "\n", "for tok in train_data:\n", "    surface = tok['form'].lower()\n", "    fst = tok['fst_analysis']\n", "    state = extract_disambiguator_state(fst)\n", "    surface_to_states[surface].add(state)\n", "    surface_to_fst[surface][state] = fst\n", "\n", "# Ambiguity istatistikleri\n", "ambiguity_dist = Counter()\n", "for surf, states in surface_to_states.items():\n", "    ambiguity_dist[len(states)] += 1\n", "\n", "print('SURFACE AMBIGUITY (Training Verisi)')\n", "print('=' * 50)\n", "total_surf = sum(ambiguity_dist.values())\n", "for n_states, count in sorted(ambiguity_dist.items()):\n", "    pct = 100 * count / total_surf\n", "    bar = '#' * int(pct / 2)\n", "    print(f'  {n_states} state: {count:>5} surface ({pct:>5.1f}%) {bar}')\n", "\n", "# Ambiguous surface ornekleri\n", "print(f'\\nAmbiguous Surface Ornekleri (2+ state):')\n", "count = 0\n", "for surf, states in sorted(surface_to_states.items()):\n", "    if len(states) >= 2:\n", "        print(f'  {surf:<20} -> {sorted(states)}')\n", "        count += 1\n", "        if count >= 15:\n", "            break\n", "\n", "# â”€â”€ Test set icin candidate listesi olustur â”€â”€\n", "def build_real_eval(test_sents, surface_to_states, surface_to_fst):\n", "    eval_set = []  # list of (surfaces, candidate_lists, gold_indices)\n", "    \n", "    stats = {'total': 0, 'ambiguous': 0, 'unambiguous': 0, 'unseen': 0}\n", "    \n", "    for sent in test_sents:\n", "        surfaces = []\n", "        candidate_lists = []\n", "        gold_indices = []\n", "        \n", "        for tok in sent:\n", "            surface = tok['form'].lower()\n", "            gold_fst = tok['fst_analysis']\n", "            gold_state = extract_disambiguator_state(gold_fst)\n", "            \n", "            stats['total'] += 1\n", "            \n", "            if surface in surface_to_states:\n", "                known_states = surface_to_states[surface]\n", "                \n", "                # Candidate listesi: training'de gorulen tum state'ler\n", "                candidates = []\n", "                gold_idx = -1\n", "                \n", "                for state in sorted(known_states):\n", "                    # Lemma'yi koruyarak candidate olustur\n", "                    lemma = gold_fst.split('+')[0] if '+' in gold_fst else gold_fst\n", "                    if state in surface_to_fst[surface]:\n", "                        cand_fst = surface_to_fst[surface][state]\n", "                    else:\n", "                        cand_fst = lemma + '+' + state\n", "                    candidates.append(cand_fst)\n", "                    if state == gold_state:\n", "                        gold_idx = len(candidates) - 1\n", "                \n", "                # Gold state training'de gorulmemis olabilir\n", "                if gold_idx == -1:\n", "                    candidates.append(gold_fst)\n", "                    gold_idx = len(candidates) - 1\n", "                \n", "                if len(candidates) > 1:\n", "                    stats['ambiguous'] += 1\n", "                else:\n", "                    stats['unambiguous'] += 1\n", "            else:\n", "                # OOV: sadece gold\n", "                candidates = [gold_fst]\n", "                gold_idx = 0\n", "                stats['unseen'] += 1\n", "            \n", "            surfaces.append(surface)\n", "            candidate_lists.append(candidates)\n", "            gold_indices.append(gold_idx)\n", "        \n", "        if surfaces:\n", "            eval_set.append((surfaces, candidate_lists, gold_indices))\n", "    \n", "    return eval_set, stats\n", "\n", "real_eval, eval_stats = build_real_eval(test_sents, surface_to_states, surface_to_fst)\n", "\n", "print(f'\\nTEST SET ISTATISTIKLERI')\n", "print(f'  Toplam token: {eval_stats[\"total\"]}')\n", "print(f'  Ambiguous:    {eval_stats[\"ambiguous\"]} ({100*eval_stats[\"ambiguous\"]/max(eval_stats[\"total\"],1):.1f}%)')\n", "print(f'  Unambiguous:  {eval_stats[\"unambiguous\"]} ({100*eval_stats[\"unambiguous\"]/max(eval_stats[\"total\"],1):.1f}%)')\n", "print(f'  OOV/unseen:   {eval_stats[\"unseen\"]} ({100*eval_stats[\"unseen\"]/max(eval_stats[\"total\"],1):.1f}%)')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 23: GERCEK ACCURACY + GRID SWEEP\n", "# ============================================================\n", "\n", "def eval_real_accuracy(model, eval_set, detail=False):\n", "    correct = 0\n", "    total = 0\n", "    ambig_correct = 0\n", "    ambig_total = 0\n", "    errors = []\n", "    \n", "    for surfaces, cand_lists, golds in eval_set:\n", "        preds = model.disambiguate(surfaces, cand_lists)\n", "        \n", "        for t, (pred, cands, g_idx) in enumerate(zip(preds, cand_lists, golds)):\n", "            gold = cands[g_idx]\n", "            total += 1\n", "            is_ambig = len(cands) > 1\n", "            \n", "            if pred == gold:\n", "                correct += 1\n", "                if is_ambig:\n", "                    ambig_correct += 1\n", "            else:\n", "                if is_ambig and len(errors) < 30:\n", "                    errors.append({\n", "                        'surface': surfaces[t],\n", "                        'gold': extract_disambiguator_state(gold),\n", "                        'pred': extract_disambiguator_state(pred),\n", "                        'n_cands': len(cands),\n", "                    })\n", "            \n", "            if is_ambig:\n", "                ambig_total += 1\n", "    \n", "    overall = correct / max(total, 1)\n", "    ambig_acc = ambig_correct / max(ambig_total, 1)\n", "    \n", "    return {\n", "        'overall': overall,\n", "        'overall_n': (correct, total),\n", "        'ambiguous_acc': ambig_acc,\n", "        'ambiguous_n': (ambig_correct, ambig_total),\n", "        'errors': errors,\n", "    }\n", "\n", "# â”€â”€ GRID SWEEP â”€â”€\n", "lambdas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n", "betas = [0.5, 1.0, 1.5, 2.0]\n", "\n", "print('GRID SWEEP (Train/Test Split, Surface-Conditioned Candidates)')\n", "print('=' * 70)\n", "print(f'Train: {len(train_data)} tok | Test: {len(test_data)} tok | Ambig: {eval_stats[\"ambiguous\"]} tok')\n", "print()\n", "\n", "# Overall accuracy table\n", "header = f'{\"\":<8}'\n", "for b in betas:\n", "    header += f'  b={b:<6}'\n", "print('OVERALL ACCURACY:')\n", "print(header)\n", "print('-' * 45)\n", "\n", "best_overall = 0\n", "best_ambig = 0\n", "best_config_overall = (0, 0)\n", "best_config_ambig = (0, 0)\n", "all_results = {}\n", "\n", "for lam in lambdas:\n", "    row_overall = f'l={lam:<5}'\n", "    for beta in betas:\n", "        model = TunableHMMDisambiguator(lam=lam, beta=beta, smoothing=0.01)\n", "        model.train(train_data)  # SADECE TRAIN ILE EGIT\n", "        res = eval_real_accuracy(model, real_eval)\n", "        \n", "        row_overall += f'  {res[\"overall\"]*100:>5.1f}%'\n", "        all_results[(lam, beta)] = res\n", "        \n", "        if res['overall'] > best_overall:\n", "            best_overall = res['overall']\n", "            best_config_overall = (lam, beta)\n", "        if res['ambiguous_acc'] > best_ambig:\n", "            best_ambig = res['ambiguous_acc']\n", "            best_config_ambig = (lam, beta)\n", "    print(row_overall)\n", "\n", "# Ambiguous accuracy table\n", "print(f'\\nAMBIGUOUS-ONLY ACCURACY:')\n", "print(header)\n", "print('-' * 45)\n", "for lam in lambdas:\n", "    row = f'l={lam:<5}'\n", "    for beta in betas:\n", "        res = all_results[(lam, beta)]\n", "        row += f'  {res[\"ambiguous_acc\"]*100:>5.1f}%'\n", "    print(row)\n", "\n", "print(f'\\nEN IYI OVERALL:  l={best_config_overall[0]} b={best_config_overall[1]} -> {best_overall*100:.1f}%')\n", "print(f'EN IYI AMBIG:    l={best_config_ambig[0]} b={best_config_ambig[1]} -> {best_ambig*100:.1f}%')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# BOLUM 24: HATA ANALIZI + OVERFITTING TESTI\n", "# ============================================================\n", "\n", "# â”€â”€ En iyi modelin hata analizi â”€â”€\n", "bl, bb = best_config_overall\n", "best_model = TunableHMMDisambiguator(lam=bl, beta=bb, smoothing=0.01)\n", "best_model.train(train_data)\n", "best_res = eval_real_accuracy(best_model, real_eval, detail=True)\n", "\n", "print(f'HATA ANALIZI (l={bl}, b={bb})')\n", "print(f'Overall: {best_res[\"overall_n\"][0]}/{best_res[\"overall_n\"][1]} ({best_res[\"overall\"]*100:.1f}%)')\n", "print(f'Ambiguous: {best_res[\"ambiguous_n\"][0]}/{best_res[\"ambiguous_n\"][1]} ({best_res[\"ambiguous_acc\"]*100:.1f}%)')\n", "\n", "# Hatalar\n", "if best_res['errors']:\n", "    print(f'\\nIlk 20 Hata (sadece ambiguous):')\n", "    print(f'{\"Surface\":<20} {\"Gold\":<25} {\"Predicted\":<25} {\"#C\"}')\n", "    print('-' * 75)\n", "    for err in best_res['errors'][:20]:\n", "        print(f'{err[\"surface\"]:<20} {err[\"gold\"]:<25} {err[\"pred\"]:<25} {err[\"n_cands\"]}')\n", "    \n", "    # Hata pattern'leri\n", "    err_patterns = Counter()\n", "    for err in best_res['errors']:\n", "        err_patterns[f'{err[\"gold\"]} -> {err[\"pred\"]}'] += 1\n", "    print(f'\\nEn Sik Hata Pattern\\'leri:')\n", "    for pattern, count in err_patterns.most_common(10):\n", "        print(f'  {count:>3}x  {pattern}')\n", "else:\n", "    print(f'Hata yok.')\n", "\n", "# â”€â”€ OVERFITTING TESTI â”€â”€\n", "print(f'\\n{\"=\" * 60}')\n", "print(f'OVERFITTING TESTI')\n", "print(f'={\"=\" * 59}')\n", "\n", "# Train set ile eval et (leakage testi)\n", "train_eval, _ = build_real_eval(train_sents, surface_to_states, surface_to_fst)\n", "train_res = eval_real_accuracy(best_model, train_eval)\n", "\n", "print(f'Train accuracy: {train_res[\"overall\"]*100:.1f}% ({train_res[\"overall_n\"][0]}/{train_res[\"overall_n\"][1]})')\n", "print(f'Test accuracy:  {best_res[\"overall\"]*100:.1f}% ({best_res[\"overall_n\"][0]}/{best_res[\"overall_n\"][1]})')\n", "gap = train_res['overall'] - best_res['overall']\n", "print(f'Gap:            {gap*100:.1f}%')\n", "\n", "if gap > 0.10:\n", "    print(f'UYARI: Yuksek overfitting gabi ({gap*100:.1f}% > 10%)')\n", "elif gap > 0.05:\n", "    print(f'DIKKAT: Orta overfitting ({gap*100:.1f}%)')\n", "else:\n", "    print(f'OK: Dusuk overfitting ({gap*100:.1f}%)')\n", "\n", "# Ambiguous bazinda\n", "print(f'\\nTrain ambig acc: {train_res[\"ambiguous_acc\"]*100:.1f}% ({train_res[\"ambiguous_n\"][0]}/{train_res[\"ambiguous_n\"][1]})')\n", "print(f'Test ambig acc:  {best_res[\"ambiguous_acc\"]*100:.1f}% ({best_res[\"ambiguous_n\"][0]}/{best_res[\"ambiguous_n\"][1]})')\n", "\n", "# â”€â”€ Random gold testi â”€â”€\n", "print(f'\\nRANDOM GOLD TESTI (gold yerine rastgele candidate sec):')\n", "import copy\n", "random_eval = []\n", "for surfaces, cands, golds in real_eval:\n", "    fake_golds = []\n", "    for g, c in zip(golds, cands):\n", "        if len(c) > 1:\n", "            # Gold OLMAYAN bir candidate sec\n", "            alternatives = [i for i in range(len(c)) if i != g]\n", "            fake_golds.append(random.choice(alternatives) if alternatives else g)\n", "        else:\n", "            fake_golds.append(g)\n", "    random_eval.append((surfaces, cands, fake_golds))\n", "\n", "random_res = eval_real_accuracy(best_model, random_eval)\n", "print(f'Random-gold accuracy: {random_res[\"overall\"]*100:.1f}%')\n", "print(f'Gercek accuracy:      {best_res[\"overall\"]*100:.1f}%')\n", "diff = best_res['overall'] - random_res['overall']\n", "print(f'Fark: {diff*100:.1f}%')\n", "if diff > 0.1:\n", "    print(f'Model gercekten ayirt edebiliyor (fark > 10%)')\n", "else:\n", "    print(f'UYARI: Model yeterince ayirt edemiyor')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "# Part D: FST + HMM Entegre Test (Colab)\n", "\n", "Bu bolum FST analyzer ile HMM disambiguator'u birlestirerek gercek test yapar.\n", "\n", "**Pipeline:**\n", "```\n", "Cumle -> tokenize -> FST(kelime) -> candidate listesi -> HMM Viterbi -> disambiguated\n", "```\n", "\n", "**Gereksinim:** `turkish_morphological_segmentation.ipynb`'deki hucrelerin oncelikle calistirilmis olmasi.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# FST ANALYZER YUKLEME (Colab)\n", "# ============================================================\n", "\n", "import os, glob, json as _json\n", "\n", "# 1. pynini kur\n", "try:\n", "    import pynini\n", "    print('pynini zaten kurulu.')\n", "except ImportError:\n", "    print('pynini kuruluyor...')\n", "    !pip install pynini -q\n", "    import pynini\n", "    print('pynini kuruldu.')\n", "\n", "# 2. FST notebook dosyasini bul\n", "_fst_name = 'turkish_morphological_segmentation.ipynb'\n", "_search_dirs = [\n", "    '.',\n", "    '/content',\n", "    '/content/drive/MyDrive',\n", "    '/content/drive/MyDrive/Colab Notebooks',\n", "    '/content/drive/MyDrive/notebooks',\n", "    os.path.dirname(os.path.abspath('.')),\n", "]\n", "\n", "_fst_path = None\n", "for _d in _search_dirs:\n", "    _candidate = os.path.join(_d, _fst_name)\n", "    if os.path.exists(_candidate):\n", "        _fst_path = _candidate\n", "        break\n", "\n", "# Hala bulamadiysa glob ile ara\n", "if _fst_path is None:\n", "    _found = glob.glob(f'/content/**/{_fst_name}', recursive=True)\n", "    if _found:\n", "        _fst_path = _found[0]\n", "\n", "if _fst_path is None:\n", "    print(f'HATA: {_fst_name} bulunamadi!')\n", "    print(f'\\nMevcut dosyalar:')\n", "    for f in sorted(glob.glob('/content/**/*.*', recursive=True))[:20]:\n", "        print(f'  {f}')\n", "    print(f'\\nCOZUM: {_fst_name} dosyasini upload edin:')\n", "    print(f'  1. Sol panelde \"Files\" ikonuna tiklayin')\n", "    print(f'  2. \"Upload\" ile {_fst_name} dosyasini yukleyin')\n", "    print(f'  3. Bu hucreyi tekrar calistirin')\n", "    raise FileNotFoundError(_fst_name)\n", "\n", "print(f'FST notebook bulundu: {_fst_path}')\n", "\n", "# 3. Kod hucrelerini yukle\n", "try:\n", "    analyze_word\n", "    print('analyze_word() zaten mevcut.')\n", "except NameError:\n", "    print('FST fonksiyonlari yukleniyor...')\n", "    with open(_fst_path, 'r', encoding='utf-8') as _f:\n", "        _fst_nb = _json.load(_f)\n", "    \n", "    _skip = ['main()', 'if __name__', 'input(', '# TEST', '# DEMO', 'analyze_sentence']\n", "    _loaded = 0\n", "    _errors = 0\n", "    for _cell in _fst_nb['cells']:\n", "        if _cell['cell_type'] != 'code':\n", "            continue\n", "        _src = ''.join(_cell['source'])\n", "        if any(kw in _src for kw in _skip):\n", "            continue\n", "        if '!pip' in _src or '!apt' in _src or '%' in _src:\n", "            continue\n", "        try:\n", "            exec(_src, globals())\n", "            _loaded += 1\n", "        except Exception as _e:\n", "            _errors += 1\n", "    \n", "    print(f'  {_loaded} hucre yuklendi, {_errors} atlandi.')\n", "\n", "# 4. Lexicon + Analyzer\n", "try:\n", "    analyzer\n", "    print('analyzer zaten mevcut.')\n", "except NameError:\n", "    print('Lexicon araniyor...')\n", "    _lex_names = ['final_core_roots.json', 'MASTER_FINAL_ROOTS.json', 'turkish_lexicon.json', 'lexicon.json']\n", "    _lex_path = None\n", "    \n", "    for _name in _lex_names:\n", "        for _d in _search_dirs:\n", "            _c = os.path.join(_d, _name)\n", "            if os.path.exists(_c):\n", "                _lex_path = _c\n", "                break\n", "        if _lex_path:\n", "            break\n", "    \n", "    # glob fallback\n", "    if _lex_path is None:\n", "        for _name in _lex_names:\n", "            _found = glob.glob(f'/content/**/{_name}', recursive=True)\n", "            if _found:\n", "                _lex_path = _found[0]\n", "                break\n", "    \n", "    if _lex_path:\n", "        print(f'Lexicon bulundu: {_lex_path}')\n", "        lexicon = load_lexicon(_lex_path)\n", "        lexicon = normalize_lexicon(lexicon)\n", "        analyzer = build_analyzer(lexicon)\n", "        print(f'Analyzer HAZIR.')\n", "    else:\n", "        print('HATA: Lexicon dosyasi bulunamadi!')\n", "        _jsons = glob.glob('/content/**/*.json', recursive=True)\n", "        print(f'Mevcut JSON dosyalari: {_jsons[:10]}')\n", "        print('Lexicon dosyanizi upload edin.')\n", "\n", "# 5. Hizli test\n", "try:\n", "    _test = analyze_word('ev', analyzer)\n", "    print(f'\\nTest: analyze_word(\"ev\") = {_test}')\n", "    print('\\nFST + ANALYZER HAZIR!')\n", "except Exception as e:\n", "    print(f'Test hatasi: {e}')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# FST + HMM PIPELINE\n", "# ============================================================\n", "\n", "import re\n", "\n", "def tokenize_turkish(text):\n", "    \"\"\"Turkce cumleyi tokenize et.\"\"\"\n", "    return re.findall(r\"[\\w']+|[.,!?;:]\", text.lower())\n", "\n", "\n", "def disambiguate_sentence(sentence, analyzer, hmm_model):\n", "    \"\"\"\n", "    Tam pipeline: cumle -> FST analiz -> HMM disambiguasyon\n", "    \n", "    Args:\n", "        sentence: Turkce cumle (str)\n", "        analyzer: pynini FST analyzer\n", "        hmm_model: TunableHMMDisambiguator instance\n", "    \n", "    Returns:\n", "        list of dict: her token icin {surface, candidates, selected, state}\n", "    \"\"\"\n", "    tokens = tokenize_turkish(sentence)\n", "    \n", "    surfaces = []\n", "    candidate_lists = []\n", "    \n", "    for word in tokens:\n", "        # FST'den gercek candidate'lari al\n", "        candidates = analyze_word(word, analyzer)\n", "        \n", "        # Sonuc yoksa veya 'No analysis' ise OOV\n", "        if not candidates:\n", "            candidates = [word + '+UNK']\n", "        elif len(candidates) == 1 and 'No analysis' in candidates[0]:\n", "            candidates = [word + '+UNK']\n", "        \n", "        surfaces.append(word)\n", "        candidate_lists.append(candidates)\n", "    \n", "    # HMM Viterbi ile disambiguate et\n", "    best_path = hmm_model.disambiguate(surfaces, candidate_lists)\n", "    \n", "    # Sonuclari formatla\n", "    results = []\n", "    for word, cands, selected in zip(tokens, candidate_lists, best_path):\n", "        state = extract_disambiguator_state(selected)\n", "        results.append({\n", "            'surface': word,\n", "            'candidates': cands,\n", "            'n_candidates': len(cands),\n", "            'selected': selected,\n", "            'state': state,\n", "        })\n", "    \n", "    return results\n", "\n", "\n", "def print_result(results, title=None):\n", "    \"\"\"Disambiguasyon sonucunu formatli yazdir.\"\"\"\n", "    if title:\n", "        print(f'\\nCumle: {title}')\n", "    print(f'{\"Surface\":<15} {\"Secilen Analiz\":<35} {\"State\":<20} {\"#Cand\"}')\n", "    print('-' * 75)\n", "    for r in results:\n", "        marker = '*' if r['n_candidates'] > 1 else ' '\n", "        print(f'{marker} {r[\"surface\"]:<14} {r[\"selected\"]:<35} {r[\"state\"]:<20} {r[\"n_candidates\"]}')\n", "        if r['n_candidates'] > 1:\n", "            for c in r['candidates']:\n", "                flag = ' <<' if c == r['selected'] else ''\n", "                print(f'  {\"\":<14}   {c}{flag}')\n", "    print(f'\\n* = FST birden fazla analiz uretti, HMM secim yapti')\n", "\n", "\n", "print('disambiguate_sentence() ve print_result() HAZIR.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# MODEL EGITIMI (henuz egitilmediyse)\n", "# ============================================================\n", "\n", "try:\n", "    hmm_model\n", "    print(f'Model zaten egitilmis: {hmm_model.total_tokens} token')\n", "except NameError:\n", "    print('HMM model egitiliyor...')\n", "    hmm_model = TunableHMMDisambiguator(lam=0.7, beta=1.5, smoothing=0.01)\n", "    hmm_model.train(mapped_data)\n", "    print(f'Egitim tamam: {hmm_model.total_tokens} token, {len(hmm_model.states)} state')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# GERCEK FST + HMM TESTI\n", "# ============================================================\n", "# FST gercek candidate'lari uretiyor, HMM seciyor.\n", "\n", "test_sentences = [\n", "    'cocuk okula gitti',\n", "    'guzel bir ev aldik',\n", "    'bu kitabi okudum',\n", "    'yarin gelecek misiniz',\n", "    'annesi onu cok seviyor',\n", "    'turkce cok guzel bir dil',\n", "    'benim adim ne',\n", "    'bugÃ¼n hava cok sicak',\n", "]\n", "\n", "print('FST + HMM ENTEGRE TEST')\n", "print('=' * 75)\n", "\n", "total_tokens = 0\n", "total_ambig = 0\n", "total_oov = 0\n", "\n", "for sent in test_sentences:\n", "    result = disambiguate_sentence(sent, analyzer, hmm_model)\n", "    print_result(result, title=sent)\n", "    \n", "    for r in result:\n", "        total_tokens += 1\n", "        if r['n_candidates'] > 1:\n", "            total_ambig += 1\n", "        if 'UNK' in r['selected']:\n", "            total_oov += 1\n", "\n", "print(f'\\n{\"=\" * 75}')\n", "print(f'OZET:')\n", "print(f'  Toplam token:  {total_tokens}')\n", "print(f'  Ambiguous:     {total_ambig} ({100*total_ambig/max(total_tokens,1):.0f}%)')\n", "print(f'  OOV:           {total_oov} ({100*total_oov/max(total_tokens,1):.0f}%)')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# KENDI CUMLENI TEST ET\n", "# ============================================================\n", "# Asagidaki cumleyi degistirip hucreyi tekrar calistirin.\n", "\n", "cumle = 'ben seni cok seviyorum'  # <-- BURAYA CUMLENIZI YAZIN\n", "\n", "result = disambiguate_sentence(cumle, analyzer, hmm_model)\n", "print_result(result, title=cumle)\n"]}]}