{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNF+e/n1vkKiNVkhNZzDCeZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import json\n","import sqlite3\n","import os\n","import glob\n","from tqdm import tqdm\n","\n","# Ana Proje Yolu (Senin Drive yapÄ±na gÃ¶re)\n","BASE_PATH = '/content/drive/MyDrive/FSTurk'\n","os.chdir(BASE_PATH)\n","\n","print(f\"ğŸ“‚ Ã‡alÄ±ÅŸma dizini: {os.getcwd()}\")\n","print(\"âœ… Gerekli kÃ¼tÃ¼phaneler yÃ¼klendi.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tykR2Zs5kVMF","executionInfo":{"status":"ok","timestamp":1765828350348,"user_tz":-180,"elapsed":33,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"a57b2e48-92db-4ea1-db04-613a5acc2b0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Ã‡alÄ±ÅŸma dizini: /content/drive/MyDrive/FSTurk\n","âœ… Gerekli kÃ¼tÃ¼phaneler yÃ¼klendi.\n"]}]},{"cell_type":"code","source":["#\" \", -, . gibi karakterler iÃ§in Ã¶zelleÅŸmiÅŸ ZEMBEREK VE KAIKKI FÄ°LTRASYONU\n","\n","import json\n","import os\n","import glob\n","from collections import defaultdict\n","\n","# --- AYARLAR ---\n","BASE_PATH = '/content/drive/MyDrive/FSTurk'\n","ZEMBEREK_FILE = os.path.join(BASE_PATH, \"turkish_processed_zemberek_lexicon.json\")\n","KAIKKI_FOLDER = os.path.join(BASE_PATH, \"kaikki_processing\")\n","OUTPUT_FILE = os.path.join(BASE_PATH, \"final_lexicon_atomic_base.json\")\n","\n","# TR -> EN MAPPING\n","TAG_MAPPING = {\n","    'ISIM': 'NOUN',     'NOUN': 'NOUN',\n","    'FIIL': 'VERB',     'VERB': 'VERB',\n","    'SIFAT': 'ADJ',     'ADJ': 'ADJ',\n","    'ZARF': 'ADV',      'ADV': 'ADV',\n","    'ZAMIR': 'PRON',    'PRON': 'PRON',\n","    'BAGLAC': 'CONJ',   'CONJ': 'CONJ', 'CCONJ': 'CONJ',\n","    'SAYI': 'NUM',      'NUM': 'NUM',\n","    'EDAT': 'ADP',      'ADP': 'ADP',\n","    'BELIRTEC': 'DET',  'DET': 'DET',\n","    'OZEL ISIM': 'NOUN', 'PROPN': 'NOUN',\n","    'UNK': 'OTHER',     'OTHER': 'OTHER'\n","}\n","\n","def is_clean_atomic_word(word, tag):\n","    \"\"\"\n","    Kelimenin FST iÃ§in uygun, temiz ve atomik olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.\n","    \"\"\"\n","    word = word.strip()\n","\n","    # 1. BoÅŸluk KontrolÃ¼ (Multi-word)\n","    if ' ' in word: return False\n","\n","    # 2. Tire KontrolÃ¼ (Compound words) -> \"e-devlet\" istemiyoruz\n","    if '-' in word: return False\n","\n","    # 3. Uzunluk KontrolÃ¼ (Tek harf 'o' hariÃ§ genelde Ã§Ã¶p olabilir ama ÅŸimdilik tutalÄ±m)\n","    if len(word) == 0: return False\n","\n","    # 4. SayÄ±/Sembol KontrolÃ¼ (NUM kategorisi hariÃ§)\n","    # EÄŸer kelime NUM deÄŸilse ama iÃ§inde rakam varsa (Ã¶rn: \"covid19\") atalÄ±m mÄ±?\n","    # Evet, temiz sÃ¶zlÃ¼k iÃ§in atalÄ±m.\n","    if tag != 'NUM':\n","        # isalpha(): Sadece harf mi? (TÃ¼rkÃ§e karakterler ve ÅŸapkalar dahildir)\n","        # Ancak tire ve boÅŸluk zaten yukarÄ±da elendi.\n","        # BazÄ± kÄ±saltmalar (T.C.) noktalÄ± olabilir. FST genelde noktasÄ±z sever.\n","        if '.' in word: return False\n","\n","        # Ä°Ã§inde rakam var mÄ±? (Word123 -> Ã‡Ã¶p)\n","        if any(char.isdigit() for char in word): return False\n","\n","    return True\n","\n","def create_pristine_lexicon():\n","    print(\"ğŸ’ PRISTINE (TERTEMÄ°Z) SÃ–ZLÃœK OLUÅTURULUYOR...\")\n","    print(\"   ğŸš« Filtreler: BoÅŸluk, Tire (-), Nokta (.), RakamlÄ± Kelimeler\")\n","\n","    grouped_data = defaultdict(set)\n","    dropped_count = 0\n","\n","    # --- 1. ZEMBEREK ---\n","    print(f\"1ï¸âƒ£ Zemberek taranÄ±yor...\")\n","    if os.path.exists(ZEMBEREK_FILE):\n","        with open(ZEMBEREK_FILE, 'r', encoding='utf-8') as f:\n","            raw_data = json.load(f)\n","\n","            # Veri listeyse\n","            if isinstance(raw_data, list):\n","                for item in raw_data:\n","                    if len(item) >= 2:\n","                        word = item[0]\n","                        tr_tag = item[1]\n","                        en_tag = TAG_MAPPING.get(tr_tag, 'OTHER')\n","\n","                        if is_clean_atomic_word(word, en_tag):\n","                            grouped_data[en_tag].add(word.strip())\n","                        else:\n","                            dropped_count += 1\n","\n","            # Veri sÃ¶zlÃ¼kse\n","            elif isinstance(raw_data, dict):\n","                for tag, words in raw_data.items():\n","                    en_tag = TAG_MAPPING.get(tag, 'OTHER')\n","                    for w in words:\n","                        if is_clean_atomic_word(w, en_tag):\n","                            grouped_data[en_tag].add(w.strip())\n","                        else:\n","                            dropped_count += 1\n","\n","    # --- 2. KAIKKI ---\n","    print(f\"2ï¸âƒ£ Kaikki taranÄ±yor...\")\n","    jsonl_files = glob.glob(os.path.join(KAIKKI_FOLDER, \"*.jsonl\"))\n","\n","    for filepath in jsonl_files:\n","        filename = os.path.basename(filepath)\n","        if filename == \"tr-extract.jsonl\": continue\n","\n","        raw_tag = filename.replace('.jsonl', '')\n","        en_tag = TAG_MAPPING.get(raw_tag, raw_tag) # VarsayÄ±lan: dosya adÄ±\n","\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                try:\n","                    data = json.loads(line)\n","                    word = data.get('word')\n","                    if word:\n","                        if is_clean_atomic_word(word, en_tag):\n","                            grouped_data[en_tag].add(word.strip())\n","                        else:\n","                            dropped_count += 1\n","                except: continue\n","\n","    # --- 3. KAYDETME ---\n","    print(f\"3ï¸âƒ£ Final dosya yazÄ±lÄ±yor...\")\n","\n","    final_output = {}\n","    total_words = 0\n","\n","    for tag, word_set in grouped_data.items():\n","        final_output[tag] = sorted(list(word_set))\n","        total_words += len(final_output[tag])\n","        print(f\"   ğŸ”¹ {tag}: {len(final_output[tag])} kelime\")\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(final_output, f, ensure_ascii=False)\n","\n","    print(f\"\\nâœ¨ TERTEMÄ°Z BAÅLANGIÃ‡ HAZIR!\")\n","    print(f\"   ğŸ—‘ï¸ Filtrelenen (Ã‡Ã¶p/BileÅŸik) Kelime: {dropped_count}\")\n","    print(f\"   ğŸ’ Saf Kelime SayÄ±sÄ±: {total_words}\")\n","    print(f\"   ğŸ“‚ Dosya: {OUTPUT_FILE}\")\n","\n","create_pristine_lexicon()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-vb-i83mUF0","executionInfo":{"status":"ok","timestamp":1765836644084,"user_tz":-180,"elapsed":4223,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"da18cbf2-9f5d-43a0-c3c8-56f632caa28a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ’ PRISTINE (TERTEMÄ°Z) SÃ–ZLÃœK OLUÅTURULUYOR...\n","   ğŸš« Filtreler: BoÅŸluk, Tire (-), Nokta (.), RakamlÄ± Kelimeler\n","1ï¸âƒ£ Zemberek taranÄ±yor...\n","2ï¸âƒ£ Kaikki taranÄ±yor...\n","3ï¸âƒ£ Final dosya yazÄ±lÄ±yor...\n","   ğŸ”¹ OTHER: 90758 kelime\n","   ğŸ”¹ PRON: 135 kelime\n","   ğŸ”¹ NOUN: 145538 kelime\n","   ğŸ”¹ ADJ: 10831 kelime\n","   ğŸ”¹ NUM: 11 kelime\n","   ğŸ”¹ VERB: 68180 kelime\n","   ğŸ”¹ CONJ: 52 kelime\n","   ğŸ”¹ ADV: 1703 kelime\n","\n","âœ¨ TERTEMÄ°Z BAÅLANGIÃ‡ HAZIR!\n","   ğŸ—‘ï¸ Filtrelenen (Ã‡Ã¶p/BileÅŸik) Kelime: 34980\n","   ğŸ’ Saf Kelime SayÄ±sÄ±: 317208\n","   ğŸ“‚ Dosya: /content/drive/MyDrive/FSTurk/final_lexicon_atomic_base.json\n"]}]},{"cell_type":"code","source":["#--AMAÃ‡ WIKIMEDIA VERÄ°LERÄ°NÄ° STANZA iÃ§in  TÃœRLERÄ°NE AYIRMA--\n","import json\n","import os\n","import re\n","from tqdm import tqdm\n","\n","# --- AYARLAR ---\n","BASE_PATH = '/content/drive/MyDrive/FSTurk'\n","# Åu anki en gÃ¼ncel dosyan bu (Zemberek + Kaikki + GruplanmÄ±ÅŸ):\n","INPUT_LEXICON = os.path.join(BASE_PATH, \"final_lexicon_grouped_english.json\")\n","CORPUS_FILE = os.path.join(BASE_PATH, \"wikimedia_data\", \"cleaned_corpus.txt\")\n","OUTPUT_FILE = os.path.join(BASE_PATH, \"contain_zemberek_kaikki_wikimedia_with_stanza.json\")\n","\n","def add_corpus_words_to_waiting_room():\n","    print(\"ğŸš€ WIKIPEDIA TARAMASI BAÅLIYOR...\")\n","\n","    # 1. Mevcut SÃ¶zlÃ¼ÄŸÃ¼ YÃ¼kle\n","    if not os.path.exists(INPUT_LEXICON):\n","        print(\"âŒ SÃ¶zlÃ¼k bulunamadÄ±!\")\n","        return\n","\n","    print(f\"1ï¸âƒ£ Mevcut sÃ¶zlÃ¼k yÃ¼kleniyor...\")\n","    with open(INPUT_LEXICON, 'r', encoding='utf-8') as f:\n","        lexicon_data = json.load(f)\n","\n","    # HÄ±zlÄ± kontrol iÃ§in mevcut kelimelerin hepsini bir kÃ¼meye (set) alalÄ±m\n","    existing_words = set()\n","    for tag, word_list in lexicon_data.items():\n","        for w in word_list:\n","            existing_words.add(w)\n","\n","    print(f\"   âœ… HafÄ±zada zaten {len(existing_words)} kelime var.\")\n","\n","    # 2. Corpus DosyasÄ±nÄ± Tara\n","    print(f\"2ï¸âƒ£ Corpus dosyasÄ±ndan YENÄ° kelimeler aranÄ±yor...\")\n","\n","    if not os.path.exists(CORPUS_FILE):\n","        print(\"âŒ Corpus dosyasÄ± yok! LÃ¼tfen wikimedia_data klasÃ¶rÃ¼nÃ¼ kontrol et.\")\n","        return\n","\n","    new_candidates = set()\n","    # Regex: Sadece TÃ¼rkÃ§e karakter iÃ§eren kelimeleri al (noktalama yok, sayÄ± yok)\n","    word_pattern = re.compile(r\"[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ]+\")\n","\n","    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n","        # Tqdm ile ilerleme Ã§ubuÄŸu gÃ¶ster\n","        for line in tqdm(f, desc=\"Wiki TaranÄ±yor\"):\n","            words = word_pattern.findall(line)\n","            for w in words:\n","                # KÃ¼Ã§Ã¼k harfe Ã§evir (Lexicon standardÄ± iÃ§in)\n","                w_clean = w.lower()\n","\n","                # Kurallar:\n","                # 1. 2 harften uzun olsun (ve, de, ki gibi baÄŸlaÃ§lar zaten vardÄ±r)\n","                # 2. Zaten sÃ¶zlÃ¼kte OLMASIN\n","                if len(w_clean) > 2 and w_clean not in existing_words:\n","                    new_candidates.add(w_clean)\n","\n","    print(f\"   âœ¨ Wikipedia'dan {len(new_candidates)} adet YEPYENÄ° kelime bulundu!\")\n","\n","    # 3. BunlarÄ± 'OTHER' Kategorisine Ekle (Bekleme OdasÄ±)\n","    if 'OTHER' not in lexicon_data:\n","        lexicon_data['OTHER'] = []\n","\n","    # KÃ¼meyi listeye Ã§evirip ekle\n","    lexicon_data['OTHER'].extend(list(new_candidates))\n","\n","    # OTHER iÃ§indeki tekrarlarÄ± temizle\n","    lexicon_data['OTHER'] = sorted(list(set(lexicon_data['OTHER'])))\n","\n","    print(f\"   ğŸ“¦ Yeni kelimeler 'OTHER' (Formatlanacaklar) listesine eklendi.\")\n","    print(f\"   ğŸ§ Åu an 'OTHER' toplam boyutu: {len(lexicon_data['OTHER'])}\")\n","\n","    # 4. Kaydet\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(lexicon_data, f, ensure_ascii=False)\n","\n","    print(f\"\\nğŸ’¾ Dosya Kaydedildi: {OUTPUT_FILE}\")\n","    print(\"ğŸ‘‰ SIRADAKÄ° ADIM: Stanza kodunu bu dosya Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±p 'OTHER'larÄ± eritmek!\")\n","\n","add_corpus_words_to_waiting_room()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp9c-a1o6dWR","executionInfo":{"status":"ok","timestamp":1765834591066,"user_tz":-180,"elapsed":39345,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"f9d65a67-a515-4617-aa73-6f1f75fa5ba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ WIKIPEDIA TARAMASI BAÅLIYOR...\n","1ï¸âƒ£ Mevcut sÃ¶zlÃ¼k yÃ¼kleniyor...\n","   âœ… HafÄ±zada zaten 322359 kelime var.\n","2ï¸âƒ£ Corpus dosyasÄ±ndan YENÄ° kelimeler aranÄ±yor...\n"]},{"output_type":"stream","name":"stderr","text":["Wiki TaranÄ±yor: 100000it [00:35, 2812.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   âœ¨ Wikipedia'dan 1033121 adet YEPYENÄ° kelime bulundu!\n","   ğŸ“¦ Yeni kelimeler 'OTHER' (Formatlanacaklar) listesine eklendi.\n","   ğŸ§ Åu an 'OTHER' toplam boyutu: 1129886\n","\n","ğŸ’¾ Dosya Kaydedildi: /content/drive/MyDrive/FSTurk/contain_zemberek_kaikki_wikimedia_with_stanza.json\n","ğŸ‘‰ SIRADAKÄ° ADIM: Stanza kodunu bu dosya Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±p 'OTHER'larÄ± eritmek!\n"]}]},{"cell_type":"code","source":["#T4 SEÃ‡TÄ°ÄÄ°MDEN EMÄ°N OLUN.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaSc_AVbJhia","executionInfo":{"status":"ok","timestamp":1765838955246,"user_tz":-180,"elapsed":1238,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"8add49b7-2338-4c07-c207-7dd6e31e475d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install stanza"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eS2MavreJtRJ","executionInfo":{"status":"ok","timestamp":1765838964534,"user_tz":-180,"elapsed":3979,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"93580faf-943e-45e4-e5f0-17d639f054c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: stanza in /usr/local/lib/python3.12/dist-packages (1.11.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from stanza) (2.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n","Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.5.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.11.12)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n"]}]},{"cell_type":"code","source":["import torch\n","print(\"GPU Var mÄ±?:\", torch.cuda.is_available())\n","try:\n","    print(\"GPU AdÄ±:\", torch.cuda.get_device_name(0))\n","    print(\"âœ… GPU AKTÄ°F! Devam edebilirsin.\")\n","except:\n","    print(\"âŒ GPU YOK! AyarlarÄ± kontrol et.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJ5BQBfmQlVK","executionInfo":{"status":"ok","timestamp":1765839893128,"user_tz":-180,"elapsed":4485,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"f3b70a8f-4cfb-4f40-a41f-3b617f4b191a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Var mÄ±?: True\n","GPU AdÄ±: Tesla T4\n","âœ… GPU AKTÄ°F! Devam edebilirsin.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAkOLmlRQphJ","executionInfo":{"status":"ok","timestamp":1765839923697,"user_tz":-180,"elapsed":17790,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"ba819aef-4cad-4ae0-dc2d-f2044bff9da2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install stanza"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-G5yCk4Qtgs","executionInfo":{"status":"ok","timestamp":1765839938137,"user_tz":-180,"elapsed":6830,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"0ad69023-a06b-4845-f053-3077e97111d0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting stanza\n","  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n","Collecting emoji (from stanza)\n","  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n","Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.5.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.11.12)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n","Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji, stanza\n","Successfully installed emoji-2.15.0 stanza-1.11.0\n"]}]},{"cell_type":"code","source":["import stanza\n","import json\n","import os\n","from tqdm import tqdm\n","import math\n","import torch\n","\n","# --- AYARLAR ---\n","BASE_PATH = '/content/drive/MyDrive/FSTurk'\n","INPUT_FILE = os.path.join(BASE_PATH, \"contain_zemberek_kaikki_wikimedia_with_stanza.json\")\n","OUTPUT_FILE = os.path.join(BASE_PATH, \"MASTER_FINAL_LEXICON.json\")\n","TEMP_SAVE_FILE = os.path.join(BASE_PATH, \"temp_stanza_progress.json\")\n","\n","def turbo_stanza_safe():\n","    # 1. GPU ZORUNLU KONTROL\n","    if not torch.cuda.is_available():\n","        raise Exception(\"âŒ HATA: GPU algÄ±lanamadÄ±! Runtime ayarlarÄ±ndan T4 GPU seÃ§ili mi?\")\n","\n","    print(\"âœ… GPU AlgÄ±landÄ±, Turbo Mod BaÅŸlatÄ±lÄ±yor...\")\n","\n","    # 2. Stanza YÃ¼kle\n","    stanza.download('tr', processors='tokenize,pos', verbose=False)\n","    # use_gpu=True ZORUNLU\n","    nlp = stanza.Pipeline('tr', processors='tokenize,pos', use_gpu=True, verbose=False, tokenize_no_ssplit=True)\n","\n","    # 3. Veri YÃ¼kle\n","    if not os.path.exists(INPUT_FILE):\n","        print(\"âŒ Dosya bulunamadÄ±!\")\n","        return\n","\n","    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    full_other_list = data.get('OTHER', [])\n","    print(f\"   ğŸ“¦ Toplam 'OTHER' Kelime SayÄ±sÄ±: {len(full_other_list)}\")\n","\n","    if len(full_other_list) == 0:\n","        print(\"   âœ… Ä°ÅŸlenecek veri yok!\")\n","        return\n","\n","    # 4. Checkpoint KontrolÃ¼\n","    processed_results = {}\n","\n","    if os.path.exists(TEMP_SAVE_FILE):\n","        print(\"   ğŸ“‚ KayÄ±tlÄ± ilerleme bulundu! Devam ediliyor...\")\n","        with open(TEMP_SAVE_FILE, 'r', encoding='utf-8') as f:\n","            processed_results = json.load(f)\n","\n","            # Zaten yapÄ±lanlarÄ± kÃ¼me (set) iÃ§ine alÄ±p hÄ±zla eleyelim\n","            done_set = set()\n","            for v_list in processed_results.values():\n","                done_set.update(v_list)\n","\n","            # List comprehension ile filtrele\n","            full_other_list = [w for w in full_other_list if w not in done_set]\n","            print(f\"   â© {len(done_set)} kelime atlandÄ±. Kalan: {len(full_other_list)}\")\n","\n","    else:\n","        processed_results = {\n","            'NOUN': [], 'VERB': [], 'ADJ': [], 'ADV': [],\n","            'PRON': [], 'CONJ': [], 'NUM': [], 'ADP': [], 'DET': []\n","        }\n","\n","    # 5. BATCH PROCESSING\n","    BATCH_SIZE = 500\n","    total_batches = math.ceil(len(full_other_list) / BATCH_SIZE)\n","\n","    print(\"   ğŸ”¥ Ä°ÅŸlem BaÅŸlÄ±yor...\")\n","    save_counter = 0\n","\n","    # Tqdm ile ilerle\n","    for i in tqdm(range(0, len(full_other_list), BATCH_SIZE), total=total_batches, desc=\"Turbo GPU\"):\n","        batch_words = full_other_list[i : i + BATCH_SIZE]\n","\n","        # Kelimeleri birleÅŸtir (Stanza iÃ§in tek metin)\n","        batch_text = \"\\n\".join(batch_words)\n","\n","        try:\n","            doc = nlp(batch_text)\n","\n","            for sentence in doc.sentences:\n","                for word_obj in sentence.words:\n","                    if word_obj.upos in ['NOUN', 'PROPN']: t = 'NOUN'\n","                    elif word_obj.upos == 'VERB': t = 'VERB'\n","                    elif word_obj.upos == 'ADJ': t = 'ADJ'\n","                    elif word_obj.upos == 'ADV': t = 'ADV'\n","                    elif word_obj.upos == 'PRON': t = 'PRON'\n","                    elif word_obj.upos in ['CCONJ', 'SCONJ']: t = 'CONJ'\n","                    elif word_obj.upos == 'NUM': t = 'NUM'\n","                    elif word_obj.upos == 'ADP': t = 'ADP'\n","                    elif word_obj.upos == 'DET': t = 'DET'\n","                    else: t = 'UNK'\n","\n","                    if t != 'UNK':\n","                        processed_results[t].append(word_obj.text)\n","        except Exception as e:\n","            # Hata olsa bile devam et, sadece o batch'i atla\n","            continue\n","\n","        # KAYDETME (Her 20 batch - 10k kelime)\n","        save_counter += 1\n","        if save_counter % 20 == 0:\n","            with open(TEMP_SAVE_FILE, 'w', encoding='utf-8') as f:\n","                json.dump(processed_results, f, ensure_ascii=False)\n","\n","    # 6. FÄ°NAL\n","    print(\"\\n   ğŸ’¾ Bitti! Kaydediliyor...\")\n","    final_data = data.copy()\n","    final_data['OTHER'] = []\n","\n","    for tag, new_words in processed_results.items():\n","        if tag in final_data:\n","            final_data[tag].extend(new_words)\n","            final_data[tag] = sorted(list(set(final_data[tag])))\n","        else:\n","            final_data[tag] = sorted(list(set(new_words)))\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(final_data, f, ensure_ascii=False)\n","\n","    if os.path.exists(TEMP_SAVE_FILE):\n","        os.remove(TEMP_SAVE_FILE)\n","\n","    print(f\"ğŸ† FÄ°NAL DOSYA HAZIR: {OUTPUT_FILE}\")\n","\n","turbo_stanza_safe()"],"metadata":{"id":"T_f3KLmfNKU7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765843397423,"user_tz":-180,"elapsed":3421983,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"9019de09-64f3-4e06-ebf4-49e2933b6f40"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… GPU AlgÄ±landÄ±, Turbo Mod BaÅŸlatÄ±lÄ±yor...\n","   ğŸ“¦ Toplam 'OTHER' Kelime SayÄ±sÄ±: 1129886\n","   ğŸ“‚ KayÄ±tlÄ± ilerleme bulundu! Devam ediliyor...\n","   â© 39686 kelime atlandÄ±. Kalan: 1091019\n","   ğŸ”¥ Ä°ÅŸlem BaÅŸlÄ±yor...\n"]},{"output_type":"stream","name":"stderr","text":["Turbo GPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2183/2183 [56:42<00:00,  1.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","   ğŸ’¾ Bitti! Kaydediliyor...\n","ğŸ† FÄ°NAL DOSYA HAZIR: /content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json\n"]}]},{"cell_type":"code","source":["import json\n","import random\n","import os\n","\n","FILE_PATH = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json'\n","\n","def check_lexicon_health():\n","    if not os.path.exists(FILE_PATH):\n","        print(\"âŒ Dosya bulunamadÄ±! Yolu kontrol et.\")\n","        return\n","\n","    print(\"ğŸ“‚ Dosya yÃ¼kleniyor (Biraz sÃ¼rebilir)...\")\n","    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    print(f\"âœ… Dosya baÅŸarÄ±yla yÃ¼klendi!\")\n","    print(\"=\"*40)\n","\n","    total_words = 0\n","\n","    # Her kategoriden rastgele 5 Ã¶rnek gÃ¶ster\n","    for category, word_list in data.items():\n","        count = len(word_list)\n","        total_words += count\n","        print(f\"ğŸ”¹ {category:<10} : {count:,} kelime\")\n","\n","        # Ã–rnekler\n","        if count > 0:\n","            sample = random.sample(word_list, min(5, count))\n","            print(f\"   Ã–rnekler: {', '.join(sample)}\")\n","        print(\"-\" * 20)\n","\n","    print(\"=\"*40)\n","    print(f\"ğŸ“Š TOPLAM KELÄ°ME SAYISI: {total_words:,}\")\n","\n","check_lexicon_health()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a73JtHSjfHLF","executionInfo":{"status":"ok","timestamp":1765843697767,"user_tz":-180,"elapsed":335,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"e5ee8e13-f74b-4a26-f4c2-24493df318db"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Dosya yÃ¼kleniyor (Biraz sÃ¼rebilir)...\n","âœ… Dosya baÅŸarÄ±yla yÃ¼klendi!\n","========================================\n","ğŸ”¹ OTHER      : 0 kelime\n","--------------------\n","ğŸ”¹ PRON       : 685 kelime\n","   Ã–rnekler: benzeri, sizgen, ÅŸunla, birinede, kendilerinin\n","--------------------\n","ğŸ”¹ NOUN       : 986,200 kelime\n","   Ã–rnekler: relerin, lappe, canmore, gaviiformes, stanbulkarta\n","--------------------\n","ğŸ”¹ ADJ        : 48,022 kelime\n","   Ã–rnekler: klimatolojik, mÃ¼cellÃ¢, gotÄ±n, sevmeyenin, edepli\n","--------------------\n","ğŸ”¹ NUM        : 67,259 kelime\n","   Ã–rnekler: stivers, klavo, felÃ¢ketzede, botanic, dakara\n","--------------------\n","ğŸ”¹ VERB       : 207,486 kelime\n","   Ã–rnekler: getirileceksin, baÅŸlamayan, dengeleneceÄŸine, yÃ¼kseltilebilen, uzayacaktÄ±\n","--------------------\n","ğŸ”¹ CONJ       : 253 kelime\n","   Ã–rnekler: ille velÃ¢kin, kaldÄ± ki, gÃ¶rÃ¼ro, muuttaa, emde\n","--------------------\n","ğŸ”¹ ADV        : 5,315 kelime\n","   Ã–rnekler: taptaze, kÄ±z kÄ±za, memnun, bedava, ciddiyken\n","--------------------\n","ğŸ”¹ ADP        : 3,947 kelime\n","   Ã–rnekler: kulluÄŸumuz, musavve, milleluci, aÄŸÄ±dÄ±yken, linde\n","--------------------\n","ğŸ”¹ DET        : 505 kelime\n","   Ã–rnekler: yayÄ±nlanmÄ±ÅŸtÄ±rbu, dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼ndebu, bildirdibu, biridirbu, baÄŸlanmÄ±ÅŸtÄ±rbu\n","--------------------\n","========================================\n","ğŸ“Š TOPLAM KELÄ°ME SAYISI: 1,319,672\n"]}]},{"cell_type":"code","source":["#GEREKSÄ°Z KELÄ°ME TEMÄ°ZLÄ°ÄÄ° 1\n","import json\n","import re\n","import os\n","from collections import Counter\n","\n","# Dosya YollarÄ±\n","INPUT_FILE = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_LEXICON.json'\n","OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json'\n","\n","def refinery_logic():\n","    print(\"ğŸ§¹ Temizlik Operasyonu BaÅŸlÄ±yor...\")\n","\n","    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    clean_lexicon = {\n","        'NOUN': set(), 'VERB': set(), 'ADJ': set(),\n","        'ADV': set(), 'PRON': set(), 'CONJ': set(),\n","        'ADP': set(), 'DET': set(), 'NUM': set(), 'PROPN': set()\n","    }\n","\n","    # Ä°stemediÄŸimiz karakterler\n","    garbage_pattern = re.compile(r'[wqx0-9_@\\.\\-\\'\\+]')\n","\n","    # Fiil temizleyici (Basit Kural TabanlÄ± Stemmer)\n","    # AmacÄ±mÄ±z \"getirileceksin\" -> \"getir\" yapmak\n","    def clean_verb(word):\n","        # Sondan eklemeli dillerde tersten budama\n","        suffixes = [\n","            'mek', 'mak', 'me', 'ma', 'iyor', 'Ä±yor', 'uyor', 'Ã¼yor',\n","            'ecek', 'acak', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'dÄ±', 'di', 'du', 'dÃ¼',\n","            'sÄ±n', 'sin', 'sun', 'sÃ¼n', 'lar', 'ler', 'nÄ±z', 'niz'\n","        ]\n","\n","        root = word\n","        # Basit bir dÃ¶ngÃ¼yle sondaki ekleri at (Ã‡ok agresif olmadan)\n","        # KÃ¶k en az 3 harf kalmalÄ± (yememek -> ye)\n","        original = word\n","        for _ in range(3): # 3 katman ek silebilir\n","            for suf in suffixes:\n","                if root.endswith(suf) and len(root) > len(suf) + 2:\n","                    root = root[:-len(suf)]\n","                    break\n","        return root\n","\n","    stats = {'deleted': 0, 'kept': 0, 'trimmed': 0}\n","\n","    for category, words in data.items():\n","        print(f\"   SÃ¼zÃ¼lÃ¼yor: {category} ({len(words)} aday)...\")\n","\n","        for word in words:\n","            word = word.lower().strip()\n","\n","            # 1. Kural: Ã‡Ã¶p karakterler ve Uzunluk\n","            if garbage_pattern.search(word) or len(word) < 2 or len(word) > 12:\n","                stats['deleted'] += 1\n","                continue\n","\n","            # 2. Kural: YapÄ±ÅŸÄ±k Kelimeler (Parser HatalarÄ±)\n","            # \"yapÄ±lmÄ±ÅŸtÄ±rbu\" -> sonu 'bu' ile bitiyorsa ve uzunsa ÅŸÃ¼phelidir\n","            if (word.endswith('bu') or word.endswith('ve')) and len(word) > 8:\n","                stats['deleted'] += 1\n","                continue\n","\n","            # 3. Kural: Kategoriye Ã–zel Temizlik\n","            if category == 'VERB':\n","                root = clean_verb(word)\n","                if root != word:\n","                    stats['trimmed'] += 1\n","                clean_lexicon['VERB'].add(root)\n","\n","            elif category == 'NOUN':\n","                # Ä°simlerde Ã§ok fazla Ã§ekim eki temizlemiyoruz, FST halletsin\n","                # Ama Ã§ok uzunlarÄ± almayalÄ±m\n","                if len(word) < 10:\n","                    clean_lexicon['NOUN'].add(word)\n","\n","            elif category == 'NUM':\n","                # SayÄ± olmayanlarÄ± at (stivers vs.)\n","                # YazÄ±yla yazÄ±lan sayÄ±lar kalsÄ±n (bir, iki, yÃ¼z...)\n","                valid_nums = [\"bir\", \"iki\", \"Ã¼Ã§\", \"dÃ¶rt\", \"beÅŸ\", \"altÄ±\", \"yedi\", \"sekiz\", \"dokuz\", \"on\", \"yirmi\", \"otuz\", \"kÄ±rk\", \"elli\", \"altmÄ±ÅŸ\", \"yetmiÅŸ\", \"seksen\", \"doksan\", \"yÃ¼z\", \"bin\", \"milyon\", \"milyar\", \"sÄ±fÄ±r\"]\n","                if word in valid_nums:\n","                    clean_lexicon['NUM'].add(word)\n","\n","            elif category == 'DET':\n","                # 'yayÄ±nlanmÄ±ÅŸtÄ±rbu' gibi hatalarÄ± elemek iÃ§in sadece bilinenleri alabiliriz\n","                # veya kÄ±sa olanlarÄ± tutabiliriz\n","                if len(word) < 8:\n","                    clean_lexicon['DET'].add(word)\n","\n","            else:\n","                clean_lexicon[category].add(word)\n","\n","    # Listeye Ã§evirip kaydet\n","    final_output = {k: sorted(list(v)) for k, v in clean_lexicon.items()}\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(final_output, f, ensure_ascii=False)\n","\n","    print(\"\\nâœ… TEMÄ°ZLÄ°K BÄ°TTÄ°!\")\n","    print(f\"   ğŸ—‘ï¸ Silinen (Ã‡Ã¶p/Hata): {stats['deleted']:,}\")\n","    print(f\"   âœ‚ï¸ Budanan (Fiil KÃ¶kÃ¼): {stats['trimmed']:,}\")\n","\n","    total_clean = sum(len(v) for v in final_output.values())\n","    print(f\"   ğŸ’ Kalan Saf Kelime: {total_clean:,}\")\n","    print(f\"   ğŸ“‚ KayÄ±t Yeri: {OUTPUT_FILE}\")\n","\n","    # VERB Ã–rnekleri\n","    print(f\"\\n   Fiil Ã–rnekleri (KÃ¶k Haline DÃ¶nmÃ¼ÅŸ mÃ¼?):\")\n","    print(f\"   {final_output['VERB'][:10]}\")\n","\n","refinery_logic()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klfvukEkfp0s","executionInfo":{"status":"ok","timestamp":1765843872848,"user_tz":-180,"elapsed":3111,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"7fb36fdf-9db4-4624-e121-6d2e2ff39b2d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ§¹ Temizlik Operasyonu BaÅŸlÄ±yor...\n","   SÃ¼zÃ¼lÃ¼yor: OTHER (0 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: PRON (685 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: NOUN (986200 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: ADJ (48022 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: NUM (67259 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: VERB (207486 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: CONJ (253 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: ADV (5315 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: ADP (3947 aday)...\n","   SÃ¼zÃ¼lÃ¼yor: DET (505 aday)...\n","\n","âœ… TEMÄ°ZLÄ°K BÄ°TTÄ°!\n","   ğŸ—‘ï¸ Silinen (Ã‡Ã¶p/Hata): 305,336\n","   âœ‚ï¸ Budanan (Fiil KÃ¶kÃ¼): 42,839\n","   ğŸ’ Kalan Saf Kelime: 650,514\n","   ğŸ“‚ KayÄ±t Yeri: /content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json\n","\n","   Fiil Ã–rnekleri (KÃ¶k Haline DÃ¶nmÃ¼ÅŸ mÃ¼?):\n","   ['aaaa', 'aaaaa', 'aaaaaa', 'aaaaaaaa', 'aaaaaad', 'aaadÄ±r', 'aban', 'abanabil', 'abanabilir', 'abanacaÄŸÄ±m']\n"]}]},{"cell_type":"code","source":["#GEREKSÄ°Z EKLER TEMÄ°ZLÄ°ÄÄ°\n","import json\n","import re\n","import os\n","\n","INPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS.json'\n","OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json'\n","\n","def acid_bath_cleaning():\n","    print(\"ğŸ§ª Asit Banyosu BaÅŸlÄ±yor (Agresif Temizlik)...\")\n","\n","    if not os.path.exists(INPUT_FILE):\n","        print(\"âŒ Ã–nceki dosya yok! LÃ¼tfen Ã¶nce Rafineri 1'i Ã§alÄ±ÅŸtÄ±r.\")\n","        return\n","\n","    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    final_lexicon = {k: set() for k in data.keys()}\n","\n","    # TÃ¼rkÃ§e Sesli Harfler\n","    vowels = set(\"aeÄ±ioÃ¶uÃ¼\")\n","    # 3 tane yan yana aynÄ± harf yakalayan regex (aaa, bbb)\n","    triple_char_regex = re.compile(r'(.)\\1\\1')\n","\n","    # Agresif Fiil BudayÄ±cÄ±\n","    def aggressive_verb_stemmer(word):\n","        # Ã–nce Ã§ekim ekleri (Tense/Person)\n","        inflectional_suffixes = [\n","            'casÄ±na', 'cesine', 'mÄ±', 'mi', 'mu', 'mÃ¼',\n","            'tÄ±r', 'tir', 'tur', 'tÃ¼r', 'dÄ±r', 'dir', 'dur', 'dÃ¼r',\n","            'nÄ±z', 'niz', 'nuz', 'nÃ¼z', 'lar', 'ler', 'sÄ±n', 'sin', 'sun', 'sÃ¼n',\n","            'Ä±z', 'iz', 'uz', 'Ã¼z', 'Ä±m', 'im', 'um', 'Ã¼m',\n","            'dÄ±', 'di', 'du', 'dÃ¼', 'tÄ±', 'ti', 'tu', 'tÃ¼',\n","            'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'yacak', 'yecek', 'acak', 'ecek',\n","            'yor', 'Ä±yor', 'iyor', 'uyor', 'Ã¼yor',\n","            'malÄ±', 'meli', 'sa', 'se', 'a', 'e'\n","        ]\n","\n","        # Sonra yapÄ±m/Ã§atÄ± ekleri (Derivational) - SÄ±ra Ã¶nemli!\n","        derivational_suffixes = [\n","            'abil', 'ebil', 'yabil', 'yebil', # Yeterlilik\n","            'iver', 'Ä±ver', 'uver', 'Ã¼ver',   # Tezlik\n","            'yaz', 'eyaz',                    # YaklaÅŸma\n","            'dur', 'edur',                    # SÃ¼erlik\n","            'mek', 'mak', 'me', 'ma'          # Mastar\n","        ]\n","\n","        root = word\n","\n","        # 1. Tur: Ã‡ekim eklerini at\n","        changed = True\n","        while changed:\n","            changed = False\n","            for suf in inflectional_suffixes:\n","                if root.endswith(suf) and len(root) > len(suf) + 2: # KÃ¶k en az 2 harf kalsÄ±n (ye, de)\n","                    root = root[:-len(suf)]\n","                    changed = True\n","                    break\n","\n","        # 2. Tur: Ã‡atÄ±/BirleÅŸik eklerini at (aban-abil -> aban)\n","        changed = True\n","        while changed:\n","            changed = False\n","            for suf in derivational_suffixes:\n","                if root.endswith(suf) and len(root) > len(suf) + 2:\n","                    root = root[:-len(suf)]\n","                    changed = True\n","                    break\n","\n","        return root\n","\n","    stats = {'deleted': 0, 'trimmed': 0}\n","\n","    for category, word_list in data.items():\n","        print(f\"   Ä°ÅŸleniyor: {category} ({len(word_list)} kelime)...\")\n","\n","        for word in word_list:\n","            original_word = word\n","\n","            # KURAL 1: 'aaaa' kontrolÃ¼\n","            if triple_char_regex.search(word):\n","                stats['deleted'] += 1\n","                continue\n","\n","            # KURAL 2: Sesli harf kontrolÃ¼ (Sessiz harflerden oluÅŸan kÄ±saltmalarÄ± at)\n","            if not any(char in vowels for char in word):\n","                stats['deleted'] += 1\n","                continue\n","\n","            # KURAL 3: Ã‡ok uzun ve anlamsÄ±z kelimeler\n","            if len(word) > 15: # TÃ¼rkÃ§e kÃ¶kler bu kadar uzun olmaz\n","                stats['deleted'] += 1\n","                continue\n","\n","            # KURAL 4: Agresif Stemming\n","            if category == 'VERB':\n","                root = aggressive_verb_stemmer(word)\n","                if root != word:\n","                    stats['trimmed'] += 1\n","                final_lexicon['VERB'].add(root)\n","            elif category == 'NOUN':\n","                 # Ä°simlerde de basit Ã§oÄŸul eki temizliÄŸi yapalÄ±m\n","                 if word.endswith('lar') and len(word) > 5:\n","                     final_lexicon['NOUN'].add(word[:-3])\n","                     stats['trimmed'] += 1\n","                 elif word.endswith('ler') and len(word) > 5:\n","                     final_lexicon['NOUN'].add(word[:-3])\n","                     stats['trimmed'] += 1\n","                 else:\n","                     final_lexicon['NOUN'].add(word)\n","            else:\n","                final_lexicon[category].add(word)\n","\n","    # DosyayÄ± kaydet\n","    output_data = {k: sorted(list(v)) for k, v in final_lexicon.items()}\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, ensure_ascii=False)\n","\n","    total_clean = sum(len(v) for v in output_data.values())\n","\n","    print(\"\\nğŸ§ª ASÄ°T BANYOSU TAMAMLANDI!\")\n","    print(f\"   ğŸ—‘ï¸ Eritilen (aaaa/Ã§Ã¶p): {stats['deleted']:,}\")\n","    print(f\"   âœ‚ï¸ Kesilen (aban-abil): {stats['trimmed']:,}\")\n","    print(f\"   ğŸ’ Kalan Saf KÃ¶k: {total_clean:,}\")\n","    print(f\"   ğŸ“‚ Yeni Dosya: {OUTPUT_FILE}\")\n","\n","    print(f\"\\n   Yeni Fiil Ã–rnekleri (Ä°lk 10):\")\n","    print(f\"   {output_data['VERB'][:10]}\")\n","\n","acid_bath_cleaning()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ag-N3YygQCb","executionInfo":{"status":"ok","timestamp":1765844022496,"user_tz":-180,"elapsed":3275,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"2d5fb8a4-db47-4343-bb6d-ff1b26c26ad2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ§ª Asit Banyosu BaÅŸlÄ±yor (Agresif Temizlik)...\n","   Ä°ÅŸleniyor: NOUN (507171 kelime)...\n","   Ä°ÅŸleniyor: VERB (93679 kelime)...\n","   Ä°ÅŸleniyor: ADJ (40859 kelime)...\n","   Ä°ÅŸleniyor: ADV (4507 kelime)...\n","   Ä°ÅŸleniyor: PRON (571 kelime)...\n","   Ä°ÅŸleniyor: CONJ (196 kelime)...\n","   Ä°ÅŸleniyor: ADP (3471 kelime)...\n","   Ä°ÅŸleniyor: DET (42 kelime)...\n","   Ä°ÅŸleniyor: NUM (18 kelime)...\n","   Ä°ÅŸleniyor: PROPN (0 kelime)...\n","\n","ğŸ§ª ASÄ°T BANYOSU TAMAMLANDI!\n","   ğŸ—‘ï¸ Eritilen (aaaa/Ã§Ã¶p): 6,010\n","   âœ‚ï¸ Kesilen (aban-abil): 52,463\n","   ğŸ’ Kalan Saf KÃ¶k: 613,755\n","   ğŸ“‚ Yeni Dosya: /content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json\n","\n","   Yeni Fiil Ã–rnekleri (Ä°lk 10):\n","   ['aban', 'abanabilir', 'abanacaÄŸ', 'aband', 'abandÄ±k', 'abandÄ±n', 'abandÄ±rÄ±r', 'abanmÄ±ÅŸsÄ±', 'abanozlaÅŸ', 'abanozlaÅŸÄ±r']\n"]}]},{"cell_type":"code","source":["import json\n","import os\n","\n","INPUT_FILE = '/content/drive/MyDrive/FSTurk/CLEAN_ROOTS_V2.json'\n","OUTPUT_FILE = '/content/drive/MyDrive/FSTurk/MASTER_FINAL_ROOTS.json'\n","\n","def turbo_smart_reduction():\n","    print(\"ğŸš€ TURBO Ä°ndirgeme BaÅŸlÄ±yor (Set Lookup Strategy)...\")\n","\n","    if not os.path.exists(INPUT_FILE):\n","        print(\"âŒ Dosya bulunamadÄ±!\")\n","        return\n","\n","    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    # Ek listesi (AynÄ± liste)\n","    valid_suffixes = tuple([ # Tuple yaptÄ±k ki startswith daha hÄ±zlÄ± Ã§alÄ±ÅŸsÄ±n\n","        'mak', 'mek', 'ma', 'me', 'Ä±ÅŸ', 'iÅŸ', 'uÅŸ', 'Ã¼ÅŸ',\n","        'yor', 'yabil', 'yebil', 'yacak', 'yecek', 'r', 'ar', 'er', 'Ä±r', 'ir', 'ur', 'Ã¼r',\n","        'dÄ±', 'di', 'du', 'dÃ¼', 'tÄ±', 'ti', 'tu', 'tÃ¼', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ',\n","        'sÄ±n', 'sin', 'sun', 'sÃ¼n', 'nÄ±z', 'niz', 'k', 'n', 'm',\n","        'abil', 'ebil', 'iver', 'Ä±ver', 'dur', 'calama', 'celeme',\n","        'lar', 'ler', 'nÄ±n', 'nin', 'nun', 'nÃ¼n', 'dan', 'den', 'tan', 'ten',\n","        'da', 'de', 'ta', 'te', 'ya', 'ye', 'yÄ±', 'yi', 'yu', 'yÃ¼',\n","        'lÄ±', 'li', 'lu', 'lÃ¼', 'sÄ±z', 'siz', 'suz', 'sÃ¼z', 'lÄ±k', 'lik', 'luk', 'lÃ¼k',\n","        'cÄ±', 'ci', 'cu', 'cÃ¼', 'Ã§a', 'Ã§e', 'ca', 'ce', 'ki', 'kÃ¼', 'sÄ±', 'si',\n","        'Ä±', 'i', 'u', 'Ã¼' # Belirtme hali eklerini de ekledim\n","    ])\n","\n","    final_lexicon = {}\n","    stats = {'deleted': 0, 'kept': 0}\n","\n","    for category, word_list in data.items():\n","        print(f\"   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: {category} ({len(word_list)} kelime)...\")\n","\n","        # 1. Kelimeleri yine kÄ±sadan uzuna sÄ±rala\n","        sorted_words = sorted(list(word_list), key=len)\n","\n","        # SET kullanÄ±yoruz (Arama hÄ±zÄ± O(1))\n","        kept_roots = set()\n","\n","        category_deleted = 0\n","\n","        for word in sorted_words:\n","            # Ã‡ok kÄ±sa kelimeleri direkt al\n","            if len(word) <= 3:\n","                kept_roots.add(word)\n","                continue\n","\n","            is_derivative = False\n","\n","            # YENÄ° MANTIK: Kelimenin kendisi iÃ§inde \"prefix\" arÄ±yoruz\n","            # word = \"kitaplÄ±k\" -> i=3'ten baÅŸla\n","            # kitaplÄ±k -> 'kit', 'kita', 'kitap'...\n","\n","            # Kelimenin uzunluÄŸu kadar dÃ¶ngÃ¼ (Max 15-20 tur, 1 Milyon tur deÄŸil!)\n","            for i in range(3, len(word)):\n","                prefix = word[:i] # Potansiyel kÃ¶k\n","                suffix = word[i:] # Geriye kalan\n","\n","                # 1. Bu prefix bizim elimizdeki kÃ¶klerde var mÄ±?\n","                if prefix in kept_roots:\n","                    # 2. Geriye kalan kÄ±sÄ±m geÃ§erli bir ek mi?\n","                    if suffix.startswith(valid_suffixes):\n","                        is_derivative = True\n","                        break\n","\n","            if is_derivative:\n","                category_deleted += 1\n","                stats['deleted'] += 1\n","            else:\n","                kept_roots.add(word)\n","\n","        final_lexicon[category] = sorted(list(kept_roots))\n","        print(f\"      -> {category} bitti. {category_deleted} kelime elendi.\")\n","\n","    # DosyayÄ± kaydet\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(final_lexicon, f, ensure_ascii=False)\n","\n","    total = sum(len(v) for v in final_lexicon.values())\n","\n","    print(\"\\nğŸ† TURBO OPERASYON TAMAMLANDI!\")\n","    print(f\"   ğŸ—‘ï¸ Toplam Elenen: {stats['deleted']:,}\")\n","    print(f\"   ğŸ’ Kalan Saf KÃ¶kler: {total:,}\")\n","    print(f\"   ğŸ“‚ Dosya: {OUTPUT_FILE}\")\n","\n","    # Kontrol\n","    if 'VERB' in final_lexicon:\n","        sample = [w for w in final_lexicon['VERB'] if w.startswith('aban')]\n","        print(f\"\\n   'aban' KontrolÃ¼: {sample}\")\n","\n","turbo_smart_reduction()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOIHlZttgtYZ","executionInfo":{"status":"ok","timestamp":1765845777643,"user_tz":-180,"elapsed":1479,"user":{"displayName":"Atakan YÄ±lmaz","userId":"18394375161712046586"}},"outputId":"0718cd4e-569e-4d6e-822d-48b89161025c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ TURBO Ä°ndirgeme BaÅŸlÄ±yor (Set Lookup Strategy)...\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: NOUN (492050 kelime)...\n","      -> NOUN bitti. 348169 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: VERB (72152 kelime)...\n","      -> VERB bitti. 54776 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADJ (40782 kelime)...\n","      -> ADJ bitti. 12758 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADV (4502 kelime)...\n","      -> ADV bitti. 608 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: PRON (571 kelime)...\n","      -> PRON bitti. 385 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: CONJ (193 kelime)...\n","      -> CONJ bitti. 8 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: ADP (3445 kelime)...\n","      -> ADP bitti. 619 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: DET (42 kelime)...\n","      -> DET bitti. 4 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: NUM (18 kelime)...\n","      -> NUM bitti. 0 kelime elendi.\n","   âš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Tarama: PROPN (0 kelime)...\n","      -> PROPN bitti. 0 kelime elendi.\n","\n","ğŸ† TURBO OPERASYON TAMAMLANDI!\n","   ğŸ—‘ï¸ Toplam Elenen: 417,327\n","   ğŸ’ Kalan Saf KÃ¶kler: 196,428\n","   ğŸ“‚ Dosya: /content/drive/MyDrive/FSTurk/MASTER_FINAL_ROOTS.json\n","\n","   'aban' KontrolÃ¼: ['aban', 'abanacaÄŸ', 'aband', 'abanozlaÅŸ']\n"]}]}]}